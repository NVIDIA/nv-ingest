{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"What is NVIDIA NeMo Retriever?","text":"<p>NVIDIA NeMo Retriever is a collection of microservices  for building and scaling multimodal data extraction, embedding, and reranking pipelines  with high accuracy and maximum data privacy \u2013 built with NVIDIA NIM.</p> <p>NeMo Retriever provides the following:</p> <ul> <li>Multimodal Data Extraction \u2014 Quickly extract documents at scale that include text, tables, charts, and infographics.</li> <li>Embedding + Indexing \u2014 Embed all extracted text from text chunks and images, and then insert into Milvus - accelerated with NVIDIA cuVS.</li> <li>Retrieval \u2014 Leverage semantic + hybrid search for high accuracy retrieval with the embedding + reranking NIM microservice.</li> </ul> <p></p>"},{"location":"#enterprise-ready-features","title":"Enterprise-Ready Features","text":"<p>NVIDIA NeMo Retriever comes with enterprise-ready features, including the following:</p> <ul> <li>High Accuracy \u2014 NeMo Retriever exhibits a high level of accuracy when retrieving across various modalities through enterprise documents. </li> <li>High Throughput \u2014 NeMo Retriever is capable of extracting, embedding, indexing and retrieving across hundreds of thousands of documents at scale with high throughput. </li> <li>Decomposable/Customizable \u2014 NeMo Retriever consists of modules that can be separately used and deployed in your own environment. </li> <li>Enterprise-Grade Security \u2014 NeMo Retriever NIMs come with security features such as the use of safetensors, continuous patching of CVEs, and more. </li> </ul>"},{"location":"#applications","title":"Applications","text":"<p>The following are some applications that use NVIDIA Nemo Retriever:</p> <ul> <li>Document Research Assistant for Blog Creation (LlamaIndex Jupyter Notebook)</li> <li>Digital Human for Customer Service (NVIDIA AI Blueprint)</li> <li>AI Virtual Assistant for Customer Service (NVIDIA AI Blueprint)</li> <li>Building Code Documentation Agents with CrewAI (CrewAI Demo)</li> <li>Video Search and Summarization (NVIDIA AI Blueprint)</li> </ul>"},{"location":"#related-topics","title":"Related Topics","text":"<ul> <li>NeMo Retriever Text Embedding NIM</li> <li>NeMo Retriever Text Reranking NIM</li> <li>NVIDIA NIM for Object Detection</li> <li>NVIDIA NIM for Table Extraction</li> </ul>"},{"location":"extraction/audio/","title":"Use RIVA Audio Processing","text":""},{"location":"extraction/audio/#use-nemo-retriever-extraction-with-riva","title":"Use NeMo Retriever Extraction with Riva","text":"<p>This documentation describes two methods to run NeMo Retriever extraction  with the RIVA ASR NIM microservice for processing audio files.</p> <ul> <li>Run the NIM locally by using Docker Compose</li> <li>Use NVIDIA Cloud Functions (NVCF) endpoints for cloud-based inference</li> </ul> <p>Note</p> <p>NeMo Retriever extraction is also known as NVIDIA Ingest and nv-ingest.</p>"},{"location":"extraction/audio/#overview","title":"Overview","text":"<p>NeMo Retriever extraction now supports the processing and retrieval of audio files for Retrieval Augmented Generation (RAG) applications.  Similar to how the multimodal document extraction pipeline leverages object detection and image OCR microservices,  NeMo Retriever leverages the RIVA ASR NIM microservice  to transcribe audio files to text, which is then embedded by using the NeMo Retriever embedding NIM. </p> <p>Important</p> <p>Due to limitations in available VRAM controls in the current release of audio NIMs, it must run on a dedicated additional GPU. For the full list of requirements to run RIVA NIM, refer to Support Matrix.</p> <p>This Early Access pipeline enables users to now retrieve audio files at the segment level. </p> <p></p>"},{"location":"extraction/audio/#run-the-nim-locally-by-using-docker-compose","title":"Run the NIM Locally by Using Docker Compose","text":"<p>Use the following procedure to run the NIM locally.</p> <p>Important</p> <p>RIVA NIM must run on a dedicated additional GPU. Edit docker-compose.yaml to set the audio service's device_id to a dedicated GPU: device_ids: [\"1\"] or higher.</p> <ol> <li> <p>To access the required container images, log in to the NVIDIA Container Registry (nvcr.io). Use your NGC key as the password. Run the following command in your terminal.</p> <ul> <li>Replace <code>&lt;your-ngc-key&gt;</code> with your actual NGC API key.</li> <li>The username is always <code>$oauthtoken</code>.</li> </ul> <pre><code>$ docker login nvcr.io\nUsername: $oauthtoken\nPassword: &lt;your-ngc-key&gt;\n</code></pre> </li> <li> <p>Store your NGC key in an environment variable file.</p> </li> </ol> <p>For convenience and security, store your NGC key in a .env file. This enables services to access it without needing to enter the key manually each time.</p> <p>Create a .env file in your working directory and add the following line: <pre><code>NGC_API_KEY=&lt;your-ngc-key&gt;\n</code></pre> Again, replace  with your actual NGC key. <ol> <li> <p>Start the nv-ingest services with the <code>audio</code> profile. This profile includes the necessary components for audio processing. Use the following command.</p> <ul> <li>The <code>--profile audio</code> flag ensures that audio-specific services are launched. For more information, refer to Profile Information.</li> <li>The <code>--build</code> flag ensures that any changes to the container images are applied before starting.</li> </ul> <pre><code>docker compose --profile retrieval --profile audio up --build\n</code></pre> </li> <li> <p>After the services are running, you can interact with nv-ingest by using Python.</p> <ul> <li>The <code>Ingestor</code> object initializes the ingestion process.</li> <li>The <code>files</code> method specifies the input files to process.</li> <li>The <code>extract</code> method tells nv-ingest to extract information from WAV audio files.</li> <li>The <code>document_type</code> parameter is optional, because <code>Ingestor</code> should detect the file type automatically.</li> </ul> <pre><code>ingestor = (\n    Ingestor()\n    .files(\"./data/*.wav\")\n    .extract(\n        document_type=\"wav\",  # Ingestor should detect type automatically in most cases\n        extract_method=\"audio\",\n    )\n)\n</code></pre> <p>Tip</p> <p>For more Python examples, refer to NV-Ingest: Python Client Quick Start Guide.</p> </li> </ol>"},{"location":"extraction/audio/#use-nvcf-endpoints-for-cloud-based-inference","title":"Use NVCF Endpoints for Cloud-Based Inference","text":"<p>Instead of running NV-Ingest locally, you can use NVCF to perform inference by using remote endpoints.</p> <ol> <li> <p>NVCF requires an authentication token and a function ID for access. Ensure you have these credentials ready before making API calls.</p> </li> <li> <p>Run inference by using Python. Provide an NVCF endpoint along with authentication details.</p> <ul> <li>The <code>Ingestor</code> object initializes the ingestion process.</li> <li>The <code>files</code> method specifies the input files to process.</li> <li>The <code>extract</code> method tells nv-ingest to extract information from WAV audio files.</li> <li>The <code>document_type</code> parameter is optional, because <code>Ingestor</code> should detect the file type automatically.</li> </ul> <pre><code>ingestor = (\n    Ingestor()\n    .files(\"./data/*.mp3\")\n    .extract(\n        document_type=\"mp3\",\n        extract_method=\"audio\",\n        extract_audio_params={\n            \"grpc_endpoint\": \"grpc.nvcf.nvidia.com:443\",\n            \"auth_token\": \"&lt;API key&gt;\",\n            \"function_id\": \"&lt;function ID&gt;\",\n            \"use_ssl\": True,\n        },\n    )\n)\n</code></pre> <p>Tip</p> <p>For more Python examples, refer to NV-Ingest: Python Client Quick Start Guide.</p> </li> </ol>"},{"location":"extraction/content-metadata/","title":"Source and Content Metadata Reference for NV-Ingest","text":"<p>This documentation contains the reference for the content metadata.  The definitions used in this documentation are the following:</p> <ul> <li>Source \u2014 The file that is ingested, and from which content and metadata is extracted.</li> <li>Content \u2014 Data extracted from a source, such as text or an image.</li> </ul> <p>Metadata can be extracted from a source or content, or generated by using models, heuristics, or other methods.</p>"},{"location":"extraction/content-metadata/#source-file-metadata","title":"Source File Metadata","text":"<p>The following is the metadata for source files.</p> Field Description Method Source Name The name of the source file. Extracted Source ID The ID of the source file. Extracted Source location The URL, URI, or pointer to the storage location of the source file. \u2014 Source Type The type of the source file, such as pdf, docx, pptx, or txt. Extracted Collection ID The ID of the collection in which the source is contained. \u2014 Date Created The date the source was created. Extracted Last Modified The date the source was last modified. Extracted Partition ID The offset of this data fragment within a larger set of fragments. Generated Access Level The role-based access control for the source. \u2014 Summary A summary of the source. (Not yet implemented.) Generated"},{"location":"extraction/content-metadata/#content-metadata","title":"Content Metadata","text":"<p>The following is the metadata for content.  These fields apply to all content types including text, images, and tables.</p> Field Description Method Type The type of the content. Text, Image, Structured, Table, or Chart. Generated Subtype The type of the content for structured data types, such as table or chart. \u2014 Content Content extracted from the source. Extracted Description A text description of the content object. Generated Page # The page # of the content in the source. Extracted Hierarchy The location or order of the content within the source. Extracted"},{"location":"extraction/content-metadata/#text-metadata","title":"Text Metadata","text":"<p>The following is the metadata for text.</p> Field Description Method Text Type The type of the text, such as header or body. Extracted Keywords Keywords, Named Entities, or other phrases. Extracted Language The language of the content. Generated Summary An abbreviated summary of the content. (Not yet implemented.) Generated"},{"location":"extraction/content-metadata/#image-metadata","title":"Image Metadata","text":"<p>The following is the metadata for images.</p> Field Description Method Image Type The type of the image, such as structured, natural, hybrid, and others. Generated (Classifier) Structured Image Type The type of the content for structured data types, such as bar chart, pie chart, and others. Generated (Classifier) Caption Any caption or subheading associated with Image Extracted Text Extracted text from a structured chart Extracted Image location Location (x,y) of chart within an image Extracted Image location max dimensions Max dimensions (x_max,y_max) of location (x,y) Extracted uploaded_image_uri Mirrors source_metadata.source_location \u2014"},{"location":"extraction/content-metadata/#table-metadata","title":"Table Metadata","text":"<p>The following is the metadata for tables within documents.</p> <p>Warning</p> <p>Tables should not be chunked</p> Field Description Method Table format Structured (dataframe / lists of rows and columns), or serialized as markdown, html, latex, simple (cells separated as spaces). Extracted Table content Extracted text content, formatted according to table_metadata.table_format. Extracted Table location The bounding box of the table. Extracted Table location max dimensions The max dimensions (x_max,y_max) of the bounding box of the table. Extracted Caption The caption for the table or chart. Extracted Title The title of the table. Extracted Subtitle The subtitle of the table. Extracted Axis Axis information for the table. Extracted uploaded_image_uri A mirror of source_metadata.source_location. Generated"},{"location":"extraction/content-metadata/#example-metadata","title":"Example Metadata","text":"<p>The following is an example JSON representation of metadata.  This is an example only, and does not contain the full metadata. For the full file, refer to the data folder.</p> <pre><code>{\n    \"document_type\": \"text\",\n    \"metadata\": \n    {\n        \"content\": \"TestingDocument...\",\n        \"content_url\": \"\",\n        \"source_metadata\": \n        {\n            \"source_name\": \"data/multimodal_test.pdf\",\n            \"source_id\": \"data/multimodal_test.pdf\",\n            \"source_location\": \"\",\n            \"source_type\": \"PDF\",\n            \"collection_id\": \"\",\n            \"date_created\": \"2025-03-13T18:37:14.715892\",\n            \"last_modified\": \"2025-03-13T18:37:14.715534\",\n            \"summary\": \"\",\n            \"partition_id\": -1,\n            \"access_level\": 1\n        },\n        \"content_metadata\": \n        {\n            \"type\": \"structured\",\n            \"description\": \"Structured chart extracted from PDF document.\",\n            \"page_number\": 1,\n            \"hierarchy\": \n            {\n                \"page_count\": 3,\n                \"page\": 1,\n                \"block\": -1,\n                \"line\": -1,\n                \"span\": -1,\n                \"nearby_objects\": \n                {\n                    \"text\": \n                    {\n                        \"content\": [],\n                        \"bbox\": [],\n                        \"type\": []\n                    },\n                    \"images\": \n                    {\n                        \"content\": [],\n                        \"bbox\": [],\n                        \"type\": []\n                    },\n                    \"structured\": \n                    {\n                        \"content\": [],\n                        \"bbox\": [],\n                        \"type\": []\n                    }\n                }\n            },\n            \"subtype\": \"chart\"\n        },\n        \"audio_metadata\": null,\n        \"text_metadata\": null,\n        \"image_metadata\": null,\n        \"table_metadata\": \n        {\n            \"caption\": \"\",\n            \"table_format\": \"image\",\n            \"table_content\": \"Below,is a high-quality picture of some shapes          Picture\",\n            \"table_content_format\": \"\",\n            \"table_location\": \n            [\n                74,\n                614,\n                728,\n                920\n            ],\n            \"table_location_max_dimensions\": \n            [\n                792,\n                1024\n            ],\n            \"uploaded_image_uri\": \"\"\n        },\n        \"chart_metadata\": null,\n        \"error_metadata\": null,\n        \"info_message_metadata\": null,\n        \"debug_metadata\": null,\n        \"raise_on_failure\": false\n    }\n}\n</code></pre>"},{"location":"extraction/contributing/","title":"Contributing to NV-Ingest","text":"<p>External contributions to NV-Ingest will be welcome soon, and they are greatly appreciated!  For more information, refer to Contributing to NV-Ingest.</p>"},{"location":"extraction/data-store/","title":"Upload Data","text":""},{"location":"extraction/data-store/#data-upload-for-nemo-retriever-extraction","title":"Data Upload for NeMo Retriever Extraction","text":"<p>Use this documentation to learn how NeMo Retriever extraction handles and uploads data.</p> <p>Note</p> <p>NeMo Retriever extraction is also known as NVIDIA Ingest and nv-ingest.</p>"},{"location":"extraction/data-store/#overview","title":"Overview","text":"<p>NeMo Retriever extraction supports extracting text representations of various forms of content,  and ingesting to the Milvus vector database.  NeMo Retriever extraction does not store data on disk directly, except through Milvus.  The data upload task pulls extraction results to the Python client,  and then pushes them to Milvus by using its underlying Minio object store service.</p> <p>The vector database stores only the extracted text representations of ingested data.  It does not store the embeddings for images.</p> <p>NeMo Retriever extraction supports uploading data by using the Ingestor.vdb_upload API.  Currently, data upload is not supported through the NV Ingest CLI.</p>"},{"location":"extraction/data-store/#upload-to-milvus","title":"Upload to Milvus","text":"<p>The <code>vdb_upload</code> method uses GPU Cagra accelerated bulk indexing support to load chunks into Milvus.  To enable hybrid retrieval, nv-ingest supports both dense (llama-embedder embeddings) and sparse (bm25) embeddings. </p> <p>Bulk indexing is high throughput, but has a built-in overhead of around one minute.  If the number of ingested documents is 10 or fewer, nv-ingest uses faster streaming inserts instead.  You can control this by setting <code>stream=True</code>. </p> <p>If you set <code>recreate=True</code>, nv-ingest drops and recreates the collection given as collection_name.  The Milvus service persists data to disk by using a Docker volume defined in docker-compose.yaml.  You can delete all collections by deleting that volume, and then restarting the nv-ingest service.</p> <p>To upload to Milvus, use code similar to the following.</p> <pre><code>Ingestor(client=client)\n    .files(\"data/multimodal_test.pdf\")\n    .extract()\n    .embed()\n    .caption()\n    .vdb_upload(\n        collection_name=collection_name,\n        milvus_uri=milvus_uri,\n        sparse=sparse,\n        # for llama-3.2 embedder, use 1024 for e5-v5\n        dense_dim=2048,\n        stream=False,\n        recreate=False\n    )\n</code></pre>"},{"location":"extraction/data-store/#upload-to-a-custom-data-store","title":"Upload to a Custom Data Store","text":"<p>You can ingest to other data stores by using the <code>Ingestor.vdb_upload</code> method;  however, you must configure other data stores and connections yourself.  NeMo Retriever extraction does not provide connections to other data sources. </p>"},{"location":"extraction/environment-config/","title":"Environment Configuration Variables for NV-Ingest","text":"<p>The following are the environment configuration variables that you can specify in your .env file.</p>"},{"location":"extraction/environment-config/#general-environment-variables","title":"General Environment Variables","text":"Name Example Description <code>INGEST_LOG_LEVEL</code> - <code>DEBUG</code>  - <code>INFO</code>  - <code>WARNING</code>  - <code>ERROR</code>  - <code>CRITICAL</code> The log level for the ingest service, which controls the verbosity of the logging output. <code>MESSAGE_CLIENT_HOST</code> - <code>redis</code>  - <code>localhost</code>  - <code>192.168.1.10</code> Specifies the hostname or IP address of the message broker used for communication between services. <code>MESSAGE_CLIENT_PORT</code> - <code>7670</code>  - <code>6379</code> Specifies the port number on which the message broker is listening. <code>MINIO_BUCKET</code> <code>nv-ingest</code> Name of MinIO bucket, used to store image, table, and chart extractions. <code>NGC_API_KEY</code> <code>nvapi-*************</code> An authorized NGC API key, used to interact with hosted NIMs. To create an NGC key, go to https://org.ngc.nvidia.com/setup/api-keys. <code>NIM_NGC_API_KEY</code> \u2014 The key that NIM microservices inside docker containers use to access NGC resources. This is necessary only in some cases when it is different from <code>NGC_API_KEY</code>. If this is not specified, <code>NGC_API_KEY</code> is used to access NGC resources. <code>NVIDIA_BUILD_API_KEY</code> \u2014 The key to access NIMs that are hosted on build.nvidia.com instead of a self-hosted NIM. This is necessary only in some cases when it is different from <code>NGC_API_KEY</code>. If this is not specified, <code>NGC_API_KEY</code> is used for build.nvidia.com. <code>OTEL_EXPORTER_OTLP_ENDPOINT</code> <code>http://otel-collector:4317</code> The endpoint for the OpenTelemetry exporter, used for sending telemetry data. <code>REDIS_MORPHEUS_TASK_QUEUE</code> <code>morpheus_task_queue</code> The name of the task queue in Redis where tasks are stored and processed. <code>DOWNLOAD_LLAMA_TOKENIZER</code> <code>True</code> If <code>True</code>, the llama-3.2 tokenizer will be pre-dowloaded at build time. If not set to <code>True</code>, the (e5-large-unsupervised)[https://huggingface.co/intfloat/e5-large-unsupervised] tokenizer will be pre-downloaded. Note: setting this to <code>True</code> requires a HuggingFace access token with access to the gated Llama-3.2 models. See below for more info. <code>HF_ACCESS_TOKEN</code> - The HuggingFace access token used to pre-downlaod the Llama-3.2 tokenizer from HuggingFace (see above for more info). Llama 3.2 is a gated model, so you must request access to the Llama-3.2 models and then set this variable to a token that can access gated repositories on your behalf in order to use <code>DOWNLOAD_LLAMA_TOKENIZER=True</code>."},{"location":"extraction/environment-config/#library-mode-environment-variables","title":"Library Mode Environment Variables","text":"<p>These environment variables apply specifically when running NV-Ingest in library mode.</p> Name Example Description <code>NVIDIA_BUILD_API_KEY</code> <code>nvapi-*************</code> API key for NVIDIA-hosted NIM services. <code>NVIDIA_API_KEY</code> <code>nvapi-*************</code> copy of <code>NVIDIA_BUILD_API_KEY</code>, llama-index connectors use this key"},{"location":"extraction/faq/","title":"Frequently Asked Questions for NeMo Retriever Extraction","text":"<p>This documentation contains the Frequently Asked Questions (FAQ) for NeMo Retriever extraction.</p> <p>Note</p> <p>NeMo Retriever extraction is also known as NVIDIA Ingest and nv-ingest.</p>"},{"location":"extraction/faq/#what-if-i-already-have-a-retrieval-pipeline-can-i-just-use-nemo-retriever-extraction","title":"What if I already have a retrieval pipeline? Can I just use NeMo Retriever extraction?","text":"<p>You can use the nv-ingest-cli or Python APIs to perform extraction only, and then consume the results. Using the Python API, <code>results</code> is a list object with one entry. For code examples, see the Jupyter notebooks Multimodal RAG with LlamaIndex  and Multimodal RAG with LangChain.</p>"},{"location":"extraction/faq/#where-does-nemo-retriever-extraction-nv-ingest-ingest-to","title":"Where does NeMo Retriever extraction (nv-ingest) ingest to?","text":"<p>NeMo Retriever extraction supports extracting text representations of various forms of content,  and ingesting to the Milvus vector database.  NeMo Retriever extraction does not store data on disk except through Milvus and its underlying Minio object store.  You can ingest to other data stores; however, you must configure other data stores yourself.  For more information, refer to Data Upload.</p>"},{"location":"extraction/faq/#how-would-i-process-unstructured-images","title":"How would I process unstructured images?","text":"<p>For images that <code>nemoretriever-page-elements-v2</code> does not classify as tables, charts, or infographics,  you can use our VLM caption task to create a dense caption of the detected image.  That caption is then be embedded along with the rest of your content.  For more information, refer to Extract Captions from Images.</p>"},{"location":"extraction/faq/#when-should-i-consider-using-nemoretriever-parse","title":"When should I consider using nemoretriever-parse?","text":"<p>For scanned documents, or documents with complex layouts,  we recommend that you use nemoretriever-parse.  Nemo Retriever parse provides higher-accuracy text extraction.  For more information, refer to Use Nemo Retriever Extraction with nemoretriever-parse.</p>"},{"location":"extraction/faq/#why-are-the-environment-variables-different-between-library-mode-and-self-hosted-mode","title":"Why are the environment variables different between library mode and self-hosted mode?","text":""},{"location":"extraction/faq/#self-hosted-deployments","title":"Self-Hosted Deployments","text":"<p>For self-hosted deployments, you should set the environment variables <code>NGC_API_KEY</code> and <code>NIM_NGC_API_KEY</code>. For more information, refer to Generate Your NGC Keys.</p> <p>For advanced scenarios, you might want to set <code>docker-compose</code> environment variables for NIM container paths, tags, and batch sizes.  You can set those directly in <code>docker-compose.yaml</code>, or in an environment variable file that docker compose uses.</p>"},{"location":"extraction/faq/#library-mode","title":"Library Mode","text":"<p>For library mode, you should set the environment variables <code>NVIDIA_BUILD_API_KEY</code> and <code>NVIDIA_API_KEY</code>.  For more information, refer to Generate Your NGC Keys.</p> <p>For advanced scenarios, you might want to use library mode with self-hosted NIM instances.  You can set custom endpoints for each NIM.  For examples of <code>*_ENDPOINT</code> variables, refer to nv-ingest/docker-compose.yaml.</p>"},{"location":"extraction/faq/#what-parameters-or-settings-can-i-adjust-to-optimize-extraction-from-my-documents-or-data","title":"What parameters or settings can I adjust to optimize extraction from my documents or data?","text":"<p>See the Profile Information section  for information about the optional NIM components of the pipeline.</p> <p>You can configure the <code>extract</code>, <code>caption</code>, and other tasks by using the Ingestor API.</p> <p>To choose what types of content to extract, use code similar to the following.  For more information, refer to Extract Specific Elements from PDFs.</p> <pre><code>Ingestor(client=client)\n    .files(\"data/multimodal_test.pdf\")\n    .extract(              \n        extract_text=True,\n        extract_tables=True,\n        extract_charts=True,\n        extract_images=True,\n        paddle_output_format=\"markdown\",\n        extract_infographics=True,\n        text_depth=\"page\"\n    )\n</code></pre> <p>To generate captions for images, use code similar to the following. For more information, refer to Extract Captions from Images.</p> <pre><code>Ingestor(client=client)\n    .files(\"data/multimodal_test.pdf\")\n    .extract()\n    .embed()\n    .caption()\n)\n</code></pre>"},{"location":"extraction/helm/","title":"Deploy With Helm for NeMo Retriever Extraction","text":"<p>To deploy NeMo Retriever extraction by using Helm,  refer to NV-Ingest Helm Charts.</p>"},{"location":"extraction/nemoretriever-parse/","title":"Use NeMo Retriever Parse","text":""},{"location":"extraction/nemoretriever-parse/#use-nemo-retriever-extraction-with-nemoretriever-parse","title":"Use Nemo Retriever Extraction with nemoretriever-parse","text":"<p>This documentation describes two methods to run NeMo Retriever extraction  with nemoretriever-parse.</p> <ul> <li>Run the NIM locally by using Docker Compose</li> <li>Use NVIDIA Cloud Functions (NVCF) endpoints for cloud-based inference</li> </ul> <p>Note</p> <p>NeMo Retriever extraction is also known as NVIDIA Ingest and nv-ingest.</p>"},{"location":"extraction/nemoretriever-parse/#run-the-nim-locally-by-using-docker-compose","title":"Run the NIM Locally by Using Docker Compose","text":"<p>Use the following procedure to run the NIM locally.</p> <p>Important</p> <p>Due to limitations in available VRAM controls in the current release of nemoretriever_parse, it must run on a dedicated additional GPU. Edit docker-compose.yaml to set nemoretriever_parse's device_id to a dedicated GPU: device_ids: [\"1\"] or higher.</p> <ol> <li> <p>Start the nv-ingest services with the <code>nemoretriever-parse</code> profile. This profile includes the necessary components for extracting text and metadata from images. Use the following command.</p> <ul> <li>The --profile nemoretriever-parse flag ensures that vision-language retrieval services are launched.  For more information, refer to Profile Information.</li> <li>The --build flag ensures that any changes to the container images are applied before starting.</li> </ul> <pre><code>docker compose --profile nemoretriever-parse up --build\n</code></pre> </li> <li> <p>After the services are running, you can interact with nv-ingest by using Python.</p> <ul> <li>The <code>Ingestor</code> object initializes the ingestion process.</li> <li>The <code>files</code> method specifies the input files to process.</li> <li>The <code>extract</code> method tells nv-ingest to use <code>nemoretriever-parse</code> for extracting text and metadata from images.</li> <li>The <code>document_type</code> parameter is optional, because <code>Ingestor</code> should detect the file type automatically.</li> </ul> <pre><code>ingestor = (\n    Ingestor()\n    .files(\"./data/*.pdf\")\n    .extract(\n        document_type=\"pdf\",  # Ingestor should detect type automatically in most cases\n        extract_method=\"nemoretriever_parse\"\n    )\n)\n</code></pre> <p>Tip</p> <p>For more Python examples, refer to NV-Ingest: Python Client Quick Start Guide.</p> </li> </ol>"},{"location":"extraction/nemoretriever-parse/#using-nvcf-endpoints-for-cloud-based-inference","title":"Using NVCF Endpoints for Cloud-Based Inference","text":"<p>Instead of running NV-Ingest locally, you can use NVCF to perform inference by using remote endpoints.</p> <ol> <li> <p>Set the authentication token in the <code>.env</code> file.</p> <pre><code>NVIDIA_BUILD_API_KEY=nvapi-...\n</code></pre> </li> <li> <p>Modify <code>docker-compose.yaml</code> to use the hosted <code>nemoretriever-parse</code> service.</p> <pre><code># build.nvidia.com hosted nemoretriever-parse\n- NEMORETRIEVER_PARSE_HTTP_ENDPOINT=https://integrate.api.nvidia.com/v1/chat/completions\n#- NEMORETRIEVER_PARSE_HTTP_ENDPOINT=http://nemoretriever-parse:8000/v1/chat/completions\n</code></pre> </li> <li> <p>Run inference by using Python.</p> <ul> <li>The <code>Ingestor</code> object initializes the ingestion process.</li> <li>The <code>files</code> method specifies the input files to process.</li> <li>The <code>extract</code> method tells nv-ingest to use <code>nemoretriever-parse</code> for extracting text and metadata from images.</li> <li>The <code>document_type</code> parameter is optional, because <code>Ingestor</code> should detect the file type automatically.</li> </ul> <pre><code>ingestor = (\n    Ingestor()\n    .files(\"./data/*.pdf\")\n    .extract(\n        document_type=\"pdf\",  # Ingestor should detect type automatically in most cases\n        extract_method=\"nemoretriever_parse\"\n    )\n)\n</code></pre> <p>Tip</p> <p>For more Python examples, refer to NV-Ingest: Python Client Quick Start Guide.</p> </li> </ol>"},{"location":"extraction/ngc-api-key/","title":"Generate Your NGC Keys","text":"<p>NGC contains many public images, models, and datasets that can be pulled immediately without authentication.  To push and pull custom images, you must generate a key and authenticate with NGC.</p> <p>To create a key, go to https://org.ngc.nvidia.com/setup/api-keys.</p> <p>When you create an NGC key, select the following for Services Included.</p> <ul> <li>NGC Catalog</li> <li>Public API Endpoints</li> </ul> <p>Important</p> <p>Early Access participants must also select Private Registry.</p> <p></p>"},{"location":"extraction/ngc-api-key/#docker-login-to-ngc","title":"Docker Login to NGC","text":"<p>To pull the NIM container image from NGC, use your key to log in to the NGC registry by entering the following command and then following the prompts.  For the username, enter <code>$oauthtoken</code> exactly as shown.  It is a special authentication key for all users.</p> <pre><code>$ docker login nvcr.io\nUsername: $oauthtoken\nPassword: &lt;Your Key&gt;\n</code></pre>"},{"location":"extraction/notebooks/","title":"Notebooks for NeMo Retriever Extraction","text":"<p>To get started using NeMo Retriever extraction, you can try one of the ready-made notebooks that are available.</p> <p>Note</p> <p>NeMo Retriever extraction is also known as NVIDIA Ingest and nv-ingest.</p> <p>To get started with the basics, try one of the following notebooks:</p> <ul> <li>NV-Ingest: CLI Client Quick Start Guide</li> <li>NV-Ingest: Python Client Quick Start Guide</li> </ul> <p>For more advanced scenarios, try one of the following notebooks:</p> <ul> <li>Try out the NVIDIA Multimodal PDF Data Extraction Blueprint</li> <li>Evaluate bo767 retrieval recall accuracy with NV-Ingest and Milvus</li> <li>Multimodal RAG with LangChain</li> <li>Multimodal RAG with LlamaIndex</li> </ul>"},{"location":"extraction/notebooks/#related-topics","title":"Related Topics","text":"<ul> <li>Prerequisites</li> <li>Support Matrix</li> <li>Deploy Without Containers (Library Mode)</li> <li>Deploy With Docker Compose (Self-Hosted)</li> <li>Deploy With Helm</li> </ul>"},{"location":"extraction/nv-ingest-python-api/","title":"Use the NV-Ingest Python API","text":"<p>The NV-Ingest Python API provides a simple and flexible interface for processing and extracting information from various document types, including PDFs.</p> <p>Tip</p> <p>There is a Jupyter notebook available to help you get started with the Python API. For more information, refer to Python Client Quick Start Guide.</p>"},{"location":"extraction/nv-ingest-python-api/#summary-of-key-methods","title":"Summary of Key Methods","text":"<p>The main class in the nv-ingest API is <code>Ingestor</code>.  The <code>Ingestor</code> class provides an interface for building, managing, and running data ingestion jobs, enabling for chainable task additions and job state tracking.  The following table describes methods of the <code>Ingestor</code> class.</p> Method Description <code>files</code> Add document paths for processing. <code>load</code> Ensure files are locally accessible (downloads if needed). <code>extract</code> Add an extraction task (text, tables, charts). <code>split</code> Split documents into smaller sections for processing. <code>embed</code> Generate embeddings from extracted content. <code>caption</code> Extract captions from images within the document. <code>ingest</code> Submit jobs and retrieve results synchronously."},{"location":"extraction/nv-ingest-python-api/#quick-start-extracting-pdfs","title":"Quick Start: Extracting PDFs","text":"<p>The following example demonstrates how to initialize <code>Ingestor</code>, load a PDF file, and extract its contents. The <code>extract</code> method enables different types of data to be extracted.</p>"},{"location":"extraction/nv-ingest-python-api/#extract-a-single-pdf","title":"Extract a Single PDF","text":"<p>Use the following code to extract a single PDF file.</p> <pre><code>from nv_ingest_client.client.interface import Ingestor\n\n# Initialize Ingestor with a local PDF file\ningestor = Ingestor().files(\"path/to/document.pdf\")\n\n# Extract text, tables, and images\nresult = ingestor.extract().ingest()\n\nprint(result)\n</code></pre>"},{"location":"extraction/nv-ingest-python-api/#extract-multiple-pdfs","title":"Extract Multiple PDFs","text":"<p>Use the following code to process multiple PDFs at one time.</p> <pre><code>ingestor = Ingestor().files([\"path/to/doc1.pdf\", \"path/to/doc2.pdf\"])\n\n# Extract content from all PDFs\nresult = ingestor.extract().ingest()\n\nfor doc in result:\n    print(doc)\n</code></pre>"},{"location":"extraction/nv-ingest-python-api/#extract-specific-elements-from-pdfs","title":"Extract Specific Elements from PDFs","text":"<p>By default, the <code>extract</code> method extracts all supported content types.  You can customize the extraction behavior by using the following code.</p> <pre><code>ingestor = ingestor.extract(\n    extract_text=True,  # Extract text\n    extract_tables=False,  # Skip table extraction\n    extract_charts=True,  # Extract charts\n    extract_infographics=True,  # Extract infographic images\n    extract_images=False  # Skip image extraction\n)\n</code></pre>"},{"location":"extraction/nv-ingest-python-api/#extract-non-standard-document-types","title":"Extract Non-standard Document Types","text":"<p>NV-Ingest also supports extracting text from <code>.md</code>, <code>.sh</code>, and <code>.html</code> files</p> <pre><code>ingestor = Ingestor().files([\"path/to/doc1.md\", \"path/to/doc2.html\"])\n\ningestor = ingestor.extract(\n    extract_text=True,  # Only extract text\n    extract_tables=False,\n    extract_charts=False,\n    extract_infographics=False,\n    extract_images=False\n)\n\nresult = ingestor.ingest()\n</code></pre>"},{"location":"extraction/nv-ingest-python-api/#extract-with-custom-document-type","title":"Extract with Custom Document Type","text":"<p>Use the following code to specify a custom document type for extraction.</p> <pre><code>ingestor = ingestor.extract(document_type=\"pdf\")\n</code></pre>"},{"location":"extraction/nv-ingest-python-api/#track-job-progress","title":"Track Job Progress","text":"<p>For large document batches, you can enable a progress bar by setting <code>show_progress</code> to true.  Use the following code.</p> <pre><code>result = ingestor.extract().ingest(show_progress=True)\n</code></pre>"},{"location":"extraction/nv-ingest-python-api/#split-documents","title":"Split Documents","text":"<p>Splitting, also known as chunking, breaks large documents or text into smaller, manageable sections to improve retrieval efficiency. Use the <code>split</code> method to chunk large documents into smaller sections before processing as shown in the following code.</p> <p>Note</p> <p>The default tokenizer (<code>\"meta-llama/Llama-3.2-1B\"</code>) requires a Hugging Face access token. You must set <code>\"hf_access_token\": \"hf_***\"</code> to authenticate.</p> <pre><code>ingestor = ingestor.split(\n    tokenizer=\"meta-llama/Llama-3.2-1B\",\n    chunk_size=16,\n    chunk_overlap=1,\n    params={\"split_source_types\": [\"text\", \"PDF\"], \"hf_access_token\": \"hf_***\"}\n)\n</code></pre> <p>To use a different tokenizer, such as <code>\"intfloat/e5-large-unsupervised\"</code>, you can modify the <code>split</code> call as shown following.</p> <pre><code>ingestor = ingestor.split(\n    tokenizer=\"intfloat/e5-large-unsupervised\",\n    chunk_size=1024,\n    chunk_overlap=150\n)\n</code></pre>"},{"location":"extraction/nv-ingest-python-api/#extract-captions-from-images","title":"Extract Captions from Images","text":"<p>The <code>caption</code> method generates image captions by using a vision-language model.  This can be used to describe images extracted from documents.</p> <p>Note</p> <p>The default model used by <code>caption</code> is <code>meta/llama-3.2-11b-vision-instruct</code>.</p> <pre><code>ingestor = ingestor.caption()\n</code></pre> <p>To specify a different API endpoint, pass additional parameters to <code>caption</code>.</p> <pre><code>ingestor = ingestor.caption(\n    endpoint_url=\"https://ai.api.nvidia.com/v1/gr/meta/llama-3.2-11b-vision-instruct/chat/completions\",\n    model_name=\"meta/llama-3.2-11b-vision-instruct\",\n    api_key=\"nvapi-\"\n)\n</code></pre>"},{"location":"extraction/nv-ingest-python-api/#extract-embeddings","title":"Extract Embeddings","text":"<p>The <code>embed</code> method in NV-Ingest generates text embeddings for document content.</p> <pre><code>ingestor = ingestor.embed()\n</code></pre> <p>Note</p> <p>By default, <code>embed</code> uses the llama-3.2-nv-embedqa-1b-v2 model.</p> <p>To use a different embedding model, such as nv-embedqa-e5-v5, specify a different <code>model_name</code> and <code>endpoint_url</code>.</p> <pre><code>ingestor = ingestor.embed(\n    endpoint_url=\"https://integrate.api.nvidia.com/v1\",\n    model_name=\"nvidia/nv-embedqa-e5-v5\",\n    api_key=\"nvapi-\"\n)\n</code></pre>"},{"location":"extraction/nv-ingest_cli/","title":"Use the NV-Ingest Command Line Interface","text":"<p>After you install the Python dependencies, you can use the NV-Ingest command line interface (CLI).  To use the CLI, use the <code>nv-ingest-cli</code> command.</p> <p>To check the version of the CLI that you have installed, run the following command.</p> <pre><code>nv-ingest-cli --version\n</code></pre> <p>To get a list of the current CLI commands and their options, run the following command.</p> <pre><code>nv-ingest-cli --help\n</code></pre> <p>Tip</p> <p>There is a Jupyter notebook available to help you get started with the CLI. For more information, refer to CLI Client Quick Start Guide.</p>"},{"location":"extraction/nv-ingest_cli/#examples","title":"Examples","text":"<p>Use the following code examples to submit a document to the <code>nv-ingest-ms-runtime</code> service.</p> <p>Each of the following commands can be run from the host machine, or from within the <code>nv-ingest-ms-runtime</code> container.</p> <ul> <li>Host: <code>nv-ingest-cli ...</code></li> <li>Container: <code>nv-ingest-cli ...</code></li> </ul>"},{"location":"extraction/nv-ingest_cli/#example-text-file-with-no-splitting","title":"Example: Text File With No Splitting","text":"<p>To submit a text file with no splitting, run the following code.</p> <p>Note</p> <p>You receive a response that contains a single document, which is the entire text file. The data that is returned is wrapped in the appropriate metadata structure.</p> <pre><code>nv-ingest-cli \\\n  --doc ./data/test.pdf \\\n  --client_host=localhost \\\n  --client_port=7670\n</code></pre>"},{"location":"extraction/nv-ingest_cli/#example-pdf-file-with-splitting-only","title":"Example: PDF File With Splitting Only","text":"<p>To submit a .pdf file with only a splitting task, run the following code.</p> <pre><code>nv-ingest-cli \\\n  --doc ./data/test.pdf \\\n  --output_directory ./processed_docs \\\n  --task='split' \\\n  --client_host=localhost \\\n  --client_port=7670\n</code></pre>"},{"location":"extraction/nv-ingest_cli/#example-pdf-file-with-splitting-and-extraction","title":"Example: PDF File With Splitting and Extraction","text":"<p>To submit a .pdf file with both a splitting task and an extraction task, run the following code.</p> <p>Note</p> <p>This currently only works for pdfium, nemoretriever_parse, and Unstructured.io. Haystack, Adobe, and LlamaParse have existing workflows, but have not been fully converted to use our unified metadata schema.</p> <pre><code>nv-ingest-cli \\\n  --doc ./data/test.pdf \\\n  --output_directory ./processed_docs \\\n  --task='extract:{\"document_type\": \"pdf\", \"extract_method\": \"pdfium\"}' \\\n  --task='extract:{\"document_type\": \"docx\", \"extract_method\": \"python_docx\"}' \\\n  --task='split' \\\n  --client_host=localhost \\\n  --client_port=7670\n</code></pre>"},{"location":"extraction/nv-ingest_cli/#example-process-a-dataset","title":"Example: Process a Dataset","text":"<p>To submit a dataset for processing, run the following code.  To create a dataset, refer to Command Line Dataset Creation with Enumeration and Sampling.</p> <pre><code>nv-ingest-cli \\\n  --dataset dataset.json \\\n  --output_directory ./processed_docs \\\n  --task='extract:{\"document_type\": \"pdf\", \"extract_method\": \"pdfium\"}' \\\n  --client_host=localhost \\\n  --client_port=7670\n</code></pre> <p>Submit a PDF file with extraction tasks and upload extracted images to MinIO.</p> <pre><code>nv-ingest-cli \\\n  --doc ./data/test.pdf \\\n  --output_directory ./processed_docs \\\n  --task='extract:{\"document_type\": \"pdf\", \"extract_method\": \"pdfium\"}' \\\n  --client_host=localhost \\\n  --client_port=7670\n</code></pre>"},{"location":"extraction/nv-ingest_cli/#command-line-dataset-creation-with-enumeration-and-sampling","title":"Command Line Dataset Creation with Enumeration and Sampling","text":"<p>The <code>gen_dataset.py</code> script samples files from a specified source directory according to defined proportions and a total size target.  It offers options for caching the file list, outputting a sampled file list, and validating the output.</p> <pre><code>python ./src/util/gen_dataset.py --source_directory=./data --size=1GB --sample pdf=60 --sample txt=40 --output_file \\\n  dataset.json --validate-output\n</code></pre>"},{"location":"extraction/overview/","title":"What is NeMo Retriever Extraction?","text":"<p>NeMo Retriever extraction is a scalable, performance-oriented document content and metadata extraction microservice.  NeMo Retriever extraction uses specialized NVIDIA NIM microservices  to find, contextualize, and extract text, tables, charts and images that you can use in downstream generative applications.</p> <p>Note</p> <p>NeMo Retriever extraction is also known as NVIDIA Ingest and nv-ingest.</p> <p>NeMo Retriever extraction enables parallelization of splitting documents into pages where artifacts are classified (such as text, tables, charts, and images), extracted, and further contextualized through optical character recognition (OCR) into a well defined JSON schema.  From there, NeMo Retriever extraction can optionally manage computation of embeddings for the extracted content,  and optionally manage storing into a vector database Milvus.</p> <p>Note</p> <p>Cached and Deplot are deprecated. Instead, NeMo Retriever extraction now uses the yolox-graphic-elements NIM. With this change, you should now be able to run NeMo Retriever extraction on a single 24GB A10G or better GPU. If you want to use the old pipeline, with Cached and Deplot, use the NeMo Retriever extraction 24.12.1 release.</p>"},{"location":"extraction/overview/#what-nemo-retriever-extraction-is","title":"What NeMo Retriever Extraction Is \u2714\ufe0f","text":"<p>The following diagram shows the Nemo Retriever extraction pipeline.</p> <p></p> <p>NeMo Retriever extraction is a microservice service that does the following:</p> <ul> <li>Accept a JSON job description, containing a document payload, and a set of ingestion tasks to perform on that payload.</li> <li>Allow the results of a job to be retrieved. The result is a JSON dictionary that contains a list of metadata describing objects extracted from the base document, and processing annotations and timing/trace data.</li> <li>Support multiple methods of extraction for each document type to balance trade-offs between throughput and accuracy. For example, for .pdf documents, extraction is performed by using pdfium, nemoretriever-parse, Unstructured.io, and Adobe Content Extraction Services.</li> <li>Support various types of pre- and post- processing operations, including text splitting and chunking, transform and filtering, embedding generation, and image offloading to storage.</li> </ul> <p>NeMo Retriever extraction supports the following file types:</p> <ul> <li><code>pdf</code></li> <li><code>docx</code></li> <li><code>pptx</code></li> <li><code>jpeg</code></li> <li><code>png</code></li> <li><code>svg</code></li> <li><code>tiff</code></li> <li><code>txt</code></li> </ul>"},{"location":"extraction/overview/#what-nemo-retriever-extraction-isnt","title":"What NeMo Retriever Extraction Isn't \u2716\ufe0f","text":"<p>NeMo Retriever extraction does not do the following:</p> <ul> <li>Run a static pipeline or fixed set of operations on every submitted document.</li> <li>Act as a wrapper for any specific document parsing library.</li> </ul>"},{"location":"extraction/overview/#related-topics","title":"Related Topics","text":"<ul> <li>Prerequisites</li> <li>Support Matrix</li> <li>Deploy Without Containers (Library Mode)</li> <li>Deploy With Docker Compose (Self-Hosted)</li> <li>Deploy With Helm</li> <li>Notebooks</li> <li>Multimodal PDF Data Extraction</li> </ul>"},{"location":"extraction/prerequisites/","title":"Prerequisites for NeMo Retriever Extraction","text":"<p>Before you begin using NeMo Retriever extraction, ensure the following software prerequisites are met.</p>"},{"location":"extraction/prerequisites/#software","title":"Software","text":"<ul> <li>Linux operating systems (Ubuntu 22.04 or later recommended)</li> <li>Docker</li> <li>Docker Compose</li> <li>CUDA Toolkit (NVIDIA Driver &gt;= <code>535</code>, CUDA &gt;= <code>12.2</code>)</li> <li>NVIDIA Container Toolkit</li> <li>Conda Python environment and package manager</li> </ul> <p>Note</p> <p>You install Python later. NV-Ingest only supports Python version 3.10.</p>"},{"location":"extraction/prerequisites/#related-topics","title":"Related Topics","text":"<ul> <li>Support Matrix</li> <li>Deploy Without Containers (Library Mode)</li> <li>Deploy With Docker Compose (Self-Hosted)</li> <li>Deploy With Helm</li> </ul>"},{"location":"extraction/quickstart-guide/","title":"Deploy With Docker Compose (Self-Hosted) for NeMo Retriever Extraction","text":"<p>Use this documentation to get started using NeMo Retriever extraction in self-hosted mode.</p>"},{"location":"extraction/quickstart-guide/#step-1-starting-containers","title":"Step 1: Starting Containers","text":"<p>This example demonstrates how to use the provided docker-compose.yaml to start all needed services with a few commands.</p> <p>Warning</p> <p>NIM containers on their first startup can take 10-15 minutes to pull and fully load models.</p> <p>If you prefer, you can run on Kubernetes by using our Helm chart. Also, there are additional environment variables you might want to configure.</p> <ol> <li> <p>Git clone the repo:</p> <p><code>git clone https://github.com/nvidia/nv-ingest</code></p> </li> <li> <p>Change the directory to the cloned repo</p> <p><code>cd nv-ingest</code>.</p> </li> <li> <p>Generate API keys and authenticate with NGC with the <code>docker login</code> command:</p> <pre><code># This is required to access pre-built containers and NIM microservices\n$ docker login nvcr.io\nUsername: $oauthtoken\nPassword: &lt;Your Key&gt;\n</code></pre> </li> <li> <p>Create a .env file that contains your NVIDIA Build API key.</p> <p>Note</p> <p>If you use an NGC personal key, then you should provide the same value for all keys, but you must specify each environment variable individually. In the past, you could create an API key. If you have an API key, you can still use that. For more information, refer to Generate Your NGC Keys and Environment Configuration Variables.</p> <pre><code># Container images must access resources from NGC.\n\nNGC_API_KEY=&lt;key to download containers from NGC&gt;\nNIM_NGC_API_KEY=&lt;key to download model files after containers start&gt;\n</code></pre> </li> <li> <p>Make sure NVIDIA is set as your default container runtime before running the docker compose command with the command:</p> <p><code>sudo nvidia-ctk runtime configure --runtime=docker --set-as-default</code></p> </li> <li> <p>Start core services. This example uses the table-structure profile.  For more information about other profiles, see Profile Information.</p> <p><code>docker compose --profile retrieval --profile table-structure up</code></p> <p>Tip</p> <p>By default, we have configured log levels to be verbose. It's possible to observe service startup proceeding. You will notice a lot of log messages. Disable verbose logging by configuring <code>NIM_TRITON_LOG_VERBOSE=0</code> for each NIM in docker-compose.yaml.</p> </li> <li> <p>When core services have fully started, <code>nvidia-smi</code> should show processes like the following:</p> <pre><code># If it's taking &gt; 1m for `nvidia-smi` to return, the bus will likely be busy setting up the models.\n+---------------------------------------------------------------------------------------+\n| Processes:                                                                            |\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n|        ID   ID                                                             Usage      |\n|=======================================================================================|\n|    0   N/A  N/A     80461      C   milvus                                     1438MiB |\n|    0   N/A  N/A     83791      C   tritonserver                               2492MiB |\n|    0   N/A  N/A     85605      C   tritonserver                               1896MiB |\n|    0   N/A  N/A     85889      C   tritonserver                               2824MiB |\n|    0   N/A  N/A     88253      C   tritonserver                               2824MiB |\n|    0   N/A  N/A     91194      C   tritonserver                               4546MiB |\n+---------------------------------------------------------------------------------------+\n</code></pre> </li> <li> <p>Observe the started containers with <code>docker ps</code>:</p> <pre><code>CONTAINER ID   IMAGE                                                                                                  COMMAND                  CREATED          STATUS                   PORTS                                                                                                                                                                                                                                                                                                       NAMES\n1b885f37c991   nvcr.io/nvidia/nemo-microservices/nv-ingest:25.03                                                      \"/opt/conda/envs/nv_\u2026\"   3 minutes ago    Up 3 minutes (healthy)   0.0.0.0:7670-7671-&gt;7670-7671/tcp, :::7670-7671-&gt;7670-7671/tcp                                                                                                                                                                                                                                               nv-ingest-nv-ingest-ms-runtime-1\n62c6b999c413   zilliz/attu:v2.3.5                                                                                     \"docker-entrypoint.s\u2026\"   13 minutes ago   Up 3 minutes             0.0.0.0:3001-&gt;3000/tcp, :::3001-&gt;3000/tcp                                                                                                                                                                                                                                                                   milvus-attu\n14ef31ed7f49   milvusdb/milvus:v2.5.3-gpu                                                                             \"/tini -- milvus run\u2026\"   13 minutes ago   Up 3 minutes (healthy)   0.0.0.0:9091-&gt;9091/tcp, :::9091-&gt;9091/tcp, 0.0.0.0:19530-&gt;19530/tcp, :::19530-&gt;19530/tcp                                                                                                                                                                                                                    milvus-standalone\ndceaf36cc5df   otel/opentelemetry-collector-contrib:0.91.0                                                            \"/otelcol-contrib --\u2026\"   13 minutes ago   Up 3 minutes             0.0.0.0:4317-4318-&gt;4317-4318/tcp, :::4317-4318-&gt;4317-4318/tcp, 0.0.0.0:8889-&gt;8889/tcp, :::8889-&gt;8889/tcp, 0.0.0.0:9988-&gt;9988/tcp, :::9988-&gt;9988/tcp, 0.0.0.0:13133-&gt;13133/tcp, :::13133-&gt;13133/tcp, 55678/tcp, 0.0.0.0:33249-&gt;9411/tcp, :::33247-&gt;9411/tcp, 0.0.0.0:55680-&gt;55679/tcp, :::55680-&gt;55679/tcp   nv-ingest-otel-collector-1\nfb252020e4d2   nvcr.io/nvidia/nim/nemoretriever-graphic-elements-v1:1.2.0-rc1-latest-datacenter-release-24734263   \"/opt/nim/start_serv\u2026\"   13 minutes ago   Up 3 minutes             0.0.0.0:8003-&gt;8000/tcp, :::8003-&gt;8000/tcp, 0.0.0.0:8004-&gt;8001/tcp, :::8004-&gt;8001/tcp, 0.0.0.0:8005-&gt;8002/tcp, :::8005-&gt;8002/tcp                                                                                                                                                                             nv-ingest-graphic-elements-1\nc944a9d76831   nvcr.io/nvidia/nim/paddleocr:1.2.0-latest-datacenter-release-24685083                               \"/opt/nim/start_serv\u2026\"   13 minutes ago   Up 3 minutes             0.0.0.0:8009-&gt;8000/tcp, :::8009-&gt;8000/tcp, 0.0.0.0:8010-&gt;8001/tcp, :::8010-&gt;8001/tcp, 0.0.0.0:8011-&gt;8002/tcp, :::8011-&gt;8002/tcp                                                                                                                                                                             nv-ingest-paddle-1\n5bea344526a2   nvcr.io/nvidia/nim/nemoretriever-page-elements-v2:1.2.0-rc0-latest-datacenter-release-24730057      \"/opt/nim/start_serv\u2026\"   13 minutes ago   Up 3 minutes             0.0.0.0:8000-8002-&gt;8000-8002/tcp, :::8000-8002-&gt;8000-8002/tcp                                                                                                                                                                                                                                               nv-ingest-page-elements-1\n16dc2311a6cc   nvcr.io/nvidia/nim/llama-3.2-nv-embedqa-1b-v2:1.5.0-rc0-latest-datacenter-release-24738403          \"/opt/nim/start_serv\u2026\"   13 minutes ago   Up 3 minutes             0.0.0.0:8012-&gt;8000/tcp, :::8012-&gt;8000/tcp, 0.0.0.0:8013-&gt;8001/tcp, :::8013-&gt;8001/tcp, 0.0.0.0:8014-&gt;8002/tcp, :::8014-&gt;8002/tcp                                                                                                                                                                             nv-ingest-embedding-1\ncea3ce001888   nvcr.io/nvidia/nim/nemoretriever-table-structure-v1:1.2.0-rc1-latest-datacenter-release-24826492    \"/opt/nim/start_serv\u2026\"   13 minutes ago   Up 3 minutes             0.0.0.0:8006-&gt;8000/tcp, :::8006-&gt;8000/tcp, 0.0.0.0:8007-&gt;8001/tcp, :::8007-&gt;8001/tcp, 0.0.0.0:8008-&gt;8002/tcp, :::8008-&gt;8002/tcp                                                                                                                                                                             nv-ingest-table-structure-1\n7ddbf7690036   openzipkin/zipkin                                                                                      \"start-zipkin\"           13 minutes ago   Up 3 minutes (healthy)   9410/tcp, 0.0.0.0:9411-&gt;9411/tcp, :::9411-&gt;9411/tcp                                                                                                                                                                                                                                                         nv-ingest-zipkin-1\nb73bbe0c202d   minio/minio:RELEASE.2023-03-20T20-16-18Z                                                               \"/usr/bin/docker-ent\u2026\"   13 minutes ago   Up 3 minutes (healthy)   0.0.0.0:9000-9001-&gt;9000-9001/tcp, :::9000-9001-&gt;9000-9001/tcp                                                                                                                                                                                                                                               minio\n97fa798dbe4f   prom/prometheus:latest                                                                                 \"/bin/prometheus --w\u2026\"   13 minutes ago   Up 3 minutes             0.0.0.0:9090-&gt;9090/tcp, :::9090-&gt;9090/tcp                                                                                                                                                                                                                                                                   nv-ingest-prometheus-1\nf17cb556b086   grafana/grafana                                                                                        \"/run.sh\"                13 minutes ago   Up 3 minutes             0.0.0.0:3000-&gt;3000/tcp, :::3000-&gt;3000/tcp                                                                                                                                                                                                                                                                   grafana-service\n3403c5a0e7be   redis/redis-stack                                                                                      \"/entrypoint.sh\"         13 minutes ago   Up 3 minutes             0.0.0.0:6379-&gt;6379/tcp, :::6379-&gt;6379/tcp, 8001/tcp                                                                                                                                                                                                                                                         nv-ingest-redis-1\n</code></pre> </li> </ol>"},{"location":"extraction/quickstart-guide/#step-2-install-python-dependencies","title":"Step 2: Install Python Dependencies","text":"<p>You can interact with the NV-Ingest service from the host, or by using <code>docker exec</code> to run commands in the NV-Ingest container.</p> <p>To interact from the host, you'll need a Python environment and install the client dependencies:</p> <pre><code># conda not required but makes it easy to create a fresh Python environment\nconda create --name nv-ingest-dev python=3.10\nconda activate nv-ingest-dev\npip install nv-ingest-client==2025.3.10.dev20250310\n</code></pre> <p>Tip</p> <p>To confirm that you have activated your Conda environment, run <code>which pip</code> and <code>which python</code>, and confirm that you see <code>nvingest</code> in the result. You can do this before any pip or python command that you run.</p> <p>Note</p> <p>Interacting from the host depends on the appropriate port being exposed from the nv-ingest container to the host as defined in docker-compose.yaml. If you prefer, you can disable exposing that port and interact with the NV-Ingest service directly from within its container. To interact within the container run <code>docker exec -it nv-ingest-nv-ingest-ms-runtime-1 bash</code>. You'll be in the <code>/workspace</code> directory with <code>DATASET_ROOT</code> from the .env file mounted at <code>./data</code>. The pre-activated <code>nv_ingest_runtime</code> conda environment has all the Python client libraries pre-installed and you should see <code>(morpheus) root@aba77e2a4bde:/workspace#</code>. From the bash prompt above, you can run the nv-ingest-cli and Python examples described following.</p>"},{"location":"extraction/quickstart-guide/#step-3-ingesting-documents","title":"Step 3: Ingesting Documents","text":"<p>You can submit jobs programmatically in Python or using the NV-Ingest CLI.</p> <p>In the below examples, we are doing text, chart, table, and image extraction:</p> <ul> <li>extract_text \u2014 Uses PDFium to find and extract text from pages.</li> <li>extract_images \u2014 Uses PDFium to extract images.</li> <li>extract_tables \u2014 Uses object detection family of NIMs to find tables and charts, and PaddleOCR NIM for table extraction.</li> <li>extract_charts \u2014 Enables or disables chart extraction, also based on the object detection NIM family.</li> </ul>"},{"location":"extraction/quickstart-guide/#in-python","title":"In Python","text":"<p>Tip</p> <p>For more Python examples, refer to NV-Ingest: Python Client Quick Start Guide.</p> <pre><code>import logging, os, time\nfrom nv_ingest_client.client import Ingestor, NvIngestClient\nfrom nv_ingest_client.util.process_json_files import ingest_json_results_to_blob\nclient = NvIngestClient(                                                                         \n    message_client_port=7670,                                                               \n    message_client_hostname=\"localhost\"        \n)                                                                 \n# do content extraction from files                               \ningestor = (\n    Ingestor(client=client)\n    .files(\"data/multimodal_test.pdf\")\n    .extract(             \n        extract_text=True,\n        extract_tables=True,\n        extract_charts=True,\n        extract_images=True,\n        paddle_output_format=\"markdown\",\n        extract_infographics=True,\n        # extract_method=\"nemoretriever_parse\", # Slower, but maximally accurate, especially for PDFs with pages that are scanned images\n        text_depth=\"page\"\n    ).embed()\n    .vdb_upload(\n        collection_name=\"test\",\n        sparse=False,\n        # for llama-3.2 embedder, use 1024 for e5-v5\n        dense_dim=2048\n    )\n)\nprint(\"Starting ingestion..\")\nt0 = time.time()\nresults = ingestor.ingest()\nt1 = time.time()\nprint(f\"Time taken: {t1-t0} seconds\")\n# results blob is directly inspectable\nprint(ingest_json_results_to_blob(results[0]))\n</code></pre> <p>Note</p> <p>To use library mode with nemoretriever_parse, uncomment <code>extract_method=\"nemoretriever_parse\"</code> in the previous code. For more information, refer to Use Nemo Retriever Extraction with nemoretriever-parse.</p> <pre><code>Starting ingestion..\n1 records to insert to milvus\nlogged 8 records\nTime taken: 5.479151725769043 seconds\nThis chart shows some gadgets, and some very fictitious costs. Gadgets and their cost   Chart 1 - Hammer - Powerdrill - Bluetooth speaker - Minifridge - Premium desk fan Dollars $- - $20.00 - $40.00 - $60.00 - $80.00 - $100.00 - $120.00 - $140.00 - $160.00 Cost\nTable 1\n| This table describes some animals, and some activities they might be doing in specific locations. | This table describes some animals, and some activities they might be doing in specific locations. | This table describes some animals, and some activities they might be doing in specific locations. |\n| Animal | Activity | Place |\n| Giraffe | Driving a car | At the beach |\n| Lion | Putting on sunscreen | At the park |\n| Cat | Jumping onto a laptop | In a home office |\n| Dog | Chasing a squirrel | In the front yard |\nTestingDocument\nA sample document with headings and placeholder text\nIntroduction\nThis is a placeholder document that can be used for any purpose. It contains some \nheadings and some placeholder text to fill the space. The text is not important and contains \nno real value, but it is useful for testing. Below, we will have some simple tables and charts \nthat we can use to confirm Ingest is working as expected.\nTable 1\nThis table describes some animals, and some activities they might be doing in specific \nlocations.\nAnimal Activity Place\nGira@e Driving a car At the beach\nLion Putting on sunscreen At the park\nCat Jumping onto a laptop In a home o@ice\nDog Chasing a squirrel In the front yard\nChart 1\nThis chart shows some gadgets, and some very fictitious costs.\nimage_caption:[]\nimage_caption:[]\nBelow,is a high-quality picture of some shapes          Picture\nTable 2\n| This table shows some popular colors that cars might come in | This table shows some popular colors that cars might come in | This table shows some popular colors that cars might come in | This table shows some popular colors that cars might come in |\n| Car | Color1 | Color2 | Color3 |\n| Coupe | White | Silver | Flat Gray |\n| Sedan | White | Metallic Gray | Matte Gray |\n| Minivan | Gray | Beige | Black |\n| Truck | Dark Gray | Titanium Gray | Charcoal |\n| Convertible | Light Gray | Graphite | Slate Gray |\nSection One\nThis is the first section of the document. It has some more placeholder text to show how \nthe document looks like. The text is not meant to be meaningful or informative, but rather to \ndemonstrate the layout and formatting of the document.\n\u2022 This is the first bullet point\n\u2022 This is the second bullet point\n\u2022 This is the third bullet point\nSection Two\nThis is the second section of the document. It is more of the same as we\u2019ve seen in the rest \nof the document. The content is meaningless, but the intent is to create a very simple \nsmoke test to ensure extraction is working as intended. This will be used in CI as time goes \non to ensure that changes we make to the library do not negatively impact our accuracy.\nTable 2\nThis table shows some popular colors that cars might come in.\nCar Color1 Color2 Color3\nCoupe White Silver Flat Gray\nSedan White Metallic Gray Matte Gray\nMinivan Gray Beige Black\nTruck Dark Gray Titanium Gray Charcoal\nConvertible Light Gray Graphite Slate Gray\nPicture\nBelow, is a high-quality picture of some shapes.\nimage_caption:[]\nimage_caption:[]\nThis chart shows some average frequency ranges for speaker drivers. Frequency Ranges ofSpeaker Drivers   Tweeter - Midrange - Midwoofer - Subwoofer Chart2 Hertz (log scale) 1 - 10 - 100 - 1000 - 10000 - 100000 FrequencyRange Start (Hz) - Frequency Range End (Hz)\nChart 2\nThis chart shows some average frequency ranges for speaker drivers.\nConclusion\nThis is the conclusion of the document. It has some more placeholder text, but the most \nimportant thing is that this is the conclusion. As we end this document, we should have \nbeen able to extract 2 tables, 2 charts, and some text including 3 bullet points.\nimage_caption:[]\n</code></pre>"},{"location":"extraction/quickstart-guide/#using-the-nv-ingest-cli","title":"Using the <code>nv-ingest-cli</code>","text":"<p>Tip</p> <p>There is a Jupyter notebook available to help you get started with the CLI. For more information, refer to CLI Client Quick Start Guide.</p> <pre><code>nv-ingest-cli \\\n  --doc ./data/multimodal_test.pdf \\\n  --output_directory ./processed_docs \\\n  --task='extract:{\"document_type\": \"pdf\", \"extract_method\": \"pdfium\", \"extract_tables\": \"true\", \"extract_images\": \"true\", \"extract_charts\": \"true\"}' \\\n  --client_host=localhost \\\n  --client_port=7670\n</code></pre> <p>You should notice output indicating document processing status followed by a breakdown of time spent during job execution: <pre><code>None of PyTorch, TensorFlow &gt;= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n[nltk_data] Downloading package punkt_tab to\n[nltk_data]     /raid/jdyer/miniforge3/envs/nv-ingest-\n[nltk_data]     dev/lib/python3.10/site-\n[nltk_data]     packages/llama_index/core/_static/nltk_cache...\n[nltk_data]   Package punkt_tab is already up-to-date!\nINFO:nv_ingest_client.nv_ingest_cli:Processing 1 documents.\nINFO:nv_ingest_client.nv_ingest_cli:Output will be written to: ./processed_docs\nProcessing files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:02&lt;00:00,  2.34s/file, pages_per_sec=1.28]\nINFO:nv_ingest_client.cli.util.processing:message_broker_task_source: Avg: 2.39 ms, Median: 2.39 ms, Total Time: 2.39 ms, Total % of Trace Computation: 0.06%\nINFO:nv_ingest_client.cli.util.processing:broker_source_network_in: Avg: 9.51 ms, Median: 9.51 ms, Total Time: 9.51 ms, Total % of Trace Computation: 0.25%\nINFO:nv_ingest_client.cli.util.processing:job_counter: Avg: 1.47 ms, Median: 1.47 ms, Total Time: 1.47 ms, Total % of Trace Computation: 0.04%\nINFO:nv_ingest_client.cli.util.processing:job_counter_channel_in: Avg: 0.46 ms, Median: 0.46 ms, Total Time: 0.46 ms, Total % of Trace Computation: 0.01%\nINFO:nv_ingest_client.cli.util.processing:metadata_injection: Avg: 3.52 ms, Median: 3.52 ms, Total Time: 3.52 ms, Total % of Trace Computation: 0.09%\nINFO:nv_ingest_client.cli.util.processing:metadata_injection_channel_in: Avg: 0.16 ms, Median: 0.16 ms, Total Time: 0.16 ms, Total % of Trace Computation: 0.00%\nINFO:nv_ingest_client.cli.util.processing:pdf_content_extractor: Avg: 475.64 ms, Median: 163.77 ms, Total Time: 2378.21 ms, Total % of Trace Computation: 62.73%\nINFO:nv_ingest_client.cli.util.processing:pdf_content_extractor_channel_in: Avg: 0.31 ms, Median: 0.31 ms, Total Time: 0.31 ms, Total % of Trace Computation: 0.01%\nINFO:nv_ingest_client.cli.util.processing:image_content_extractor: Avg: 0.67 ms, Median: 0.67 ms, Total Time: 0.67 ms, Total % of Trace Computation: 0.02%\nINFO:nv_ingest_client.cli.util.processing:image_content_extractor_channel_in: Avg: 0.21 ms, Median: 0.21 ms, Total Time: 0.21 ms, Total % of Trace Computation: 0.01%\nINFO:nv_ingest_client.cli.util.processing:docx_content_extractor: Avg: 0.46 ms, Median: 0.46 ms, Total Time: 0.46 ms, Total % of Trace Computation: 0.01%\nINFO:nv_ingest_client.cli.util.processing:docx_content_extractor_channel_in: Avg: 0.20 ms, Median: 0.20 ms, Total Time: 0.20 ms, Total % of Trace Computation: 0.01%\nINFO:nv_ingest_client.cli.util.processing:pptx_content_extractor: Avg: 0.68 ms, Median: 0.68 ms, Total Time: 0.68 ms, Total % of Trace Computation: 0.02%\nINFO:nv_ingest_client.cli.util.processing:pptx_content_extractor_channel_in: Avg: 0.46 ms, Median: 0.46 ms, Total Time: 0.46 ms, Total % of Trace Computation: 0.01%\nINFO:nv_ingest_client.cli.util.processing:audio_data_extraction: Avg: 1.08 ms, Median: 1.08 ms, Total Time: 1.08 ms, Total % of Trace Computation: 0.03%\nINFO:nv_ingest_client.cli.util.processing:audio_data_extraction_channel_in: Avg: 0.20 ms, Median: 0.20 ms, Total Time: 0.20 ms, Total % of Trace Computation: 0.01%\nINFO:nv_ingest_client.cli.util.processing:dedup_images: Avg: 0.42 ms, Median: 0.42 ms, Total Time: 0.42 ms, Total % of Trace Computation: 0.01%\nINFO:nv_ingest_client.cli.util.processing:dedup_images_channel_in: Avg: 0.42 ms, Median: 0.42 ms, Total Time: 0.42 ms, Total % of Trace Computation: 0.01%\nINFO:nv_ingest_client.cli.util.processing:filter_images: Avg: 0.59 ms, Median: 0.59 ms, Total Time: 0.59 ms, Total % of Trace Computation: 0.02%\nINFO:nv_ingest_client.cli.util.processing:filter_images_channel_in: Avg: 0.57 ms, Median: 0.57 ms, Total Time: 0.57 ms, Total % of Trace Computation: 0.02%\nINFO:nv_ingest_client.cli.util.processing:table_data_extraction: Avg: 240.75 ms, Median: 240.75 ms, Total Time: 481.49 ms, Total % of Trace Computation: 12.70%\nINFO:nv_ingest_client.cli.util.processing:table_data_extraction_channel_in: Avg: 0.38 ms, Median: 0.38 ms, Total Time: 0.38 ms, Total % of Trace Computation: 0.01%\nINFO:nv_ingest_client.cli.util.processing:chart_data_extraction: Avg: 300.54 ms, Median: 299.94 ms, Total Time: 901.62 ms, Total % of Trace Computation: 23.78%\nINFO:nv_ingest_client.cli.util.processing:chart_data_extraction_channel_in: Avg: 0.23 ms, Median: 0.23 ms, Total Time: 0.23 ms, Total % of Trace Computation: 0.01%\nINFO:nv_ingest_client.cli.util.processing:infographic_data_extraction: Avg: 0.77 ms, Median: 0.77 ms, Total Time: 0.77 ms, Total % of Trace Computation: 0.02%\nINFO:nv_ingest_client.cli.util.processing:infographic_data_extraction_channel_in: Avg: 0.25 ms, Median: 0.25 ms, Total Time: 0.25 ms, Total % of Trace Computation: 0.01%\nINFO:nv_ingest_client.cli.util.processing:caption_ext: Avg: 0.55 ms, Median: 0.55 ms, Total Time: 0.55 ms, Total % of Trace Computation: 0.01%\nINFO:nv_ingest_client.cli.util.processing:caption_ext_channel_in: Avg: 0.51 ms, Median: 0.51 ms, Total Time: 0.51 ms, Total % of Trace Computation: 0.01%\nINFO:nv_ingest_client.cli.util.processing:embed_text: Avg: 1.21 ms, Median: 1.21 ms, Total Time: 1.21 ms, Total % of Trace Computation: 0.03%\nINFO:nv_ingest_client.cli.util.processing:embed_text_channel_in: Avg: 0.21 ms, Median: 0.21 ms, Total Time: 0.21 ms, Total % of Trace Computation: 0.01%\nINFO:nv_ingest_client.cli.util.processing:store_embedding_minio: Avg: 0.32 ms, Median: 0.32 ms, Total Time: 0.32 ms, Total % of Trace Computation: 0.01%\nINFO:nv_ingest_client.cli.util.processing:store_embedding_minio_channel_in: Avg: 1.18 ms, Median: 1.18 ms, Total Time: 1.18 ms, Total % of Trace Computation: 0.03%\nINFO:nv_ingest_client.cli.util.processing:message_broker_task_sink_channel_in: Avg: 0.42 ms, Median: 0.42 ms, Total Time: 0.42 ms, Total % of Trace Computation: 0.01%\nINFO:nv_ingest_client.cli.util.processing:No unresolved time detected. Trace times account for the entire elapsed duration.\nINFO:nv_ingest_client.cli.util.processing:Processed 1 files in 2.34 seconds.\nINFO:nv_ingest_client.cli.util.processing:Total pages processed: 3\nINFO:nv_ingest_client.cli.util.processing:Throughput (Pages/sec): 1.28\nINFO:nv_ingest_client.cli.util.processing:Throughput (Files/sec): 0.43\n</code></pre></p>"},{"location":"extraction/quickstart-guide/#step-4-inspecting-and-consuming-results","title":"Step 4: Inspecting and Consuming Results","text":"<p>After the ingestion steps above have been completed, you should be able to find the <code>text</code> and <code>image</code> subfolders inside your processed docs folder. Each will contain JSON-formatted extracted content and metadata.</p> <p>When processing has completed, you'll have separate result files for text and image data: <pre><code>ls -R processed_docs/\n</code></pre> <pre><code>processed_docs/:\nimage  structured  text\n\nprocessed_docs/image:\nmultimodal_test.pdf.metadata.json\n\nprocessed_docs/structured:\nmultimodal_test.pdf.metadata.json\n\nprocessed_docs/text:\nmultimodal_test.pdf.metadata.json\n</code></pre></p> <p>For the full metadata definitions, refer to Content Metadata. </p> <p>We also provide a script for inspecting extracted images.</p> <p>First, install <code>tkinter</code> by running the following code. Choose the code for your OS.</p> <ul> <li> <p>For Ubuntu/Debian Linux:</p> <pre><code>sudo apt-get update\nsudo apt-get install python3-tk\n</code></pre> </li> <li> <p>For Fedora/RHEL Linux:</p> <pre><code>sudo dnf install python3-tkinter\n</code></pre> </li> <li> <p>For macOS using Homebrew:</p> <pre><code>brew install python-tk\n</code></pre> </li> </ul> <p>Then, run the following command to execute the script for inspecting the extracted image:</p> <pre><code>python src/util/image_viewer.py --file_path ./processed_docs/image/multimodal_test.pdf.metadata.json\n</code></pre> <p>Tip</p> <p>Beyond inspecting the results, you can read them into things like llama-index or langchain retrieval pipelines. Also, checkout our demo using a retrieval pipeline on build.nvidia.com to query over document content pre-extracted with NV-Ingest.</p>"},{"location":"extraction/quickstart-guide/#profile-information","title":"Profile Information","text":"<p>The Nemo Retriever extraction core pipeline profiles run on a single A10G or better GPU.  This includes text, table, chart, infographic extraction, embedding and indexing into Milvus.  The advanced profiles require additional GPU support.  This includes audio extraction and VLM integrations.  For more information, refer to Support Matrix.</p> <p>The values that you specify in the <code>--profile</code> option of your <code>docker compose up</code> command are explained in the following table.  You can specify multiple <code>--profile</code> options.</p> Name Type Description GPU Requirements Disk Space Requirements <code>retrieval</code> Core Enables the embedding NIM and (GPU accelerated) Milvus. 1 GPU total for all core profiles. ~150GB total for all core profiles. <code>table-structure</code> Core Enables the yolox table structure NIM which enhances markdown formatting of extracted table content. This benefits answer generation by downstream LLMs. 1 GPU total for all core profiles. ~150GB total for all core profiles. <code>audio</code> Advanced Use Riva for processing audio files. For more information, refer to Audio Processing. 1 additional dedicated GPU ~37GB additional space <code>nemoretriever-parse</code> Advanced Use nemoretriever-parse. For more information, refer to Use Nemo Retriever Extraction with nemoretriever-parse. 1 additional dedicated GPU ~16 GB additional space <code>vlm</code> Advanced Uses llama 3.2 11B VLM for experimental image captioning of unstructured images. 1 additional dedicated GPU ~16GB additional space"},{"location":"extraction/quickstart-guide/#troubleshooting","title":"Troubleshooting","text":""},{"location":"extraction/quickstart-guide/#too-many-open-files-error","title":"Too Many Open Files Error","text":"<p>In rare cases, when you run a job you might an see an error similar to <code>too many open files</code> or <code>max open file descriptor</code>.  This error occurs when the open file descriptor limit for your service user account is too low. To resolve the issue, set or raise the maximum number of open file descriptors (<code>-n</code>) by using the ulimit command. Before you change the <code>-n</code> setting, consider the following:</p> <ul> <li>Apply the <code>-n</code> setting directly to the user (or the Docker container environment) that runs your ingest service.</li> <li>For <code>-n</code> we recommend 10,000 as a baseline, but you might need to raise or lower it based on your actual usage and system configuration.</li> </ul> <pre><code>ulimit -n 10,000\n</code></pre>"},{"location":"extraction/quickstart-guide/#cant-start-new-thread-error","title":"Can't Start New Thread Error","text":"<p>In rare cases, when you run a job you might an see an error similar to <code>can't start new thread</code>.  This error occurs when the maximum number of processes available to a single user is too low. To resolve the issue, set or raise the maximum number of processes (<code>-u</code>) by using the ulimit command. Before you change the <code>-u</code> setting, consider the following:</p> <ul> <li>Apply the <code>-u</code> setting directly to the user (or the Docker container environment) that runs your ingest service.</li> <li>For <code>-u</code> we recommend 10,000 as a baseline, but you might need to raise or lower it based on your actual usage and system configuration.</li> </ul> <pre><code>ulimit -u 10,000\n</code></pre>"},{"location":"extraction/quickstart-guide/#related-topics","title":"Related Topics","text":"<ul> <li>Prerequisites</li> <li>Support Matrix</li> <li>Deploy Without Containers (Library Mode)</li> <li>Deploy With Helm</li> <li>Notebooks</li> <li>Multimodal PDF Data Extraction</li> </ul>"},{"location":"extraction/quickstart-library-mode/","title":"Deploy Without Containers (Library Mode) for NeMo Retriever Extraction","text":"<p>For small-scale workloads, such as workloads of fewer than 100 documents, you can use library mode setup.  Library mode depends on NIMs that are already self-hosted, or, by default, NIMs that are hosted on build.nvidia.com.</p> <p>To get started using NeMo Retriever extraction in library mode, you need the following:</p> <ul> <li>Linux operating systems (Ubuntu 22.04 or later recommended)</li> <li>Conda Python environment and package manager</li> <li>Python version 3.10</li> </ul>"},{"location":"extraction/quickstart-library-mode/#step-1-prepare-your-environment","title":"Step 1: Prepare Your Environment","text":"<p>Use the following procedure to prepare your environment.</p> <ol> <li> <p>Run the following code to create your NV Ingest Conda environment.</p> <pre><code>conda create -y --name nvingest python=3.10 &amp;&amp; \\\nconda activate nvingest &amp;&amp; \\\nconda install -y -c rapidsai -c conda-forge -c nvidia nv_ingest=25.4.2 nv_ingest_client=25.4.2 nv_ingest_api=25.4.2 &amp;&amp; \\\npip install opencv-python llama-index-embeddings-nvidia pymilvus 'pymilvus[bulk_writer, model]' milvus-lite nvidia-riva-client unstructured-client\n</code></pre> <p>Tip</p> <p>To confirm that you have activated your Conda environment, run <code>which python</code> and confirm that you see <code>nvingest</code> in the result. You can do this before any python command that you run.</p> </li> <li> <p>Set or create a .env file that contains your NVIDIA Build API key and other environment variables.</p> <p>Note</p> <p>If you use an NGC personal key, then you should provide the same value for all keys, but you must specify each environment variable individually. In the past, you could create an API key. If you have an API key, you can still use that. For more information, refer to Generate Your NGC Keys and Environment Configuration Variables.</p> <ul> <li> <p>To set your variables, use the following code.</p> <p><pre><code>export NVIDIA_BUILD_API_KEY=nvapi-&lt;your key&gt;\nexport NVIDIA_API_KEY=nvapi-&lt;your key&gt;\n</code></pre>     - To add your variables to an .env file, include the following.</p> <pre><code>NVIDIA_BUILD_API_KEY=nvapi-&lt;your key&gt;\nNVIDIA_API_KEY=nvapi-&lt;your key&gt;    \n</code></pre> </li> </ul> </li> </ol>"},{"location":"extraction/quickstart-library-mode/#step-2-ingest-documents","title":"Step 2: Ingest Documents","text":"<p>You can submit jobs programmatically by using Python.</p> <p>Tip</p> <p>For more Python examples, refer to NV-Ingest: Python Client Quick Start Guide.</p> <p>If you have a very high number of CPUs, and see the process hang without progress,  we recommend that you use <code>taskset</code> to limit the number of CPUs visible to the process.  Use the following code.</p> <pre><code>taskset -c 0-3 python your_ingestion_script.py\n</code></pre> <p>On a 4 CPU core low end laptop, the following code should take about 10 seconds.</p> <pre><code>import logging, os, time, sys\n\nfrom nv_ingest.framework.orchestration.morpheus.util.pipeline.pipeline_runners import (    \n    PipelineCreationSchema,\n    start_pipeline_subprocess\n)\nfrom nv_ingest_client.client import Ingestor, NvIngestClient\nfrom nv_ingest_api.util.message_brokers.simple_message_broker import SimpleClient\nfrom nv_ingest_client.util.process_json_files import ingest_json_results_to_blob\n\n# Start the pipeline subprocess for library mode                       \nconfig = PipelineCreationSchema()                                                  \n\npipeline_process = start_pipeline_subprocess(config)\n# you can configure the subprocesses to log stderr to stdout for debugging purposes\n#pipeline_process = start_pipeline_subprocess(config, stderr=sys.stderr, stdout=sys.stdout)\n\nclient = NvIngestClient(\n    message_client_allocator=SimpleClient,\n    message_client_port=7671,\n    message_client_hostname=\"localhost\"\n)\n\n# gpu_cagra accelerated indexing is not available in milvus-lite\n# Provide a filename for milvus_uri to use milvus-lite\nmilvus_uri = \"milvus.db\"\ncollection_name = \"test\"\nsparse=False\n\n# do content extraction from files                                \ningestor = (\n    Ingestor(client=client)\n    .files(\"data/multimodal_test.pdf\")\n    .extract(              \n        extract_text=True,\n        extract_tables=True,\n        extract_charts=True,\n        extract_images=True,\n        paddle_output_format=\"markdown\",\n        extract_infographics=True,\n        # Slower, but maximally accurate, especially for PDFs with pages that are scanned images\n        #extract_method=\"nemoretriever_parse\",\n        text_depth=\"page\"\n    ).embed()\n    .vdb_upload(\n        collection_name=collection_name,\n        milvus_uri=milvus_uri,\n        sparse=sparse,\n        # for llama-3.2 embedder, use 1024 for e5-v5\n        dense_dim=2048\n    )\n)\n\nprint(\"Starting ingestion..\")\nt0 = time.time()\nresults = ingestor.ingest(show_progress=True)\nt1 = time.time()\nprint(f\"Time taken: {t1-t0} seconds\")\n\n# results blob is directly inspectable\nprint(ingest_json_results_to_blob(results[0]))\n</code></pre> <p>Note</p> <p>To use library mode with nemoretriever_parse, uncomment <code>extract_method=\"nemoretriever_parse\"</code> in the previous code. For more information, refer to Use Nemo Retriever Extraction with nemoretriever-parse.</p> <p>You can see the extracted text that represents the content of the ingested test document.</p> <pre><code>Starting ingestion..\nTime taken: 9.243880033493042 seconds\n\nTestingDocument\nA sample document with headings and placeholder text\nIntroduction\nThis is a placeholder document that can be used for any purpose. It contains some \nheadings and some placeholder text to fill the space. The text is not important and contains \nno real value, but it is useful for testing. Below, we will have some simple tables and charts \nthat we can use to confirm Ingest is working as expected.\nTable 1\nThis table describes some animals, and some activities they might be doing in specific \nlocations.\nAnimal Activity Place\nGira@e Driving a car At the beach\nLion Putting on sunscreen At the park\nCat Jumping onto a laptop In a home o@ice\nDog Chasing a squirrel In the front yard\nChart 1\nThis chart shows some gadgets, and some very fictitious costs.\n\n... document extract continues ...\n</code></pre>"},{"location":"extraction/quickstart-library-mode/#step-3-query-ingested-content","title":"Step 3: Query Ingested Content","text":"<p>To query for relevant snippets of the ingested content, and use them with an LLM to generate answers, use the following code.</p> <pre><code>from openai import OpenAI\nfrom nv_ingest_client.util.milvus import nvingest_retrieval\nimport os\n\nmilvus_uri = \"milvus.db\"\ncollection_name = \"test\"\nsparse=False\n\nqueries = [\"Which animal is responsible for the typos?\"]\n\nretrieved_docs = nvingest_retrieval(\n    queries,\n    collection_name,\n    milvus_uri=milvus_uri,\n    hybrid=sparse,\n    top_k=1,\n)\n\n# simple generation example\nextract = retrieved_docs[0][0][\"entity\"][\"text\"]\nclient = OpenAI(\n  base_url = \"https://integrate.api.nvidia.com/v1\",\n  api_key = os.environ[\"NVIDIA_BUILD_API_KEY\"]\n)\n\nprompt = f\"Using the following content: {extract}\\n\\n Answer the user query: {queries[0]}\"\nprint(f\"Prompt: {prompt}\")\ncompletion = client.chat.completions.create(\n  model=\"nvidia/llama-3.1-nemotron-70b-instruct\",\n  messages=[{\"role\":\"user\",\"content\": prompt}],\n)\nresponse = completion.choices[0].message.content\n\nprint(f\"Answer: {response}\")\n</code></pre> <pre><code>Prompt: Using the following content: TestingDocument\nA sample document with headings and placeholder text\nIntroduction\nThis is a placeholder document that can be used for any purpose. It contains some \nheadings and some placeholder text to fill the space. The text is not important and contains \nno real value, but it is useful for testing. Below, we will have some simple tables and charts \nthat we can use to confirm Ingest is working as expected.\nTable 1\nThis table describes some animals, and some activities they might be doing in specific \nlocations.\nAnimal Activity Place\nGira@e Driving a car At the beach\nLion Putting on sunscreen At the park\nCat Jumping onto a laptop In a home o@ice\nDog Chasing a squirrel In the front yard\nChart 1\nThis chart shows some gadgets, and some very fictitious costs.\n\n Answer the user query: Which animal is responsible for the typos?\nAnswer: A clever query!\n\nAfter carefully examining the provided content, I'd like to point out the potential \"typos\" (assuming you're referring to the unusual or intentionally incorrect text) and attempt to playfully \"assign blame\" to an animal based on the context:\n\n1. **Gira@e** (instead of Giraffe) - **Animal blamed: Giraffe** (Table 1, first row)\n    * The \"@\" symbol in \"Gira@e\" suggests a possible typo or placeholder character, which we'll humorously attribute to the Giraffe's alleged carelessness.\n2. **o@ice** (instead of Office) - **Animal blamed: Cat**\n    * The same \"@\" symbol appears in \"o@ice\", which is related to the Cat's activity in the same table. Perhaps the Cat was in a hurry while typing and introduced the error?\n\nSo, according to this whimsical analysis, both the **Giraffe** and the **Cat** are \"responsible\" for the typos, with the Giraffe possibly being the more egregious offender given the more blatant character substitution in its name.\n</code></pre>"},{"location":"extraction/quickstart-library-mode/#related-topics","title":"Related Topics","text":"<ul> <li>Prerequisites</li> <li>Support Matrix</li> <li>Deploy With Docker Compose (Self-Hosted)</li> <li>Deploy With Helm</li> <li>Notebooks</li> <li>Multimodal PDF Data Extraction</li> </ul>"},{"location":"extraction/releasenotes-nv-ingest/","title":"Release Notes for NeMo Retriever Extraction","text":"<p>This documentation contains the release notes for NeMo Retriever extraction.</p> <p>Note</p> <p>NeMo Retriever extraction is also known as NVIDIA Ingest and nv-ingest.</p>"},{"location":"extraction/releasenotes-nv-ingest/#release-2542","title":"Release 25.4.2","text":"<p>The NeMo Retriever extraction 25.04 release focuses on small bug fixes and improvements, including the following:</p> <ul> <li>Fixed a known issue where large text file ingestion failed.</li> <li>The REST service is now more resilient, and recovers from worker failures and connection errors.</li> <li>Various improvements on the client side to reduce retry rates, and improve overall quality of life.</li> <li>New notebook for How to reindex a collection.</li> <li>Expanded chunking documentation. For more information, refer to Split Documents.</li> </ul>"},{"location":"extraction/releasenotes-nv-ingest/#breaking-changes","title":"Breaking Changes","text":"<p>There are no breaking changes in this version.</p>"},{"location":"extraction/releasenotes-nv-ingest/#upgrade","title":"Upgrade","text":"<p>To upgrade the Helm Charts for this version, refer to NV-Ingest Helm Charts.</p>"},{"location":"extraction/releasenotes-nv-ingest/#release-2503-ga-release","title":"Release 25.03 (GA Release)","text":""},{"location":"extraction/releasenotes-nv-ingest/#summary","title":"Summary","text":"<p>The NeMo Retriever extraction 25.03 release includes accuracy improvements, feature expansions, and throughput improvements.</p>"},{"location":"extraction/releasenotes-nv-ingest/#new-features","title":"New Features","text":"<ul> <li>Consolidated NeMo Retriever extraction to run on a single GPU (H100, A100, L40S, or A10G). For details, refer to Support Matrix.</li> <li>Added Library Mode for a lightweight no-GPU deployment that uses NIM endpoints hosted on build.nvidia.com. For details, refer to Deploy Without Containers (Library Mode).</li> <li>Added support for infographics extraction.</li> <li>Added support for RIVA NIM for Audio extraction (Early Access). For details, refer to Audio Processing.</li> <li>Added support for Llama-3.2 VLM for Image Captioning capability.</li> <li>docX, pptx, jpg, png support for image detection &amp; extraction.</li> <li>Deprecated DePlot and CACHED NIMs.</li> </ul>"},{"location":"extraction/releasenotes-nv-ingest/#release-24121-prerelease","title":"Release 24.12.1 (Prerelease)","text":""},{"location":"extraction/releasenotes-nv-ingest/#bug-fixes","title":"Bug fixes","text":"<p>Cases where .split() tasks fail during ingestion are now fixed.</p>"},{"location":"extraction/releasenotes-nv-ingest/#release-2412-prerelease","title":"Release 24.12 (Prerelease)","text":""},{"location":"extraction/releasenotes-nv-ingest/#known-issues","title":"Known Issues","text":"<p>We currently do not support OCR-based text extraction. This was discovered in an unsupported use case and is not a product functionality issue.</p>"},{"location":"extraction/support-matrix/","title":"Support Matrix for NeMo Retriever Extraction","text":"<p>Before you begin using NeMo Retriever extraction, ensure that you have the hardware for your use case.</p>"},{"location":"extraction/support-matrix/#hardware","title":"Hardware","text":"GPU Family Memory Minimum GPUs H100 SXM or PCIe 80GB 1 A100 SXM or PCIe 80GB 1 A10G \u2014 24GB 1 L40S \u2014 48GB 1 <p>The core pipeline requires approximately 150GB disk space.  To run the core pipeline and all optional features, you need approximately 210GB disk space.</p>"},{"location":"extraction/support-matrix/#advanced-feature-support","title":"Advanced Feature Support","text":"<p>Some advanced features, such as VLM integrations and audio extraction, require additional GPU support and disk space.  For more information, refer to Profile Information.</p>"},{"location":"extraction/support-matrix/#vlm-integrations","title":"VLM Integrations","text":"<ul> <li>nemoretriever-parse VLM NIM \u2014 NeMo Retriever is compatible with nemoretriever-parse VLM NIM, which adds state-of-the-art text and table extraction. To integrate this NIM into the nv-ingest pipeline, you need 1 additional GPU (H100, A100, A10G, L40S). Nemo Retriever parse requires ~16GB additional disk space.</li> <li>Llama3.2 Vision VLM NIMs \u2014 NeMo Retriever is compatible with the Llama3.2 VLM NIMs for image captioning capabilities. To integrate these NIM into the nv-ingest pipeline, you need 1 additional GPU (H100, A100, A10G, L40S). Image captioning requires ~16GB additional disk space.</li> </ul>"},{"location":"extraction/support-matrix/#audio-extraction-early-access","title":"Audio Extraction (Early Access)","text":"<ul> <li>RIVA NIM \u2014 NeMo Retriever can retrieve across audio files by using the RIVA NIMs. To integrate this capability into the nv-ingest pipeline, you need 1 additional GPU (H100, A100, A10G, L40S). Audio extraction requires ~37GB additional disk space.</li> </ul>"},{"location":"extraction/support-matrix/#related-topics","title":"Related Topics","text":"<ul> <li>Prerequisites</li> <li>Deploy Without Containers (Library Mode)</li> <li>Deploy With Docker Compose (Self-Hosted)</li> <li>Deploy With Helm</li> </ul>"},{"location":"extraction/telemetry/","title":"Telemetry with NeMo Retriever Extraction","text":"<p>You can view telemetry data for NeMo Retriever extraction.</p> <p>Note</p> <p>NeMo Retriever extraction is also known as NVIDIA Ingest and nv-ingest.</p>"},{"location":"extraction/telemetry/#opentelemetry","title":"OpenTelemetry","text":"<p>After OpenTelemetry and Zipkin are running, you can open your browser to explore traces: </p> <ul> <li>Docker \u2014 Use http://$YOUR_DOCKER_HOST:9411/zipkin/ </li> <li>Kubernetes \u2014 Use http://$YOUR_K8S_OTEL_POD:9411/zipkin/</li> </ul> <p></p>"},{"location":"extraction/telemetry/#prometheus","title":"Prometheus","text":"<p>After Prometheus is running, you can open your browser to explore metrics: </p> <ul> <li>Docker \u2014 Use http://$YOUR_DOCKER_HOST:9090/ziplin/</li> <li>Kubernetes \u2014 Use http://$YOUR_K8S_OTEL_POD:9090/zipkin/</li> </ul> <p></p>"}]}