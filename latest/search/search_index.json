{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"What is NVIDIA NeMo Retriever?","text":"<p>NVIDIA NeMo Retriever is a collection of microservices  for building and scaling multimodal data extraction, embedding, and reranking pipelines  with high accuracy and maximum data privacy \u2013 built with NVIDIA NIM.  NeMo Retriever, part of the NVIDIA NeMo software suite for managing the AI agent lifecycle,  ensures data privacy and seamlessly connects to proprietary data wherever it resides,  empowering secure, enterprise-grade retrieval.</p> <p>NeMo Retriever provides the following:</p> <ul> <li>Multimodal Data Extraction \u2014 Quickly extract documents at scale that include text, tables, charts, and infographics.</li> <li>Embedding + Indexing \u2014 Embed all extracted text from text chunks and images, and then insert into Milvus - accelerated with NVIDIA cuVS.</li> <li>Retrieval \u2014 Leverage semantic + hybrid search for high accuracy retrieval with the embedding + reranking NIM microservice.</li> </ul> <p></p>"},{"location":"#enterprise-ready-features","title":"Enterprise-Ready Features","text":"<p>NVIDIA NeMo Retriever comes with enterprise-ready features, including the following:</p> <ul> <li>High Accuracy \u2014 NeMo Retriever exhibits a high level of accuracy when retrieving across various modalities through enterprise documents. </li> <li>High Throughput \u2014 NeMo Retriever is capable of extracting, embedding, indexing and retrieving across hundreds of thousands of documents at scale with high throughput. </li> <li>Decomposable/Customizable \u2014 NeMo Retriever consists of modules that can be separately used and deployed in your own environment. </li> <li>Enterprise-Grade Security \u2014 NeMo Retriever NIMs come with security features such as the use of safetensors, continuous patching of CVEs, and more. </li> </ul>"},{"location":"#applications","title":"Applications","text":"<p>The following are some applications that use NVIDIA Nemo Retriever:</p> <ul> <li>AI Virtual Assistant for Customer Service (NVIDIA AI Blueprint)</li> <li>Build an Enterprise RAG pipeline (NVIDIA AI Blueprint)</li> <li>Building Code Documentation Agents with CrewAI (CrewAI Demo)</li> <li>Digital Human for Customer Service (NVIDIA AI Blueprint)</li> <li>Document Research Assistant for Blog Creation (LlamaIndex Jupyter Notebook)</li> <li>Video Search and Summarization (NVIDIA AI Blueprint)</li> </ul>"},{"location":"#related-topics","title":"Related Topics","text":"<ul> <li>NeMo Retriever Text Embedding NIM</li> <li>NeMo Retriever Text Reranking NIM</li> <li>NVIDIA NIM for Object Detection</li> <li>NVIDIA NIM for Image OCR</li> </ul>"},{"location":"extraction/audio/","title":"Extract Speech with NeMo Retriever Extraction","text":"<p>This documentation describes two methods to run NeMo Retriever extraction  with the RIVA ASR NIM microservice  to extract speech from audio files.</p> <ul> <li>Run the NIM locally by using Docker Compose</li> <li>Use NVIDIA Cloud Functions (NVCF) endpoints for cloud-based inference</li> </ul> <p>Note</p> <p>NeMo Retriever extraction is also known as NVIDIA Ingest and nv-ingest.</p> <p>Currently, you can extract speech from the following file types:</p> <ul> <li><code>mp3</code></li> <li><code>wav</code></li> </ul>"},{"location":"extraction/audio/#overview","title":"Overview","text":"<p>NeMo Retriever extraction supports extracting speech from audio files for Retrieval Augmented Generation (RAG) applications.  Similar to how the multimodal document extraction pipeline leverages object detection and image OCR microservices,  NeMo Retriever leverages the RIVA ASR NIM microservice  to transcribe speech to text, which is then embedded by using the NeMo Retriever embedding NIM. </p> <p>Important</p> <p>Due to limitations in available VRAM controls in the current release, the RIVA ASR NIM microservice must run on a dedicated additional GPU. For the full list of requirements, refer to Support Matrix.</p> <p>This pipeline enables users to retrieve speech files at the segment level.</p> <p></p>"},{"location":"extraction/audio/#run-the-nim-locally-by-using-docker-compose","title":"Run the NIM Locally by Using Docker Compose","text":"<p>Use the following procedure to run the NIM locally.</p> <p>Important</p> <p>The RIVA ASR NIM microservice must run on a dedicated additional GPU. Edit docker-compose.yaml to set the device_id to a dedicated GPU: device_ids: [\"1\"] or higher.</p> <ol> <li> <p>To access the required container images, log in to the NVIDIA Container Registry (nvcr.io). Use your NGC key as the password. Run the following command in your terminal.</p> <ul> <li>Replace <code>&lt;your-ngc-key&gt;</code> with your actual NGC API key.</li> <li>The username is always <code>$oauthtoken</code>.</li> </ul> <pre><code>$ docker login nvcr.io\nUsername: $oauthtoken\nPassword: &lt;your-ngc-key&gt;\n</code></pre> </li> <li> <p>For convenience and security, store your NGC key in an environment variable file (<code>.env</code>). This enables services to access it without needing to enter the key manually each time. Create a .env file in your working directory and add the following line. Replace <code>&lt;your-ngc-key&gt;</code> with your actual NGC key.</p> <pre><code>NGC_API_KEY=&lt;your-ngc-key&gt;\n</code></pre> </li> <li> <p>Start the nv-ingest services with the <code>audio</code> profile. This profile includes the necessary components for audio processing. Use the following command. The <code>--profile audio</code> flag ensures that speech-specific services are launched. For more information, refer to Profile Information.</p> <pre><code>docker compose --profile retrieval --profile audio up\n</code></pre> </li> <li> <p>After the services are running, you can interact with nv-ingest by using Python.</p> <ul> <li>The <code>Ingestor</code> object initializes the ingestion process.</li> <li>The <code>files</code> method specifies the input files to process.</li> <li>The <code>extract</code> method tells nv-ingest to extract information from WAV audio files.</li> <li>The <code>document_type</code> parameter is optional, because <code>Ingestor</code> should detect the file type automatically.</li> </ul> <pre><code>ingestor = (\n    Ingestor()\n    .files(\"./data/*.wav\")\n    .extract(\n        document_type=\"wav\",  # Ingestor should detect type automatically in most cases\n        extract_method=\"audio\",\n    )\n)\n</code></pre> <p>Tip</p> <p>For more Python examples, refer to NV-Ingest: Python Client Quick Start Guide.</p> </li> </ol>"},{"location":"extraction/audio/#use-nvcf-endpoints-for-cloud-based-inference","title":"Use NVCF Endpoints for Cloud-Based Inference","text":"<p>Instead of running NV-Ingest locally, you can use NVCF to perform inference by using remote endpoints.</p> <ol> <li> <p>NVCF requires an authentication token and a function ID for access. Ensure you have these credentials ready before making API calls.</p> </li> <li> <p>Run inference by using Python. Provide an NVCF endpoint along with authentication details.</p> <ul> <li>The <code>Ingestor</code> object initializes the ingestion process.</li> <li>The <code>files</code> method specifies the input files to process.</li> <li>The <code>extract</code> method tells nv-ingest to extract information from WAV audio files.</li> <li>The <code>document_type</code> parameter is optional, because <code>Ingestor</code> should detect the file type automatically.</li> </ul> <pre><code>ingestor = (\n    Ingestor()\n    .files(\"./data/*.mp3\")\n    .extract(\n        document_type=\"mp3\",\n        extract_method=\"audio\",\n        extract_audio_params={\n            \"grpc_endpoint\": \"grpc.nvcf.nvidia.com:443\",\n            \"auth_token\": \"&lt;API key&gt;\",\n            \"function_id\": \"&lt;function ID&gt;\",\n            \"use_ssl\": True,\n        },\n    )\n)\n</code></pre> <p>Tip</p> <p>For more Python examples, refer to NV-Ingest: Python Client Quick Start Guide.</p> </li> </ol>"},{"location":"extraction/audio/#related-topics","title":"Related Topics","text":"<ul> <li>Support Matrix</li> <li>Troubleshoot Nemo Retriever Extraction</li> <li>Use the NV-Ingest Python API</li> </ul>"},{"location":"extraction/benchmarking/","title":"nv-ingest Integration Testing Framework","text":"<p>A configurable, dataset-agnostic testing framework for end-to-end validation of nv-ingest pipelines. This framework uses structured YAML configuration for type safety, validation, and parameter management.</p>"},{"location":"extraction/benchmarking/#quick-start","title":"Quick Start","text":""},{"location":"extraction/benchmarking/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker and Docker Compose running</li> <li>Python environment with nv-ingest-client</li> <li>Access to test datasets</li> </ul>"},{"location":"extraction/benchmarking/#run-your-first-test","title":"Run Your First Test","text":"<pre><code># 1. Navigate to the nv-ingest-harness directory\ncd tools/harness\n\n# 2. Install dependencies\nuv sync\n\n# 3. Run with a pre-configured dataset (assumes services are running)\nuv run nv-ingest-harness-run --case=e2e --dataset=bo767\n\n# Or use a custom path that uses the \"active\" configuration\nuv run nv-ingest-harness-run --case=e2e --dataset=/path/to/your/data\n\n# With managed infrastructure (starts/stops services)\nuv run nv-ingest-harness-run --case=e2e --dataset=bo767 --managed\n</code></pre>"},{"location":"extraction/benchmarking/#configuration-system","title":"Configuration System","text":""},{"location":"extraction/benchmarking/#yaml-configuration-test_configsyaml","title":"YAML Configuration (<code>test_configs.yaml</code>)","text":"<p>The framework uses a structured YAML file for all test configuration. Configuration is organized into logical sections:</p>"},{"location":"extraction/benchmarking/#active-configuration","title":"Active Configuration","text":"<p>The <code>active</code> section contains your current test settings. Edit these values directly for your test runs:</p> <pre><code>active:\n  # Dataset\n  dataset_dir: /path/to/your/dataset\n  test_name: null  # Auto-generated if null\n\n  # API Configuration\n  api_version: v1  # v1 or v2\n  pdf_split_page_count: null  # V2 only: pages per chunk (null = default 32)\n\n  # Infrastructure\n  hostname: localhost\n  readiness_timeout: 600\n  profiles: [retrieval]\n\n  # Runtime\n  sparse: true\n  gpu_search: false\n  embedding_model: auto\n\n  # Extraction\n  extract_text: true\n  extract_tables: true\n  extract_charts: true\n  extract_images: false\n  extract_infographics: true\n  text_depth: page\n  table_output_format: markdown\n\n  # Pipeline (optional steps)\n  enable_caption: false\n  enable_split: false\n  split_chunk_size: 1024\n  split_chunk_overlap: 150\n\n  # Storage\n  spill_dir: /tmp/spill\n  artifacts_dir: null\n  collection_name: null\n</code></pre>"},{"location":"extraction/benchmarking/#pre-configured-datasets","title":"Pre-Configured Datasets","text":"<p>Each dataset includes its path, extraction settings, and recall evaluator in one place:</p> <pre><code>datasets:\n  bo767:\n    path: /raid/jioffe/bo767\n    extract_text: true\n    extract_tables: true\n    extract_charts: true\n    extract_images: false\n    extract_infographics: false\n    recall_dataset: bo767  # Evaluator for recall testing\n\n  bo20:\n    path: /raid/jioffe/bo20\n    extract_text: true\n    extract_tables: true\n    extract_charts: true\n    extract_images: true\n    extract_infographics: false\n    recall_dataset: null  # bo20 does not have recall\n\n  earnings:\n    path: /raid/jioffe/earnings_conusulting\n    extract_text: true\n    extract_tables: true\n    extract_charts: true\n    extract_images: false\n    extract_infographics: false\n    recall_dataset: earnings  # Evaluator for recall testing\n</code></pre> <p>Automatic Configuration: When you use <code>--dataset=bo767</code>, the framework automatically: - Sets the dataset path - Applies the correct extraction settings (text, tables, charts, images, infographics) - Configures the recall evaluator (if applicable)</p> <p>Usage: <pre><code># Single dataset - configs applied automatically\nuv run nv-ingest-harness-run --case=e2e --dataset=bo767\n\n# Multiple datasets (sweeping) - each gets its own config\nuv run nv-ingest-harness-run --case=e2e --dataset=bo767,earnings,bo20\n\n# Custom path still works (uses active section config)\nuv run nv-ingest-harness-run --case=e2e --dataset=/custom/path\n</code></pre></p> <p>Dataset Extraction Settings:</p> Dataset Text Tables Charts Images Infographics Recall <code>bo767</code> \u2705 \u2705 \u2705 \u274c \u274c \u2705 <code>earnings</code> \u2705 \u2705 \u2705 \u274c \u274c \u2705 <code>bo20</code> \u2705 \u2705 \u2705 \u2705 \u274c \u274c <code>financebench</code> \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 <code>single</code> \u2705 \u2705 \u2705 \u2705 \u2705 \u274c"},{"location":"extraction/benchmarking/#configuration-precedence","title":"Configuration Precedence","text":"<p>Settings are applied in order of priority:</p> <p>Environment variables &gt; Dataset-specific config (path + extraction + recall_dataset) &gt; YAML active config</p> <p>Note: CLI arguments are only used for runtime decisions (which test to run, which dataset, execution mode). All configuration values come from YAML or environment variables.</p> <p>Example: <pre><code># YAML active section has api_version: v1\n# Dataset bo767 has extract_images: false\n# Override via environment variable (highest priority)\nEXTRACT_IMAGES=true API_VERSION=v2 uv run nv-ingest-harness-run --case=e2e --dataset=bo767\n# Result: Uses bo767 path, but extract_images=true (env override) and api_version=v2 (env override)\n</code></pre></p> <p>Precedence Details: 1. Environment variables - Highest priority, useful for CI/CD overrides 2. Dataset-specific config - Applied automatically when using <code>--dataset=&lt;name&gt;</code>    - Includes: path, extraction settings, recall_dataset    - Only applies if dataset is defined in <code>datasets</code> section 3. YAML active config - Base configuration, used as fallback</p>"},{"location":"extraction/benchmarking/#configuration-options-reference","title":"Configuration Options Reference","text":""},{"location":"extraction/benchmarking/#core-options","title":"Core Options","text":"<ul> <li><code>dataset_dir</code> (string, required): Path to dataset directory</li> <li><code>test_name</code> (string): Test identifier (auto-generated if null)</li> <li><code>api_version</code> (string): API version - <code>v1</code> or <code>v2</code></li> <li><code>pdf_split_page_count</code> (integer): PDF splitting page count (V2 only, 1-128)</li> </ul>"},{"location":"extraction/benchmarking/#extraction-options","title":"Extraction Options","text":"<ul> <li><code>extract_text</code>, <code>extract_tables</code>, <code>extract_charts</code>, <code>extract_images</code>, <code>extract_infographics</code> (boolean): Content extraction toggles</li> <li><code>text_depth</code> (string): Text extraction granularity - <code>page</code>, <code>document</code>, <code>block</code>, <code>line</code>, etc.</li> <li><code>table_output_format</code> (string): Table output format - <code>markdown</code>, <code>html</code>, <code>latex</code>, <code>pseudo_markdown</code>, <code>simple</code></li> </ul>"},{"location":"extraction/benchmarking/#pipeline-options","title":"Pipeline Options","text":"<ul> <li><code>enable_caption</code> (boolean): Enable image captioning (requires the VLM profile to be running)</li> <li><code>caption_prompt</code> (string): Override the user prompt sent to the captioning VLM. Defaults to <code>\"Caption the content of this image:\"</code>.</li> <li><code>caption_reasoning</code> (boolean): Enable reasoning mode for the captioning VLM. <code>True</code> enables reasoning, <code>False</code> disables reasoning. Defaults to <code>null</code> (service default, typically disabled).</li> <li><code>enable_split</code> (boolean): Enable text chunking</li> <li><code>split_chunk_size</code> (integer): Chunk size for text splitting</li> <li><code>split_chunk_overlap</code> (integer): Overlap for text splitting</li> </ul>"},{"location":"extraction/benchmarking/#infrastructure-options","title":"Infrastructure Options","text":"<ul> <li><code>hostname</code> (string): Service hostname</li> <li><code>readiness_timeout</code> (integer): Docker startup timeout in seconds</li> <li><code>profiles</code> (list): Docker compose profiles</li> </ul>"},{"location":"extraction/benchmarking/#runtime-options","title":"Runtime Options","text":"<ul> <li><code>sparse</code> (boolean): Use sparse embeddings</li> <li><code>gpu_search</code> (boolean): Use GPU for search</li> <li><code>embedding_model</code> (string): Embedding model name (<code>auto</code> for auto-detection)</li> <li><code>llm_summarization_model</code> (string): LLM model for summarization (used by <code>e2e_with_llm_summary</code>)</li> </ul>"},{"location":"extraction/benchmarking/#storage-options","title":"Storage Options","text":"<ul> <li><code>spill_dir</code> (string): Temporary processing directory</li> <li><code>artifacts_dir</code> (string): Test output directory (auto-generated if null)</li> <li><code>collection_name</code> (string): Milvus collection name (auto-generated as <code>{test_name}_multimodal</code> if null, deterministic - no timestamp)</li> </ul>"},{"location":"extraction/benchmarking/#valid-configuration-values","title":"Valid Configuration Values","text":"<p>text_depth: <code>block</code>, <code>body</code>, <code>document</code>, <code>header</code>, <code>line</code>, <code>nearby_block</code>, <code>other</code>, <code>page</code>, <code>span</code></p> <p>table_output_format: <code>html</code>, <code>image</code>, <code>latex</code>, <code>markdown</code>, <code>pseudo_markdown</code>, <code>simple</code></p> <p>api_version: <code>v1</code>, <code>v2</code></p> <p>Configuration is validated on load with helpful error messages.</p>"},{"location":"extraction/benchmarking/#running-tests","title":"Running Tests","text":""},{"location":"extraction/benchmarking/#basic-usage","title":"Basic Usage","text":"<pre><code># Run with default YAML configuration (assumes services are running)\nuv run nv-ingest-harness-run --case=e2e --dataset=bo767\n\n# With document-level analysis\nuv run nv-ingest-harness-run --case=e2e --dataset=bo767 --doc-analysis\n\n# With managed infrastructure (starts/stops services)\nuv run nv-ingest-harness-run --case=e2e --dataset=bo767 --managed\n</code></pre>"},{"location":"extraction/benchmarking/#dataset-sweeping","title":"Dataset Sweeping","text":"<p>Run multiple datasets in a single command - each dataset automatically gets its native extraction configuration:</p> <pre><code># Sweep multiple datasets\nuv run nv-ingest-harness-run --case=e2e --dataset=bo767,earnings,bo20\n\n# Each dataset runs sequentially with its own:\n# - Extraction settings (from dataset config)\n# - Artifact directory (timestamped per dataset)\n# - Results summary at the end\n\n# With managed infrastructure (services start once, shared across all datasets)\nuv run nv-ingest-harness-run --case=e2e --dataset=bo767,earnings,bo20 --managed\n\n# E2E+Recall sweep (each dataset ingests then evaluates recall)\nuv run nv-ingest-harness-run --case=e2e_recall --dataset=bo767,earnings\n\n# Recall-only sweep (evaluates existing collections)\nuv run nv-ingest-harness-run --case=recall --dataset=bo767,earnings\n</code></pre> <p>Sweep Behavior: - Services start once (if <code>--managed</code>) before the sweep - Each dataset gets its own artifact directory - Each dataset automatically applies its extraction config from <code>datasets</code> section - Summary printed at end showing success/failure for each dataset - Services stop once at end (unless <code>--keep-up</code>)</p>"},{"location":"extraction/benchmarking/#using-environment-variables","title":"Using Environment Variables","text":"<pre><code># Override via environment (useful for CI/CD)\nAPI_VERSION=v2 EXTRACT_TABLES=false uv run nv-ingest-harness-run --case=e2e\n\n# Temporary changes without editing YAML\nDATASET_DIR=/custom/path uv run nv-ingest-harness-run --case=e2e\n</code></pre>"},{"location":"extraction/benchmarking/#test-scenarios","title":"Test Scenarios","text":""},{"location":"extraction/benchmarking/#available-tests","title":"Available Tests","text":"Name Description Configuration Needed Status <code>e2e</code> Dataset-agnostic E2E ingestion <code>active</code> section only \u2705 Primary (YAML config) <code>e2e_with_llm_summary</code> E2E with LLM summarization via UDF <code>active</code> section only \u2705 Available (YAML config) <code>recall</code> Recall evaluation against existing collections <code>active</code> + <code>recall</code> sections \u2705 Available (YAML config) <code>e2e_recall</code> Fresh ingestion + recall evaluation <code>active</code> + <code>recall</code> sections \u2705 Available (YAML config) <p>Note: Legacy test cases (<code>dc20_e2e</code>, <code>dc20_v2_e2e</code>) have been moved to <code>scripts/private_local</code>.</p>"},{"location":"extraction/benchmarking/#configuration-synergy","title":"Configuration Synergy","text":"<p>For E2E-only users: - Only configure <code>active</code> section - <code>collection_name</code> in active: auto-generates from <code>test_name</code> or dataset basename if <code>null</code> (deterministic, no timestamp) - Collection name pattern: <code>{test_name}_multimodal</code> (e.g., <code>bo767_multimodal</code>, <code>earnings_consulting_multimodal</code>) - <code>recall</code> section is optional (not used unless running recall tests) - Note: You can run <code>recall</code> later against the same collection created by <code>e2e</code></p> <p>For Recall-only users: - Configure <code>active</code> section: <code>hostname</code>, <code>sparse</code>, <code>gpu_search</code>, etc. (for evaluation) - Configure <code>recall</code> section: <code>recall_dataset</code> (required) - Set <code>test_name</code> in active to match your existing collection (collection must be <code>{test_name}_multimodal</code>) - <code>collection_name</code> in active is ignored (recall generates <code>{test_name}_multimodal</code>)</p> <p>For E2E+Recall users: - Configure <code>active</code> section: <code>dataset_dir</code>, <code>test_name</code>, extraction settings, etc. - Configure <code>recall</code> section: <code>recall_dataset</code> (required) - Collection naming: e2e_recall automatically creates <code>{test_name}_multimodal</code> collection - <code>collection_name</code> in active is ignored (e2e_recall forces <code>{test_name}_multimodal</code> pattern)</p>"},{"location":"extraction/benchmarking/#example-configurations","title":"Example Configurations","text":"<p>V2 API with PDF Splitting: <pre><code># Edit test_configs.yaml active section:\nactive:\n  api_version: v2\n  pdf_split_page_count: 32\n  extract_text: true\n  extract_tables: true\n  extract_charts: true\n</code></pre></p> <p>Text-Only Processing: <pre><code>active:\n  extract_text: true\n  extract_tables: false\n  extract_charts: false\n  extract_images: false\n  extract_infographics: false\n</code></pre></p> <p>RAG with Text Chunking: <pre><code>active:\n  enable_split: true\n  split_chunk_size: 1024\n  split_chunk_overlap: 150\n</code></pre></p> <p>Multimodal with Image Extraction: <pre><code>active:\n  extract_text: true\n  extract_tables: true\n  extract_charts: true\n  extract_images: true\n  extract_infographics: true\n  enable_caption: true\n</code></pre></p>"},{"location":"extraction/benchmarking/#recall-testing","title":"Recall Testing","text":"<p>Recall testing evaluates retrieval accuracy against ground truth query sets. Two test cases are available:</p>"},{"location":"extraction/benchmarking/#test-cases","title":"Test Cases","text":"<p><code>recall</code> - Recall-only evaluation against existing collections: - Skips ingestion (assumes collections already exist) - Loads existing collections from Milvus - Evaluates recall using multimodal queries (all datasets are multimodal-only) - Supports reranker comparison (no reranker, with reranker, or reranker-only)</p> <p><code>e2e_recall</code> - Fresh ingestion + recall evaluation: - Performs full ingestion pipeline - Creates multimodal collection during ingestion - Evaluates recall immediately after ingestion - Combines ingestion metrics with recall metrics</p>"},{"location":"extraction/benchmarking/#reranker-configuration","title":"Reranker Configuration","text":"<p>Three modes via <code>reranker_mode</code> setting:</p> <ol> <li>No reranker (default): <code>reranker_mode: none</code></li> <li> <p>Runs evaluation without reranker only</p> </li> <li> <p>Both modes: <code>reranker_mode: both</code></p> </li> <li>Runs evaluation twice: once without reranker, once with reranker</li> <li> <p>Useful for comparing reranker impact</p> </li> <li> <p>Reranker only: <code>reranker_mode: with</code></p> </li> <li>Runs evaluation with reranker only</li> <li>Faster when you only need reranked results</li> </ol>"},{"location":"extraction/benchmarking/#collection-naming","title":"Collection Naming","text":"<p>Deterministic Collection Names (No Timestamps)</p> <p>All test cases use deterministic collection names (no timestamps) to enable: - Reusing collections across test runs - Running recall evaluation after e2e ingestion - Consistent collection naming patterns</p> <p>Collection Name Patterns:</p> <p>All test cases use the same consistent pattern: <code>{test_name}_multimodal</code></p> Test Case Pattern Example <code>e2e</code> <code>{test_name}_multimodal</code> <code>bo767_multimodal</code> <code>e2e_with_llm_summary</code> <code>{test_name}_multimodal</code> <code>bo767_multimodal</code> <code>e2e_recall</code> <code>{test_name}_multimodal</code> <code>bo767_multimodal</code> <code>recall</code> <code>{test_name}_multimodal</code> <code>bo767_multimodal</code> <p>Benefits: - \u2705 Run <code>e2e</code> then <code>recall</code> separately - they use the same collection - \u2705 Consistent naming across all test cases - \u2705 Deterministic names (no timestamps) enable collection reuse</p> <p>Recall Collections: - A single multimodal collection is created for recall evaluation - Pattern: <code>{test_name}_multimodal</code> - Example: <code>bo767_multimodal</code> - All datasets evaluate against this multimodal collection (no modality-specific collections)</p> <p>Note: Artifact directories still use timestamps for tracking over time (e.g., <code>bo767_20251106_180859_UTC</code>), but collection names are deterministic.</p>"},{"location":"extraction/benchmarking/#multimodal-only-evaluation","title":"Multimodal-Only Evaluation","text":"<p>All datasets use multimodal-only evaluation: - Ground truth queries contain all content types (text, tables, charts) - Single collection contains all extracted content types - Simplified evaluation interface (no modality filtering)</p>"},{"location":"extraction/benchmarking/#ground-truth-files","title":"Ground Truth Files","text":"<p>bo767 dataset: - Ground truth file: <code>bo767_query_gt.csv</code> (consolidated multimodal queries) - Located in repo <code>data/</code> directory - Default <code>ground_truth_dir: null</code> automatically uses <code>data/</code> directory - Custom path can be specified via <code>ground_truth_dir</code> config</p> <p>Other datasets (finance_bench, earnings, audio): - Ground truth files must be obtained separately (not in public repo) - Set <code>ground_truth_dir</code> to point to your ground truth directory - Dataset-specific evaluators are extensible (see <code>recall_utils.py</code>)</p>"},{"location":"extraction/benchmarking/#configuration","title":"Configuration","text":"<p>Edit the <code>recall</code> section in <code>test_configs.yaml</code>:</p> <pre><code>recall:\n  # Reranker configuration\n  reranker_mode: none  # Options: \"none\", \"with\", \"both\"\n\n  # Recall evaluation settings\n  recall_top_k: 10\n  ground_truth_dir: null  # null = use repo data/ directory\n  recall_dataset: bo767  # Required: must be explicitly set (bo767, finance_bench, earnings, audio)\n</code></pre>"},{"location":"extraction/benchmarking/#usage-examples","title":"Usage Examples","text":"<p>Recall-only (existing collections): <pre><code># Evaluate existing bo767 collections (no reranker)\n# recall_dataset automatically set from dataset config\nuv run nv-ingest-harness-run --case=recall --dataset=bo767\n\n# With reranker only (set reranker_mode in YAML recall section)\nuv run nv-ingest-harness-run --case=recall --dataset=bo767\n\n# Sweep multiple datasets for recall evaluation\nuv run nv-ingest-harness-run --case=recall --dataset=bo767,earnings\n</code></pre></p> <p>E2E + Recall (fresh ingestion): <pre><code># Fresh ingestion with recall evaluation\n# recall_dataset automatically set from dataset config\nuv run nv-ingest-harness-run --case=e2e_recall --dataset=bo767\n\n# Sweep multiple datasets (each ingests then evaluates)\nuv run nv-ingest-harness-run --case=e2e_recall --dataset=bo767,earnings\n</code></pre></p> <p>Dataset configuration: - Dataset path: Automatically set from <code>datasets</code> section when using <code>--dataset=&lt;name&gt;</code> - Extraction settings: Automatically applied from <code>datasets</code> section - recall_dataset: Automatically set from <code>datasets</code> section (e.g., <code>bo767</code>, <code>earnings</code>, <code>finance_bench</code>)   - Can be overridden via environment variable: <code>RECALL_DATASET=bo767</code> - test_name: Auto-generated from dataset name or basename of path (can set in YAML <code>active</code> section) - Collection naming: <code>{test_name}_multimodal</code> (automatically generated for recall cases) - All datasets evaluate against the same <code>{test_name}_multimodal</code> collection (multimodal-only)</p>"},{"location":"extraction/benchmarking/#output","title":"Output","text":"<p>Recall results are included in <code>results.json</code>: <pre><code>{\n  \"recall_results\": {\n    \"no_reranker\": {\n      \"1\": 0.554,\n      \"3\": 0.746,\n      \"5\": 0.807,\n      \"10\": 0.857\n    },\n    \"with_reranker\": {\n      \"1\": 0.601,\n      \"3\": 0.781,\n      \"5\": 0.832,\n      \"10\": 0.874\n    }\n  }\n}\n</code></pre></p> <p>Metrics are also logged via <code>kv_event_log()</code>: - <code>recall_multimodal_@{k}_no_reranker</code> - <code>recall_multimodal_@{k}_with_reranker</code> - <code>recall_eval_time_s_no_reranker</code> - <code>recall_eval_time_s_with_reranker</code></p>"},{"location":"extraction/benchmarking/#sweeping-parameters","title":"Sweeping Parameters","text":""},{"location":"extraction/benchmarking/#dataset-sweeping-recommended","title":"Dataset Sweeping (Recommended)","text":"<p>The easiest way to test multiple datasets is using dataset sweeping:</p> <pre><code># Test multiple datasets - each gets its native config automatically\nuv run nv-ingest-harness-run --case=e2e --dataset=bo767,earnings,bo20\n\n# Each dataset runs with its pre-configured extraction settings\n# Results are organized in separate artifact directories\n</code></pre>"},{"location":"extraction/benchmarking/#parameter-sweeping","title":"Parameter Sweeping","text":"<p>To sweep through different parameter values:</p> <ol> <li>Edit <code>test_configs.yaml</code> - Update values in the <code>active</code> section</li> <li>Run the test: <code>uv run nv-ingest-harness-run --case=e2e --dataset=&lt;name&gt;</code></li> <li>Analyze results in <code>artifacts/&lt;test_name&gt;_&lt;timestamp&gt;/</code></li> <li>Repeat steps 1-3 for next parameter combination</li> </ol> <p>Example parameter sweep workflow: <pre><code># Test 1: Baseline V1\nvim test_configs.yaml  # Set: api_version=v1, extract_tables=true\nuv run nv-ingest-harness-run --case=e2e --dataset=bo767\n\n# Test 2: V2 with 32-page splitting\nvim test_configs.yaml  # Set: api_version=v2, pdf_split_page_count=32\nuv run nv-ingest-harness-run --case=e2e --dataset=bo767\n\n# Test 3: V2 with 8-page splitting\nvim test_configs.yaml  # Set: pdf_split_page_count=8\nuv run nv-ingest-harness-run --case=e2e --dataset=bo767\n\n# Test 4: Tables disabled (override via env var)\nEXTRACT_TABLES=false uv run nv-ingest-harness-run --case=e2e --dataset=bo767\n</code></pre></p> <p>Note: Each test run creates a new timestamped artifact directory, so you can compare results across sweeps.</p>"},{"location":"extraction/benchmarking/#execution-modes","title":"Execution Modes","text":""},{"location":"extraction/benchmarking/#attach-mode-default","title":"Attach Mode (Default)","text":"<pre><code>uv run nv-ingest-harness-run --case=e2e --dataset=bo767\n</code></pre> <ul> <li>Default behavior: Assumes services are already running</li> <li>Runs test case only (no service management)</li> <li>Faster for iterative testing</li> <li>Use when Docker services are already up</li> <li><code>--no-build</code> and <code>--keep-up</code> flags are ignored in attach mode</li> </ul>"},{"location":"extraction/benchmarking/#managed-mode","title":"Managed Mode","text":"<pre><code>uv run nv-ingest-harness-run --case=e2e --dataset=bo767 --managed\n</code></pre> <ul> <li>Starts Docker services automatically</li> <li>Waits for service readiness (configurable timeout)</li> <li>Runs test case</li> <li>Collects artifacts</li> <li>Stops services after test (unless <code>--keep-up</code>)</li> </ul> <p>Managed mode options: <pre><code># Skip Docker image rebuild (faster startup)\nuv run nv-ingest-harness-run --case=e2e --dataset=bo767 --managed --no-build\n\n# Keep services running after test (useful for multi-test scenarios)\nuv run nv-ingest-harness-run --case=e2e --dataset=bo767 --managed --keep-up\n</code></pre></p>"},{"location":"extraction/benchmarking/#artifacts-and-logging","title":"Artifacts and Logging","text":"<p>All test outputs are collected in timestamped directories:</p> <pre><code>tools/harness/artifacts/&lt;test_name&gt;_&lt;timestamp&gt;_UTC/\n\u251c\u2500\u2500 results.json         # Consolidated test metadata and results\n\u251c\u2500\u2500 stdout.txt          # Complete test output\n\u2514\u2500\u2500 e2e.json            # Structured metrics and events\n</code></pre> <p>Note: Artifact directories use timestamps for tracking test runs over time, while collection names are deterministic (no timestamps) to enable collection reuse and recall evaluation.</p>"},{"location":"extraction/benchmarking/#results-structure","title":"Results Structure","text":"<p><code>results.json</code> contains: - Runner metadata: case name, timestamp, git commit, infrastructure mode - Test configuration: API version, extraction settings, dataset info - Test results: chunks created, timing, performance metrics</p>"},{"location":"extraction/benchmarking/#document-analysis","title":"Document Analysis","text":"<p>Enable per-document element breakdown:</p> <pre><code>uv run nv-ingest-harness-run --case=e2e --doc-analysis\n</code></pre> <p>Sample Output: <pre><code>Document Analysis:\n  document1.pdf: 44 elements (text: 15, tables: 13, charts: 15, images: 0, infographics: 1)\n  document2.pdf: 14 elements (text: 9, tables: 0, charts: 4, images: 0, infographics: 1)\n</code></pre></p> <p>This provides: - Element counts by type for each document - Useful for understanding dataset characteristics - Helps identify processing bottlenecks - Validates extraction completeness</p>"},{"location":"extraction/benchmarking/#architecture","title":"Architecture","text":""},{"location":"extraction/benchmarking/#framework-components","title":"Framework Components","text":"<p>1. Configuration Layer - <code>test_configs.yaml</code> - Structured configuration file   - Active test configuration (edit directly)   - Dataset shortcuts for quick access - <code>src/nv_ingest_harness/config.py</code> - Configuration management   - YAML loading and parsing   - Type-safe config dataclass   - Validation logic with helpful errors   - Environment variable override support</p> <p>2. Test Runner - <code>src/nv_ingest_harness/cli/run.py</code> - Main orchestration   - Configuration loading with precedence chain   - Docker service management (managed mode)   - Test case execution with config injection   - Artifact collection and consolidation</p> <p>3. Test Cases - <code>src/nv_ingest_harness/cases/e2e.py</code> - Primary E2E test (\u2705 YAML-based)   - Accepts config object directly   - Type-safe parameter access   - Full pipeline validation (extract \u2192 embed \u2192 VDB \u2192 retrieval)   - Transparent configuration logging - <code>cases/e2e_with_llm_summary.py</code> - E2E with LLM (\u2705 YAML-based)   - Adds UDF-based LLM summarization   - Same config-based architecture as e2e.py - <code>src/nv_ingest_harness/cases/recall.py</code> - Recall evaluation (\u2705 YAML-based)   - Evaluates retrieval accuracy against existing collections   - Requires <code>recall_dataset</code> in config (from dataset config or env var)   - Supports reranker comparison modes (none, with, both)   - Multimodal-only evaluation against <code>{test_name}_multimodal</code> collection - <code>src/nv_ingest_harness/cases/e2e_recall.py</code> - E2E + Recall (\u2705 YAML-based)   - Combines ingestion (via e2e.py) with recall evaluation (via recall.py)   - Automatically creates collection during ingestion   - Requires <code>recall_dataset</code> in config (from dataset config or env var)   - Merges ingestion and recall metrics in results</p> <p>4. Shared Utilities - <code>src/nv_ingest_harness/utils/interact.py</code> - Common testing utilities   - <code>embed_info()</code> - Embedding model detection   - <code>milvus_chunks()</code> - Vector database statistics   - <code>segment_results()</code> - Result categorization by type   - <code>kv_event_log()</code> - Structured logging   - <code>pdf_page_count()</code> - Dataset page counting</p>"},{"location":"extraction/benchmarking/#configuration-flow","title":"Configuration Flow","text":"<pre><code>test_configs.yaml \u2192 load_config() \u2192 TestConfig object \u2192 test case\n    (active +        (applies          (validated,\n     datasets)        dataset config)    type-safe)\n         \u2191                    \u2191\n    Env overrides      Dataset configs\n    (highest)          (auto-applied)\n</code></pre> <p>Configuration Loading: 1. Start with <code>active</code> section from YAML 2. If <code>--dataset=&lt;name&gt;</code> matches a configured dataset:    - Apply dataset path    - Apply dataset extraction settings    - Apply dataset <code>recall_dataset</code> (if set) 3. Apply environment variable overrides (if any) 4. Validate and create <code>TestConfig</code> object</p> <p>All test cases receive a validated <code>TestConfig</code> object with typed fields, eliminating string parsing errors.</p>"},{"location":"extraction/benchmarking/#development-guide","title":"Development Guide","text":""},{"location":"extraction/benchmarking/#adding-new-test-cases","title":"Adding New Test Cases","text":"<ol> <li> <p>Create test script in <code>cases/</code> directory</p> </li> <li> <p>Accept config parameter:    <pre><code>def main(config, log_path: str = \"test_results\") -&gt; int:\n    \"\"\"\n    Test case entry point.\n\n    Args:\n        config: TestConfig object with all settings\n        log_path: Path for structured logging\n\n    Returns:\n        Exit code (0 = success)\n    \"\"\"\n    # Access config directly (type-safe)\n    data_dir = config.dataset_dir\n    api_version = config.api_version\n    extract_text = config.extract_text\n    # ...\n</code></pre></p> </li> <li> <p>Add transparent logging:    <pre><code>print(\"=== Test Configuration ===\")\nprint(f\"Dataset: {config.dataset_dir}\")\nprint(f\"API: {config.api_version}\")\nprint(f\"Extract: text={config.extract_text}, tables={config.extract_tables}\")\nprint(\"=\" * 60)\n</code></pre></p> </li> <li> <p>Use structured logging:    <pre><code>from interact import kv_event_log\n\nkv_event_log(\"ingestion_time_s\", elapsed_time, log_path)\nkv_event_log(\"text_chunks\", num_text_chunks, log_path)\n</code></pre></p> </li> <li> <p>Register case in <code>run.py</code>:    <pre><code>CASES = [\"e2e\", \"e2e_with_llm_summary\", \"your_new_case\"]\n</code></pre></p> </li> </ol>"},{"location":"extraction/benchmarking/#extending-configuration","title":"Extending Configuration","text":"<p>To add new configurable parameters:</p> <ol> <li> <p>Add to <code>TestConfig</code> dataclass in <code>config.py</code>:    <pre><code>@dataclass\nclass TestConfig:\n    # ... existing fields\n    new_param: bool = False  # Add with type and default\n</code></pre></p> </li> <li> <p>Add to YAML <code>active</code> section:    <pre><code>active:\n  # ... existing config\n  new_param: false  # Match Python default\n</code></pre></p> </li> <li> <p>Add environment variable mapping in <code>config.py</code> (if needed):    <pre><code>env_mapping = {\n    # ... existing mappings\n    \"NEW_PARAM\": (\"new_param\", parse_bool),\n}\n</code></pre></p> </li> <li> <p>Add validation (if needed) in <code>TestConfig.validate()</code>:    <pre><code>def validate(self) -&gt; List[str]:\n    errors = []\n    # ... existing validation\n    if self.new_param and self.some_other_field is None:\n        errors.append(\"new_param requires some_other_field to be set\")\n    return errors\n</code></pre></p> </li> <li> <p>Update this README with parameter description</p> </li> </ol>"},{"location":"extraction/benchmarking/#testing-different-datasets","title":"Testing Different Datasets","text":"<p>The framework is dataset-agnostic and supports multiple approaches:</p> <p>Option 1: Use pre-configured dataset (Recommended) <pre><code># Dataset configs automatically applied\nuv run nv-ingest-harness-run --case=e2e --dataset=bo767\n</code></pre></p> <p>Option 2: Add new dataset to YAML <pre><code>datasets:\n  my_dataset:\n    path: /path/to/your/dataset\n    extract_text: true\n    extract_tables: true\n    extract_charts: true\n    extract_images: false\n    extract_infographics: false\n    recall_dataset: null  # or set to evaluator name if applicable\n</code></pre> Then use: <code>uv run nv-ingest-harness-run --case=e2e --dataset=my_dataset</code></p> <p>Option 3: Use custom path (uses active section config) <pre><code>uv run nv-ingest-harness-run --case=e2e --dataset=/path/to/your/dataset\n</code></pre></p> <p>Option 4: Environment variable override <pre><code># Override specific settings via env vars\nEXTRACT_IMAGES=true uv run nv-ingest-harness-run --case=e2e --dataset=bo767\n</code></pre></p> <p>Best Practice: For repeated testing, add your dataset to the <code>datasets</code> section with its native extraction settings. This ensures consistent configuration and enables dataset sweeping.</p>"},{"location":"extraction/benchmarking/#additional-resources","title":"Additional Resources","text":"<ul> <li>Configuration: See <code>config.py</code> for complete field list and validation logic</li> <li>Test utilities: See <code>interact.py</code> for shared helper functions  </li> <li>Docker setup: See project root README for service management commands</li> <li>API documentation: See <code>docs/</code> for API version differences</li> </ul> <p>The framework prioritizes clarity, type safety, and validation to support reliable testing of nv-ingest pipelines.</p>"},{"location":"extraction/chunking/","title":"Split Documents","text":"<p>Splitting, also known as chunking, breaks large documents or text into smaller, manageable sections to improve retrieval efficiency.  After chunking, only the most relevant pieces of information are retrieved for a given query.  Chunking also prevents text from exceeding the context window of the embedding model.</p> <p>There are two ways that NV Ingest chunks text:</p> <ul> <li>By using the <code>text_depth</code> parameter in the <code>extraction</code> task.</li> <li>Token-based splitting by using the <code>split</code> task.</li> </ul> <p>Warning</p> <p>NeMo Retriever extraction is designed to process language and language-length strings. If you submit a document that contains extremely long, or non-language text strings, such as a DNA sequence, errors or unexpected results occur.</p>"},{"location":"extraction/chunking/#extraction-text-depth","title":"Extraction Text Depth","text":"<p>You can use the <code>text_depth</code> parameter to specify how extracted text is chunked together by the extractor.  For example, the following code chunks the document text by page, for document types that have pages.</p> <pre><code>ingestor = ingestor.extract(\n    extract_text=True,\n    text_depth=\"page\"\n)\n</code></pre> <p>The following table contains the <code>text_depth</code> parameter values.</p> Value Description <code>document</code> Doesn't perform any splitting, and returns the full document as one chunk. <code>page</code> Returns a single chunk of text for each page. <p>For most documents, we recommend that you set <code>text_depth</code> to <code>page</code>, because this tends to give the best performance for retrieval.  However, in some cases, such as with <code>.txt</code> documents, there aren't any page breaks to split on. </p> <p>If you want chunks smaller than <code>page</code>, use token-based splitting as described in the following section.</p>"},{"location":"extraction/chunking/#token-based-splitting","title":"Token-Based Splitting","text":"<p>The <code>split</code> task uses a tokenizer to count the number of tokens in the document,  and splits the document based on the desired maximum chunk size and chunk overlap.  We recommend that you use the <code>meta-llama/Llama-3.2-1B</code> tokenizer,  because it's the same tokenizer as the llama-3.2 embedding model that we use for embedding. However, you can use any tokenizer from any HuggingFace model that includes a tokenizer file.</p> <p>Use the <code>split</code> method to chunk large documents as shown in the following code.</p> <p>Note</p> <p>The default tokenizer (<code>meta-llama/Llama-3.2-1B</code>) requires a Hugging Face access token. You must set <code>hf_access_token\": \"hf_***</code> to authenticate.</p> <pre><code>ingestor = ingestor.split(\n    tokenizer=\"meta-llama/Llama-3.2-1B\",\n    chunk_size=1024,\n    chunk_overlap=150,\n    params={\"split_source_types\": [\"text\", \"PDF\"], \"hf_access_token\": \"hf_***\"}\n)\n</code></pre> <p>To use a different tokenizer, such as <code>intfloat/e5-large-unsupervised</code>, you can modify the <code>split</code> call as shown following.</p> <pre><code>ingestor = ingestor.split(\n    tokenizer=\"intfloat/e5-large-unsupervised\",\n    chunk_size=1024,\n    chunk_overlap=150,\n    params={\"split_source_types\": [\"text\", \"PDF\"], \"hf_access_token\": \"hf_***\"}\n)\n</code></pre>"},{"location":"extraction/chunking/#split-parameters","title":"Split Parameters","text":"<p>The following table contains the <code>split</code> parameters.</p> Parameter Description Default <code>tokenizer</code> HuggingFace Tokenizer identifier or path. <code>meta-llama/Llama-3.2-1B</code> <code>chunk_size</code> Maximum number of tokens per chunk. <code>1024</code> <code>chunk_overlap</code> Number of tokens to overlap between chunks. <code>150</code> <code>params</code> A sub-dictionary that can contain <code>split_source_types</code> and <code>hf_access_token</code> <code>{}</code> <code>hf_access_token</code> Your Hugging Face access token. \u2014 <code>split_source_types</code> The source types to split on (only splits on text by default). \u2014"},{"location":"extraction/chunking/#pre-download-the-tokenizer","title":"Pre-download the Tokenizer","text":"<p>By default, the NV Ingest container comes with the <code>meta-llama/Llama-3.2-1B</code> tokenizer pre-downloaded  so that it doesn't have to download a tokenizer at runtime. If you are building the container yourself and want to pre-download this model, do the following:</p> <ul> <li>Review the license agreement.</li> <li>Request access.</li> <li>Set the <code>DOWNLOAD_LLAMA_TOKENIZER</code> environment variable to <code>True</code></li> <li>Set the <code>HF_ACCESS_TOKEN</code> environment variable to your HuggingFace access token.</li> </ul>"},{"location":"extraction/chunking/#related-topics","title":"Related Topics","text":"<ul> <li>Use the Python API</li> <li>NeMo Retriever Extraction V2 API Guide</li> <li>Environment Variables</li> </ul>"},{"location":"extraction/content-metadata/","title":"Metadata Reference for NeMo Retriever Extraction","text":"<p>This documentation contains the reference for the metadata used in NeMo Retriever extraction.  The definitions used in this documentation are the following:</p> <ul> <li>Source \u2014 The file that is ingested, and from which content and metadata is extracted.</li> <li>Content \u2014 Data extracted from a source, such as text or an image.</li> </ul> <p>Metadata can be extracted from a source or content, or generated by using models, heuristics, or other methods.</p> <p>Note</p> <p>NeMo Retriever extraction is also known as NVIDIA Ingest and nv-ingest.</p>"},{"location":"extraction/content-metadata/#source-file-metadata","title":"Source File Metadata","text":"<p>The following is the metadata for source files.</p> Field Description Method Source Name The name of the source file. Extracted Source ID The ID of the source file. Extracted Source location The URL, URI, or pointer to the storage location of the source file. \u2014 Source Type The type of the source file, such as pdf, docx, pptx, or txt. Extracted Collection ID The ID of the collection in which the source is contained. \u2014 Date Created The date the source was created. Extracted Last Modified The date the source was last modified. Extracted Partition ID The offset of this data fragment within a larger set of fragments. Generated Access Level The role-based access control for the source. \u2014 Summary A summary of the source. (Not yet implemented.) Generated"},{"location":"extraction/content-metadata/#content-metadata","title":"Content Metadata","text":"<p>The following is the metadata for content.  These fields apply to all content types including text, images, and tables.</p> Field Description Method Type The type of the content. Text, Image, Structured, Table, or Chart. Generated Subtype The type of the content for structured data types, such as table or chart. \u2014 Content Content extracted from the source. Extracted Description A text description of the content object. Generated Page # The page # of the content in the source. Prior to 26.1.2, this field was 0-indexed. Beginning with 26.1.2, this field is 1-indexed. Extracted Hierarchy The location or order of the content within the source. Extracted"},{"location":"extraction/content-metadata/#text-metadata","title":"Text Metadata","text":"<p>The following is the metadata for text.</p> Field Description Method Text Type The type of the text, such as header or body. Extracted Keywords Keywords, Named Entities, or other phrases. Extracted Language The language of the content. Generated Summary An abbreviated summary of the content. (Not yet implemented.) Generated"},{"location":"extraction/content-metadata/#image-metadata","title":"Image Metadata","text":"<p>The following is the metadata for images.</p> Field Description Method Image Type The type of the image, such as structured, natural, hybrid, and others. Generated (Classifier) Structured Image Type The type of the content for structured data types, such as bar chart, pie chart, and others. Generated (Classifier) Caption Any caption or subheading associated with Image Extracted Text Extracted text from a structured chart Extracted Image location Location (x,y) of chart within an image Extracted Image location max dimensions Max dimensions (x_max,y_max) of location (x,y) Extracted uploaded_image_uri Mirrors source_metadata.source_location \u2014"},{"location":"extraction/content-metadata/#table-metadata","title":"Table Metadata","text":"<p>The following is the metadata for tables within documents.</p> <p>Warning</p> <p>Tables should not be chunked</p> Field Description Method Table format Structured (dataframe / lists of rows and columns), or serialized as markdown, html, latex, simple (cells separated as spaces). Extracted Table content Extracted text content, formatted according to table_metadata.table_format. Extracted Table location The bounding box of the table. Extracted Table location max dimensions The max dimensions (x_max,y_max) of the bounding box of the table. Extracted Caption The caption for the table or chart. Extracted Title The title of the table. Extracted Subtitle The subtitle of the table. Extracted Axis Axis information for the table. Extracted uploaded_image_uri A mirror of source_metadata.source_location. Generated"},{"location":"extraction/content-metadata/#metadata-schema-documentation","title":"Metadata Schema Documentation","text":"<p>The following is a detailed explanation of the <code>MetadataSchema</code> and its constituent sub-schemas used within the NVIDIA Ingest Framework. This schema defines the structure for metadata associated with ingested content.</p>"},{"location":"extraction/content-metadata/#metadataschema","title":"MetadataSchema","text":"<p>The <code>MetadataSchema</code> is the primary container for all metadata. It includes the core content, its URL, embedding, and various specialized metadata blocks.</p> Field Type Default Value/Behavior Description <code>content</code> <code>str</code> <code>\"\"</code> The actual textual content extracted from the source. <code>content_url</code> <code>str</code> <code>\"\"</code> URL pointing to the location of the content, if applicable. <code>embedding</code> <code>Optional[List[float]]</code> <code>None</code> Optional numerical vector representation (embedding) of the content. <code>source_metadata</code> <code>Optional[SourceMetadataSchema]</code> <code>None</code> Metadata about the original source of the content. See SourceMetadataSchema. <code>content_metadata</code> <code>Optional[ContentMetadataSchema]</code> <code>None</code> General metadata about the extracted content itself. See ContentMetadataSchema. <code>audio_metadata</code> <code>Optional[AudioMetadataSchema]</code> <code>None</code> Specific metadata for audio content. Automatically set to <code>None</code> if <code>content_metadata.type</code> is not <code>AUDIO</code>. See AudioMetadataSchema. <code>text_metadata</code> <code>Optional[TextMetadataSchema]</code> <code>None</code> Specific metadata for text content. Automatically set to <code>None</code> if <code>content_metadata.type</code> is not <code>TEXT</code>. See TextMetadataSchema. <code>image_metadata</code> <code>Optional[ImageMetadataSchema]</code> <code>None</code> Specific metadata for image content. Automatically set to <code>None</code> if <code>content_metadata.type</code> is not <code>IMAGE</code>. See ImageMetadataSchema. <code>table_metadata</code> <code>Optional[TableMetadataSchema]</code> <code>None</code> Specific metadata for tabular content. Automatically set to <code>None</code> if <code>content_metadata.type</code> is not <code>STRUCTURED</code>. See TableMetadataSchema. <code>chart_metadata</code> <code>Optional[ChartMetadataSchema]</code> <code>None</code> Specific metadata for chart content. See ChartMetadataSchema. <code>error_metadata</code> <code>Optional[ErrorMetadataSchema]</code> <code>None</code> Metadata describing any errors encountered during processing. See ErrorMetadataSchema. <code>info_message_metadata</code> <code>Optional[InfoMessageMetadataSchema]</code> <code>None</code> Informational messages related to the processing. See InfoMessageMetadataSchema. <code>debug_metadata</code> <code>Optional[Dict[str, Any]]</code> <code>None</code> A dictionary for storing any arbitrary debug information. <code>raise_on_failure</code> <code>bool</code> <code>False</code> If <code>True</code>, indicates that processing should halt on failure. <p>Note: A <code>model_validator</code> ensures that type-specific metadata fields (<code>audio_metadata</code>, <code>image_metadata</code>, <code>text_metadata</code>, <code>table_metadata</code>) are set to <code>None</code> if the <code>content_metadata.type</code> does not match the respective content type.</p>"},{"location":"extraction/content-metadata/#sourcemetadataschema","title":"<code>SourceMetadataSchema</code>","text":"<p>Describes the origin of the ingested content.</p> Field Type Default Value Description <code>source_name</code> <code>str</code> Required Name of the source (e.g., filename, URL). <code>source_id</code> <code>str</code> Required Unique identifier for the source. <code>source_location</code> <code>str</code> <code>\"\"</code> Physical or logical location of the source (e.g., path, database table). <code>source_type</code> <code>Union[DocumentTypeEnum, str]</code> Required Type of the source document (e.g., <code>pdf</code>, <code>docx</code>, <code>url</code>). Uses <code>DocumentTypeEnum</code>. <code>collection_id</code> <code>str</code> <code>\"\"</code> Identifier for any collection this source belongs to. <code>date_created</code> <code>str</code> <code>datetime.now().isoformat()</code> ISO 8601 timestamp of when the source was created. Validated to be in ISO 8601 format. <code>last_modified</code> <code>str</code> <code>datetime.now().isoformat()</code> ISO 8601 timestamp of when the source was last modified. Validated to be in ISO 8601 format. <code>summary</code> <code>str</code> <code>\"\"</code> A brief summary of the source content. <code>partition_id</code> <code>int</code> <code>-1</code> Identifier for a partition if the source is part of a larger, partitioned dataset. <code>access_level</code> <code>Union[AccessLevelEnum, int]</code> <code>AccessLevelEnum.UNKNOWN</code> Access level associated with the source. Uses <code>AccessLevelEnum</code>."},{"location":"extraction/content-metadata/#contentmetadataschema","title":"<code>ContentMetadataSchema</code>","text":"<p>General metadata about the extracted content.</p> Field Type Default Value Description <code>type</code> <code>ContentTypeEnum</code> Required The type of the extracted content (e.g., <code>TEXT</code>, <code>IMAGE</code>, <code>AUDIO</code>). Uses <code>ContentTypeEnum</code>. <code>description</code> <code>str</code> <code>\"\"</code> A description of the extracted content. <code>page_number</code> <code>int</code> <code>-1</code> Page number from which the content was extracted, if applicable (e.g., for PDFs). <code>hierarchy</code> <code>ContentHierarchySchema</code> <code>ContentHierarchySchema()</code> Hierarchical information about the content's location within the source. See ContentHierarchySchema. <code>subtype</code> <code>Union[ContentTypeEnum, str]</code> <code>\"\"</code> A more specific subtype for the content (e.g., if <code>type</code> is <code>IMAGE</code>, <code>subtype</code> could be <code>diagram</code>). <code>start_time</code> <code>int</code> <code>-1</code> Start time in milliseconds for time-based media (e.g., audio, video). <code>end_time</code> <code>int</code> <code>-1</code> End time in milliseconds for time-based media."},{"location":"extraction/content-metadata/#contenthierarchyschema","title":"<code>ContentHierarchySchema</code>","text":"<p>Describes the structural location of content within a document.</p> Field Type Default Value Description <code>page_count</code> <code>int</code> <code>-1</code> Total number of pages in the document, if applicable. <code>page</code> <code>int</code> <code>-1</code> The specific page number where the content resides. <code>block</code> <code>int</code> <code>-1</code> Identifier for a block of content (e.g., paragraph, section). <code>line</code> <code>int</code> <code>-1</code> Line number within a block, if applicable. <code>span</code> <code>int</code> <code>-1</code> Span identifier within a line, for finer granularity. <code>nearby_objects</code> <code>NearbyObjectsSchema</code> <code>NearbyObjectsSchema()</code> Information about objects (text, images, structured data) near the current content. See NearbyObjectsSchema."},{"location":"extraction/content-metadata/#nearbyobjectsschema-currently-unused","title":"<code>NearbyObjectsSchema</code> (Currently Unused)","text":"<p>Container for different types of nearby objects.</p> Field Type Default Value Description <code>text</code> <code>NearbyObjectsSubSchema</code> <code>NearbyObjectsSubSchema()</code> Nearby textual objects. See NearbyObjectsSubSchema. <code>images</code> <code>NearbyObjectsSubSchema</code> <code>NearbyObjectsSubSchema()</code> Nearby image objects. <code>structured</code> <code>NearbyObjectsSubSchema</code> <code>NearbyObjectsSubSchema()</code> Nearby structured data objects (e.g., tables)."},{"location":"extraction/content-metadata/#nearbyobjectssubschema","title":"<code>NearbyObjectsSubSchema</code>","text":"<p>Describes a list of nearby objects of a specific type.</p> Field Type Default Value Description <code>content</code> <code>List[str]</code> <code>default_factory=list</code> List of content strings for the nearby objects. <code>bbox</code> <code>List[tuple]</code> <code>default_factory=list</code> List of bounding boxes (e.g., coordinates) for the nearby objects. <code>type</code> <code>List[str]</code> <code>default_factory=list</code> List of types for the nearby objects."},{"location":"extraction/content-metadata/#textmetadataschema","title":"<code>TextMetadataSchema</code>","text":"<p>Specific metadata for textual content.</p> Field Type Default Value Description <code>text_type</code> <code>TextTypeEnum</code> Required Type of text (e.g., <code>document</code>, <code>title</code>, <code>ocr</code>). Uses <code>TextTypeEnum</code>. <code>summary</code> <code>str</code> <code>\"\"</code> A summary of this specific text segment. <code>keywords</code> <code>Union[str, List[str], Dict]</code> <code>\"\"</code> Keywords extracted from or associated with the text. Can be a single string, list of strings, or a dictionary. <code>language</code> <code>LanguageEnum</code> <code>\"en\"</code> Detected or specified language of the text. Uses <code>LanguageEnum</code>. Defaults to English. <code>text_location</code> <code>tuple</code> <code>(0, 0, 0, 0)</code> Bounding box or coordinates of the text within its source (e.g., on a page). <code>text_location_max_dimensions</code> <code>tuple</code> <code>(0, 0, 0, 0)</code> Maximum dimensions of the space where <code>text_location</code> is defined (e.g., page width/height)."},{"location":"extraction/content-metadata/#imagemetadataschema","title":"<code>ImageMetadataSchema</code>","text":"<p>Specific metadata for image content.</p> Field Type Default Value Description <code>image_type</code> <code>Union[DocumentTypeEnum, str]</code> Required Type of the image document (e.g., <code>png</code>, <code>jpeg</code>). Uses <code>DocumentTypeEnum</code> or a string. <code>structured_image_type</code> <code>ContentTypeEnum</code> <code>ContentTypeEnum.NONE</code> If the image represents structured data (e.g., a table or chart), its <code>ContentTypeEnum</code>. <code>caption</code> <code>str</code> <code>\"\"</code> Caption associated with the image. <code>text</code> <code>str</code> <code>\"\"</code> Text extracted from the image (e.g., via OCR). <code>image_location</code> <code>tuple</code> <code>(0, 0, 0, 0)</code> Bounding box or coordinates of the image within its source. <code>image_location_max_dimensions</code> <code>tuple</code> <code>(0, 0)</code> Maximum dimensions of the space where <code>image_location</code> is defined. <code>uploaded_image_url</code> <code>str</code> <code>\"\"</code> URL of the image if it has been uploaded to a separate storage location. <code>width</code> <code>int</code> <code>0</code> Width of the image in pixels. Clamped to be non-negative. <code>height</code> <code>int</code> <code>0</code> Height of the image in pixels. Clamped to be non-negative."},{"location":"extraction/content-metadata/#tablemetadataschema","title":"<code>TableMetadataSchema</code>","text":"<p>Specific metadata for tabular content.</p> Field Type Default Value Description <code>caption</code> <code>str</code> <code>\"\"</code> Caption associated with the table. <code>table_format</code> <code>TableFormatEnum</code> Required Format of the table (e.g., <code>csv</code>, <code>html</code>). Uses <code>TableFormatEnum</code>. <code>table_content</code> <code>str</code> <code>\"\"</code> String representation of the table's content (e.g., CSV string, HTML markup). <code>table_content_format</code> <code>Union[TableFormatEnum, str]</code> <code>\"\"</code> Specific format of <code>table_content</code>. <code>table_location</code> <code>tuple</code> <code>(0, 0, 0, 0)</code> Bounding box or coordinates of the table within its source. <code>table_location_max_dimensions</code> <code>tuple</code> <code>(0, 0)</code> Maximum dimensions of the space where <code>table_location</code> is defined. <code>uploaded_image_uri</code> <code>str</code> <code>\"\"</code> URI of an image representation of the table, if applicable."},{"location":"extraction/content-metadata/#chartmetadataschema","title":"<code>ChartMetadataSchema</code>","text":"<p>Metadata for table content extracted from charts.</p> Field Type Default Value Description <code>caption</code> <code>str</code> <code>\"\"</code> Caption associated with the chart. <code>table_format</code> <code>TableFormatEnum</code> Required Underlying data format of the chart (e.g., data might be in <code>csv</code> format). Uses <code>TableFormatEnum</code>. <code>table_content</code> <code>str</code> <code>\"\"</code> String representation of the chart's underlying data. <code>table_content_format</code> <code>Union[TableFormatEnum, str]</code> <code>\"\"</code> Specific format of <code>table_content</code>. <code>table_location</code> <code>tuple</code> <code>(0, 0, 0, 0)</code> Bounding box or coordinates of the chart within its source. <code>table_location_max_dimensions</code> <code>tuple</code> <code>(0, 0)</code> Maximum dimensions of the space where <code>table_location</code> is defined. <code>uploaded_image_uri</code> <code>str</code> <code>\"\"</code> URI of an image representation of the chart, if applicable."},{"location":"extraction/content-metadata/#audiometadataschema","title":"<code>AudioMetadataSchema</code>","text":"<p>Specific metadata for audio content.</p> Field Type Default Value Description <code>audio_transcript</code> <code>str</code> <code>\"\"</code> Transcript of the audio content. <code>audio_type</code> <code>str</code> <code>\"\"</code> Type or format of the audio (e.g., <code>mp3</code>, <code>wav</code>)."},{"location":"extraction/content-metadata/#errormetadataschema-currently-unused","title":"<code>ErrorMetadataSchema</code> (Currently Unused)","text":"<p>Metadata describing errors encountered during processing.</p> Field Type Default Value Description <code>task</code> <code>TaskTypeEnum</code> Required The task that was being performed when the error occurred. Uses <code>TaskTypeEnum</code>. <code>status</code> <code>StatusEnum</code> Required The status indicating failure. Uses <code>StatusEnum</code>. <code>source_id</code> <code>str</code> <code>\"\"</code> Identifier of the source item that caused the error, if applicable. <code>error_msg</code> <code>str</code> Required The error message."},{"location":"extraction/content-metadata/#infomessagemetadataschema-currently-unused","title":"<code>InfoMessageMetadataSchema</code> (Currently Unused)","text":"<p>Informational messages related to processing.</p> Field Type Default Value Description <code>task</code> <code>TaskTypeEnum</code> Required The task associated with this informational message. Uses <code>TaskTypeEnum</code>. <code>status</code> <code>StatusEnum</code> Required The status related to this message (e.g., <code>INFO</code>, <code>WARNING</code>). Uses <code>StatusEnum</code>. <code>message</code> <code>str</code> Required The informational message content. <code>filter</code> <code>bool</code> Required A flag indicating if this message should be used for filtering purposes."},{"location":"extraction/content-metadata/#enums","title":"Enums","text":"<p>The following enums are used by this schema:</p> <ul> <li><code>AccessLevelEnum</code> \u2013 Defines access levels (e.g., <code>PUBLIC</code>, <code>CONFIDENTIAL</code>, <code>UNKNOWN</code>).</li> <li><code>ContentTypeEnum</code> \u2013 Defines types of content (e.g., <code>TEXT</code>, <code>IMAGE</code>, <code>AUDIO</code>, <code>STRUCTURED</code>, <code>NONE</code>).</li> <li><code>TextTypeEnum</code> \u2013 Defines types of text (e.g., <code>DOCUMENT</code>, <code>TITLE</code>, <code>OCR</code>, <code>CAPTION</code>).</li> <li><code>LanguageEnum</code> \u2013 Defines languages (e.g., <code>ENGLISH</code> (<code>en</code>), <code>SPANISH</code> (<code>es</code>)).</li> <li><code>TableFormatEnum</code> \u2013 Defines table formats (e.g., <code>CSV</code>, <code>HTML</code>, <code>TEXT</code>).</li> <li><code>StatusEnum</code> \u2013 Defines processing statuses (e.g., <code>SUCCESS</code>, <code>FAILURE</code>, <code>PROCESSING</code>, <code>INFO</code>, <code>WARNING</code>).</li> <li><code>DocumentTypeEnum</code> \u2013 Defines types of source documents (e.g., <code>PDF</code>, <code>DOCX</code>, <code>TXT</code>, <code>URL</code>, <code>PNG</code>, <code>MP3</code>).</li> <li><code>TaskTypeEnum</code> \u2013 Defines types of processing tasks (e.g., <code>EXTRACTION</code>, <code>EMBEDDING</code>, <code>STORAGE</code>).</li> </ul>"},{"location":"extraction/content-metadata/#example-metadata","title":"Example Metadata","text":"<p>The following is an example JSON representation of metadata.  This is an example only, and does not contain the full metadata. For the full file, refer to the data folder.</p> <pre><code>{\n    \"document_type\": \"text\",\n    \"metadata\": \n    {\n        \"content\": \"TestingDocument...\",\n        \"content_url\": \"\",\n        \"source_metadata\": \n        {\n            \"source_name\": \"data/multimodal_test.pdf\",\n            \"source_id\": \"data/multimodal_test.pdf\",\n            \"source_location\": \"\",\n            \"source_type\": \"PDF\",\n            \"collection_id\": \"\",\n            \"date_created\": \"2025-03-13T18:37:14.715892\",\n            \"last_modified\": \"2025-03-13T18:37:14.715534\",\n            \"summary\": \"\",\n            \"partition_id\": -1,\n            \"access_level\": 1\n        },\n        \"content_metadata\": \n        {\n            \"type\": \"structured\",\n            \"description\": \"Structured chart extracted from PDF document.\",\n            \"page_number\": 1,\n            \"hierarchy\": \n            {\n                \"page_count\": 3,\n                \"page\": 1,\n                \"block\": -1,\n                \"line\": -1,\n                \"span\": -1,\n                \"nearby_objects\": \n                {\n                    \"text\": \n                    {\n                        \"content\": [],\n                        \"bbox\": [],\n                        \"type\": []\n                    },\n                    \"images\": \n                    {\n                        \"content\": [],\n                        \"bbox\": [],\n                        \"type\": []\n                    },\n                    \"structured\": \n                    {\n                        \"content\": [],\n                        \"bbox\": [],\n                        \"type\": []\n                    }\n                }\n            },\n            \"subtype\": \"chart\"\n        },\n        \"audio_metadata\": null,\n        \"text_metadata\": null,\n        \"image_metadata\": null,\n        \"table_metadata\": \n        {\n            \"caption\": \"\",\n            \"table_format\": \"image\",\n            \"table_content\": \"Below,is a high-quality picture of some shapes          Picture\",\n            \"table_content_format\": \"\",\n            \"table_location\": \n            [\n                74,\n                614,\n                728,\n                920\n            ],\n            \"table_location_max_dimensions\": \n            [\n                792,\n                1024\n            ],\n            \"uploaded_image_uri\": \"\"\n        },\n        \"chart_metadata\": null,\n        \"error_metadata\": null,\n        \"info_message_metadata\": null,\n        \"debug_metadata\": null,\n        \"raise_on_failure\": false\n    }\n}\n</code></pre>"},{"location":"extraction/content-metadata/#related-topics","title":"Related Topics","text":"<ul> <li>Environment Variables</li> </ul>"},{"location":"extraction/contributing/","title":"Contributing to NV-Ingest","text":"<p>External contributions to NV-Ingest will be welcome soon, and they are greatly appreciated!  For more information, refer to Contributing to NV-Ingest.</p>"},{"location":"extraction/custom-metadata/","title":"Use Custom Metadata to Filter Search Results","text":"<p>You can upload custom metadata for documents during ingestion.  By uploading custom metadata you can attach additional information to documents,  and use it for filtering results during retrieval operations.  For example, you can add author metadata to your documents, and filter by author when you retrieve results.  To create filters, you use Milvus Filtering Expressions.</p> <p>Use this documentation to use custom metadata to filter search results when you work with NeMo Retriever extraction.</p>"},{"location":"extraction/custom-metadata/#limitations","title":"Limitations","text":"<p>The following are limitation when you use custom metadata:</p> <ul> <li>Metadata fields must be consistent across documents in the same collection.</li> <li>Complex filter expressions may impact retrieval performance.</li> <li>If you update your custom metadata, you must ingest your documents again to use the new metadata.</li> </ul>"},{"location":"extraction/custom-metadata/#add-custom-metadata-during-ingestion","title":"Add Custom Metadata During Ingestion","text":"<p>You can add custom metadata during the document ingestion process.  You can specify metadata for each file,  and you can specify different metadata for different documents in the same ingestion batch.</p>"},{"location":"extraction/custom-metadata/#metadata-structure","title":"Metadata Structure","text":"<p>You specify custom metadata as a dataframe or a file (json, csv, or parquet). </p> <p>The following example contains metadata fields for category, department, and timestamp.  You can create whatever metadata is helpful for your scenario.</p> <pre><code>import pandas as pd\n\nmeta_df = pd.DataFrame(\n    {\n        \"source\": [\"data/woods_frost.pdf\", \"data/multimodal_test.pdf\"],\n        \"category\": [\"Alpha\", \"Bravo\"],\n        \"department\": [\"Language\", \"Engineering\"],\n        \"timestamp\": [\"2025-05-01T00:00:00\", \"2025-05-02T00:00:00\"]\n    }\n)\n\n# Convert the dataframe to a csv file, \n# to demonstrate how to ingest a metadata file in a later step.\n\nfile_path = \"./meta_file.csv\"\nmeta_df.to_csv(file_path)\n</code></pre>"},{"location":"extraction/custom-metadata/#example-add-custom-metadata-during-ingestion","title":"Example: Add Custom Metadata During Ingestion","text":"<p>The following example adds custom metadata during ingestion.  For more information about the <code>Ingestor</code> class, see Use the NV-Ingest Python API. For more information about the <code>vdb_upload</code> method, see Upload Data.</p> <pre><code>from nv_ingest_client.client import Ingestor\n\nhostname=\"localhost\"\ncollection_name = \"nv_ingest_collection\"\nsparse = True\n\ningestor = ( \n    Ingestor(message_client_hostname=hostname)\n        .files([\"data/woods_frost.pdf\", \"data/multimodal_test.pdf\"])\n        .extract(\n            extract_text=True,\n            extract_tables=True,\n            extract_charts=True,\n            extract_images=True,\n            text_depth=\"page\"\n        )\n        .embed()\n        .vdb_upload(\n            collection_name=collection_name, \n            milvus_uri=f\"http://{hostname}:19530\", \n            sparse=sparse, \n            minio_endpoint=f\"{hostname}:9000\", \n            dense_dim=2048,\n            meta_dataframe=file_path, \n            meta_source_field=\"source\", \n            meta_fields=[\"category\", \"department\", \"timestamp\"]\n        )\n)\nresults = ingestor.ingest_async().result()\n</code></pre>"},{"location":"extraction/custom-metadata/#best-practices","title":"Best Practices","text":"<p>The following are the best practices when you work with custom metadata:</p> <ul> <li>Plan metadata structure before ingestion.</li> <li>Test filter expressions with small datasets first.</li> <li>Consider performance implications of complex filters.</li> <li>Validate metadata during ingestion.</li> <li>Handle missing metadata fields gracefully.</li> <li>Log invalid filter expressions.</li> </ul>"},{"location":"extraction/custom-metadata/#use-custom-metadata-to-filter-results-during-retrieval","title":"Use Custom Metadata to Filter Results During Retrieval","text":"<p>You can use custom metadata to filter documents during retrieval operations.  Use filter expressions that follow the Milvus boolean expression syntax.  For more information, refer to Filtering Explained.</p>"},{"location":"extraction/custom-metadata/#example-filter-expressions","title":"Example Filter Expressions","text":"<p>The following example filters results by category.</p> <pre><code>filter_expr = 'content_metadata[\"category\"] == \"technical\"'\n</code></pre> <p>The following example filters results by time range.</p> <pre><code>filter_expr = 'content_metadata[\"timestamp\"] &gt;= \"2024-03-01T00:00:00\" and content_metadata[\"timestamp\"] &lt;= \"2025-12-31T00:00:00\"'\n</code></pre> <p>The following example filters by category and uses multiple logical operators.</p> <pre><code>filter_expr = '(content_metadata[\"department\"] == \"engineering\" and content_metadata[\"priority\"] == \"high\") or content_metadata[\"category\"] == \"critical\"'\n</code></pre>"},{"location":"extraction/custom-metadata/#example-use-a-filter-expression-in-search","title":"Example: Use a Filter Expression in Search","text":"<p>After ingestion is complete, and documents are uploaded to the database with metadata,  you can use the <code>content_metadata</code> field to filter search results.</p> <p>The following example uses a filter expression to narrow results by department.</p> <pre><code>from nv_ingest_client.util.milvus import nvingest_retrieval\n\nhostname=\"localhost\"\ncollection_name = \"nv_ingest_collection\"\nsparse = True\ntop_k = 5\nmodel_name=\"nvidia/llama-3.2-nv-embedqa-1b-v2\"\n\nfilter_expr = 'content_metadata[\"department\"] == \"Engineering\"'\n\nqueries = [\"this is expensive\"]\nq_results = []\nfor que in queries:\n    q_results.append(\n        nvingest_retrieval(\n            [que], \n            collection_name, \n            milvus_uri=f\"http://{hostname}:19530\", \n            embedding_endpoint=f\"http://{hostname}:8012/v1\",  \n            hybrid=sparse, \n            top_k=top_k, \n            model_name=model_name, \n            gpu_search=False, \n            _filter=filter_expr\n        )\n    )\n\nprint(f\"{q_results}\")\n</code></pre>"},{"location":"extraction/custom-metadata/#related-content","title":"Related Content","text":"<ul> <li>For a notebook that uses the CLI to add custom metadata and filter query results, see metadata_and_filtered_search.ipynb .</li> </ul>"},{"location":"extraction/data-store/","title":"Data Upload for NeMo Retriever Extraction","text":"<p>Use this documentation to learn how NeMo Retriever extraction handles and uploads data.</p> <p>Note</p> <p>NeMo Retriever extraction is also known as NVIDIA Ingest and nv-ingest.</p>"},{"location":"extraction/data-store/#overview","title":"Overview","text":"<p>NeMo Retriever extraction supports extracting text representations of various forms of content,  and ingesting to the Milvus vector database.  The data upload task (<code>vdb_upload</code>) pulls extraction results to the Python client,  and then pushes them to Milvus by using its underlying MinIO object store service.</p> <p>The vector database stores only the extracted text representations of ingested data.  It does not store the embeddings for images.</p> <p>Storing Extracted Images</p> <p>To persist extracted images, tables, and chart renderings to disk or object storage, use the <code>store</code> task in addition to <code>vdb_upload</code>. The <code>store</code> task supports any fsspec-compatible backend (local filesystem, S3, GCS, etc.). For details, refer to Store Extracted Images.</p> <p>NeMo Retriever extraction supports uploading data by using the Ingestor.vdb_upload API.  Currently, data upload is not supported through the NV Ingest CLI.</p>"},{"location":"extraction/data-store/#upload-to-milvus","title":"Upload to Milvus","text":"<p>The <code>vdb_upload</code> method uses GPU Cagra accelerated bulk indexing support to load chunks into Milvus.  To enable hybrid retrieval, nv-ingest supports both dense (llama-embedder embeddings) and sparse (bm25) embeddings. </p> <p>Bulk indexing is high throughput, but has a built-in overhead of around one minute.  If the number of ingested documents is 10 or fewer, nv-ingest uses faster streaming inserts instead.  You can control this by setting <code>stream=True</code>. </p> <p>If you set <code>recreate=True</code>, nv-ingest drops and recreates the collection given as collection_name.  The Milvus service persists data to disk by using a Docker volume defined in docker-compose.yaml.  You can delete all collections by deleting that volume, and then restarting the nv-ingest service.</p> <p>Warning</p> <p>When you use the <code>vdb_upload</code> task with Milvus, you must expose the ports for the Milvus and MinIO containers to the nv-ingest client. This ensures that the nv-ingest client can connect to both services and perform the <code>vdb_upload</code> action.</p> <p>Tip</p> <p>When you use the <code>vdb_upload</code> method, the behavior of the upload depends on the <code>return_failures</code> parameter of the <code>ingest</code> method. For details, refer to Capture Job Failures.</p> <p>To upload to Milvus, use code similar to the following to define your <code>Ingestor</code>.</p> <pre><code>Ingestor(client=client)\n    .files(\"data/multimodal_test.pdf\")\n    .extract()\n    .embed()\n    .caption()\n    .vdb_upload(\n        collection_name=collection_name,\n        milvus_uri=milvus_uri,\n        sparse=sparse,\n        # for llama-3.2 embedder, use 1024 for e5-v5\n        dense_dim=2048,\n        stream=False,\n        recreate=False\n    )\n</code></pre>"},{"location":"extraction/data-store/#upload-to-a-custom-data-store","title":"Upload to a Custom Data Store","text":"<p>You can ingest to other data stores by using the <code>Ingestor.vdb_upload</code> method;  however, you must configure other data stores and connections yourself.  NeMo Retriever extraction does not provide connections to other data sources. </p> <p>Important</p> <p>NVIDIA makes no claim about accuracy, performance, or functionality of any vector database except Milvus. If you use a different vector database, it's your responsibility to test and maintain it.</p> <p>For more information, refer to Build a Custom Vector Database Operator.</p>"},{"location":"extraction/data-store/#related-topics","title":"Related Topics","text":"<ul> <li>Use the NeMo Retriever Extraction Python API</li> <li>Store Extracted Images</li> <li>Environment Variables</li> <li>Troubleshoot Nemo Retriever Extraction</li> </ul>"},{"location":"extraction/environment-config/","title":"Environment Variables for NeMo Retriever Extraction","text":"<p>The following are the environment variables that you can use to configure NeMo Retriever extraction. You can specify these in your .env file or directly in your environment.</p> <p>Note</p> <p>NeMo Retriever extraction is also known as NVIDIA Ingest and nv-ingest.</p>"},{"location":"extraction/environment-config/#general-environment-variables","title":"General Environment Variables","text":"Name Example Description <code>DOWNLOAD_LLAMA_TOKENIZER</code> - The Llama tokenizer is now pre-downloaded at build time. For details, refer to Token-Based Splitting. <code>HF_ACCESS_TOKEN</code> - A token to access HuggingFace models. For details, refer to Token-Based Splitting. <code>INGEST_LOG_LEVEL</code> - <code>DEBUG</code>  - <code>INFO</code>  - <code>WARNING</code>  - <code>ERROR</code>  - <code>CRITICAL</code> The log level for the ingest service, which controls the verbosity of the logging output. <code>MESSAGE_CLIENT_HOST</code> - <code>redis</code>  - <code>localhost</code>  - <code>192.168.1.10</code> Specifies the hostname or IP address of the message broker used for communication between services. <code>MESSAGE_CLIENT_PORT</code> - <code>7670</code>  - <code>6379</code> Specifies the port number on which the message broker is listening. <code>MINIO_BUCKET</code> <code>nv-ingest</code> Name of MinIO bucket, used to store image, table, and chart extractions. <code>NGC_API_KEY</code> <code>nvapi-*************</code> An authorized NGC API key, used to interact with hosted NIMs. To create an NGC key, go to https://org.ngc.nvidia.com/setup/api-keys. <code>NIM_NGC_API_KEY</code> \u2014 The key that NIM microservices inside docker containers use to access NGC resources. This is necessary only in some cases when it is different from <code>NGC_API_KEY</code>. If this is not specified, <code>NGC_API_KEY</code> is used to access NGC resources. <code>OTEL_EXPORTER_OTLP_ENDPOINT</code> <code>http://otel-collector:4317</code> The endpoint for the OpenTelemetry exporter, used for sending telemetry data. <code>REDIS_INGEST_TASK_QUEUE</code> <code>ingest_task_queue</code> The name of the task queue in Redis where tasks are stored and processed. <code>IMAGE_STORAGE_URI</code> <code>s3://nv-ingest/artifacts/store/images</code> Default fsspec-compatible URI for the <code>store</code> task. Supports <code>s3://</code>, <code>file://</code>, <code>gs://</code>, etc. See Store Extracted Images. <code>IMAGE_STORAGE_PUBLIC_BASE_URL</code> <code>https://assets.example.com/images</code> Optional HTTP(S) base URL for serving stored images."},{"location":"extraction/environment-config/#library-mode-environment-variables","title":"Library Mode Environment Variables","text":"<p>These environment variables apply specifically when running NV-Ingest in library mode.</p> Name Example Description <code>NVIDIA_API_KEY</code> <code>nvapi-*************</code> API key for NVIDIA-hosted NIM services."},{"location":"extraction/environment-config/#related-topics","title":"Related Topics","text":"<ul> <li>Configure Ray Logging</li> </ul>"},{"location":"extraction/faq/","title":"Frequently Asked Questions for NeMo Retriever Extraction","text":"<p>This documentation contains the Frequently Asked Questions (FAQ) for NeMo Retriever extraction.</p> <p>Note</p> <p>NeMo Retriever extraction is also known as NVIDIA Ingest and nv-ingest.</p>"},{"location":"extraction/faq/#what-if-i-already-have-a-retrieval-pipeline-can-i-just-use-nemo-retriever-extraction","title":"What if I already have a retrieval pipeline? Can I just use NeMo Retriever extraction?","text":"<p>You can use the nv-ingest-cli or Python APIs to perform extraction only, and then consume the results. Using the Python API, <code>results</code> is a list object with one entry. For code examples, see the Jupyter notebooks Multimodal RAG with LlamaIndex  and Multimodal RAG with LangChain.</p>"},{"location":"extraction/faq/#where-does-nemo-retriever-extraction-nv-ingest-ingest-to","title":"Where does NeMo Retriever extraction (nv-ingest) ingest to?","text":"<p>NeMo Retriever extraction supports extracting text representations of various forms of content,  and ingesting to the Milvus vector database.  NeMo Retriever extraction does not store data on disk except through Milvus and its underlying Minio object store.  You can ingest to other data stores; however, you must configure other data stores yourself.  For more information, refer to Data Upload.</p>"},{"location":"extraction/faq/#how-would-i-process-unstructured-images","title":"How would I process unstructured images?","text":"<p>For images that <code>nemoretriever-page-elements-v3</code> does not classify as tables, charts, or infographics, you can use our VLM caption task to create a dense caption of the detected image.  That caption is then be embedded along with the rest of your content.  For more information, refer to Extract Captions from Images.</p>"},{"location":"extraction/faq/#when-should-i-consider-advanced-visual-parsing","title":"When should I consider advanced visual parsing?","text":"<p>For scanned documents, or documents with complex layouts,  we recommend that you use nemotron-parse.  Nemotron parse provides higher-accuracy text extraction.  For more information, refer to Advanced Visual Parsing.</p>"},{"location":"extraction/faq/#why-are-the-environment-variables-different-between-library-mode-and-self-hosted-mode","title":"Why are the environment variables different between library mode and self-hosted mode?","text":""},{"location":"extraction/faq/#self-hosted-deployments","title":"Self-Hosted Deployments","text":"<p>For self-hosted deployments, you should set the environment variables <code>NGC_API_KEY</code> and <code>NIM_NGC_API_KEY</code>. For more information, refer to Generate Your NGC Keys.</p> <p>For advanced scenarios, you might want to set <code>docker-compose</code> environment variables for NIM container paths, tags, and batch sizes.  You can set those directly in <code>docker-compose.yaml</code>, or in an environment variable file that docker compose uses.</p>"},{"location":"extraction/faq/#library-mode","title":"Library Mode","text":"<p>For production environments, you should use the provided Helm charts. For library mode, you should set the environment variable <code>NVIDIA_API_KEY</code>. This is because the NeMo Retriever containers and the NeMo Retriever services running inside them do not have access to the environment variables on the host machine where you run the <code>docker compose</code> command. Setting the variables in the <code>.env</code> file ensures that they are passed into the containers and available to the services that need them.</p> <p>For advanced scenarios, you might want to use library mode with self-hosted NIM instances.  You can set custom endpoints for each NIM.  For examples of <code>*_ENDPOINT</code> variables, refer to nv-ingest/docker-compose.yaml.</p>"},{"location":"extraction/faq/#what-parameters-or-settings-can-i-adjust-to-optimize-extraction-from-my-documents-or-data","title":"What parameters or settings can I adjust to optimize extraction from my documents or data?","text":"<p>See the Profile Information section  for information about the optional NIM components of the pipeline.</p> <p>You can configure the <code>extract</code>, <code>caption</code>, and other tasks by using the Ingestor API.</p> <p>To choose what types of content to extract, use code similar to the following.  For more information, refer to Extract Specific Elements from PDFs.</p> <pre><code>Ingestor(client=client)\n    .files(\"data/multimodal_test.pdf\")\n    .extract(              \n        extract_text=True,\n        extract_tables=True,\n        extract_charts=True,\n        extract_images=True,\n        paddle_output_format=\"markdown\",\n        extract_infographics=True,\n        text_depth=\"page\"\n    )\n</code></pre> <p>To generate captions for images, use code similar to the following. For more information, refer to Extract Captions from Images.</p> <pre><code>Ingestor(client=client)\n    .files(\"data/multimodal_test.pdf\")\n    .extract()\n    .embed()\n    .caption()\n)\n</code></pre>"},{"location":"extraction/helm/","title":"Deploy With Helm for NeMo Retriever Extraction","text":"<p>To deploy NeMo Retriever extraction by using Helm,  refer to NV-Ingest Helm Charts.</p>"},{"location":"extraction/nemoretriever-parse/","title":"Advanced Visual Parsing in NeMo Retriever Extraction","text":"<p>For scanned documents, or documents with complex layouts,  we recommend that you use nemotron-parse.  Nemotron parse provides higher-accuracy text extraction. </p> <p>This documentation describes the following two methods  to run NeMo Retriever extraction with nemotron-parse.</p> <ul> <li>Run the NIM locally by using Docker Compose</li> <li>Use NVIDIA Cloud Functions (NVCF) endpoints for cloud-based inference</li> </ul> <p>Note</p> <p>NeMo Retriever extraction is also known as NVIDIA Ingest and nv-ingest.</p>"},{"location":"extraction/nemoretriever-parse/#limitations","title":"Limitations","text":"<p>Currently, the limitations to using <code>nemotron-parse</code> with NeMo Retriever Extraction are the following:</p> <ul> <li>Extraction with <code>nemotron-parse</code> only supports PDFs, not image files. For more information, refer to Troubleshoot Nemo Retriever Extraction.</li> <li><code>nemotron-parse</code> is not supported on RTX Pro 6000, B200, or H200 NVL. For more information, refer to the Nemotron Parse Support Matrix.</li> </ul>"},{"location":"extraction/nemoretriever-parse/#run-the-nim-locally-by-using-docker-compose","title":"Run the NIM Locally by Using Docker Compose","text":"<p>Use the following procedure to run the NIM locally.</p> <p>Important</p> <p>Due to limitations in available VRAM controls in the current release of nemotron-parse, it must run on a dedicated additional GPU. Edit docker-compose.yaml to set nemotron-parse's device_id to a dedicated GPU: device_ids: [\"1\"] or higher.</p> <ol> <li> <p>Start the nv-ingest services with the <code>nemotron-parse</code> profile. This profile includes the necessary components for extracting text and metadata from images. Use the following command.</p> <ul> <li>The --profile nemotron-parse flag ensures that vision-language retrieval services are launched.  For more information, refer to Profile Information.</li> </ul> <pre><code>docker compose --profile nemotron-parse up\n</code></pre> </li> <li> <p>After the services are running, you can interact with nv-ingest by using Python.</p> <ul> <li>The <code>Ingestor</code> object initializes the ingestion process.</li> <li>The <code>files</code> method specifies the input files to process.</li> <li>The <code>extract</code> method tells nv-ingest to use <code>nemotron-parse</code> for extracting text and metadata from images.</li> <li>The <code>document_type</code> parameter is optional, because <code>Ingestor</code> should detect the file type automatically.</li> </ul> <pre><code>ingestor = (\n    Ingestor()\n    .files(\"./data/*.pdf\")\n    .extract(\n        document_type=\"pdf\",  # Ingestor should detect type automatically in most cases\n        extract_method=\"nemotron_parse\"\n    )\n)\n</code></pre> <p>Tip</p> <p>For more Python examples, refer to NV-Ingest: Python Client Quick Start Guide.</p> </li> </ol>"},{"location":"extraction/nemoretriever-parse/#using-nvcf-endpoints-for-cloud-based-inference","title":"Using NVCF Endpoints for Cloud-Based Inference","text":"<p>Instead of running NV-Ingest locally, you can use NVCF to perform inference by using remote endpoints.</p> <ol> <li> <p>Set the authentication token in the <code>.env</code> file.</p> <pre><code>NVIDIA_API_KEY=nvapi-...\n</code></pre> </li> <li> <p>Modify <code>docker-compose.yaml</code> to use the hosted <code>nemotron-parse</code> service.</p> <pre><code># build.nvidia.com hosted nemotron-parse\n- NEMOTRON_PARSE_HTTP_ENDPOINT=https://integrate.api.nvidia.com/v1/chat/completions\n#- NEMOTRON_PARSE_HTTP_ENDPOINT=http://nemotron-parse:8000/v1/chat/completions\n</code></pre> </li> <li> <p>Run inference by using Python.</p> <ul> <li>The <code>Ingestor</code> object initializes the ingestion process.</li> <li>The <code>files</code> method specifies the input files to process.</li> <li>The <code>extract</code> method tells nv-ingest to use <code>nemotron-parse</code> for extracting text and metadata from images.</li> <li>The <code>document_type</code> parameter is optional, because <code>Ingestor</code> should detect the file type automatically.</li> </ul> <pre><code>ingestor = (\n    Ingestor()\n    .files(\"./data/*.pdf\")\n    .extract(\n        document_type=\"pdf\",  # Ingestor should detect type automatically in most cases\n        extract_method=\"nemotron_parse\"\n    )\n)\n</code></pre> <p>Tip</p> <p>For more Python examples, refer to NV-Ingest: Python Client Quick Start Guide.</p> </li> </ol>"},{"location":"extraction/nemoretriever-parse/#related-topics","title":"Related Topics","text":"<ul> <li>Support Matrix</li> <li>Troubleshoot Nemo Retriever Extraction</li> <li>Use the NV-Ingest Python API</li> </ul>"},{"location":"extraction/ngc-api-key/","title":"Generate Your NGC Keys","text":"<p>NGC contains many public images, models, and datasets that can be pulled immediately without authentication.  To push and pull custom images, you must generate a key and authenticate with NGC.</p> <p>To create a key, go to https://org.ngc.nvidia.com/setup/api-keys.</p> <p>When you create an NGC key, select the following for Services Included.</p> <ul> <li>NGC Catalog</li> <li>Public API Endpoints</li> </ul> <p>Important</p> <p>Early Access participants must also select Private Registry.</p> <p></p>"},{"location":"extraction/ngc-api-key/#docker-login-to-ngc","title":"Docker Login to NGC","text":"<p>To pull the NIM container image from NGC, use your key to log in to the NGC registry by entering the following command and then following the prompts.  For the username, enter <code>$oauthtoken</code> exactly as shown.  It is a special authentication key for all users.</p> <pre><code>$ docker login nvcr.io\nUsername: $oauthtoken\nPassword: &lt;Your Key&gt;\n</code></pre>"},{"location":"extraction/nimclient/","title":"NimClient Usage Guide for NeMo Retriever Extraction","text":"<p>The <code>NimClient</code> class provides a unified interface for connecting to and interacting with NVIDIA NIM Microservices.  This documentation demonstrates how to create custom NIM integrations for use in NeMo Retriever extraction pipelines and User Defined Functions (UDFs).</p> <p>Note</p> <p>NeMo Retriever extraction is also known as NVIDIA Ingest and nv-ingest.</p> <p>The NimClient architecture consists of two main components:</p> <ol> <li>NimClient: The client class that handles communication with NIM endpoints via gRPC or HTTP protocols</li> <li>ModelInterface: An abstract base class that defines how to format input data, parse output responses, and process inference results for specific models</li> </ol> <p>For advanced usage patterns, see the existing model interfaces in <code>api/src/nv_ingest_api/internal/primitives/nim/model_interface/</code>.</p>"},{"location":"extraction/nimclient/#quick-start","title":"Quick Start","text":""},{"location":"extraction/nimclient/#basic-nimclient-creation","title":"Basic NimClient Creation","text":"<pre><code>from nv_ingest_api.util.nim import create_inference_client\nfrom nv_ingest_api.internal.primitives.nim import ModelInterface\n\n# Create a custom model interface (see examples below)\nmodel_interface = MyCustomModelInterface()\n\n# Define endpoints (gRPC, HTTP)\nendpoints = (\"grpc://my-nim-service:8001\", \"http://my-nim-service:8000\")\n\n# Create the client\nclient = create_inference_client(\n    endpoints=endpoints,\n    model_interface=model_interface,\n    auth_token=\"your-ngc-api-key\",  # Optional\n    infer_protocol=\"grpc\",          # Optional: \"grpc\" or \"http\"\n    timeout=120.0,                  # Optional: request timeout\n    max_retries=5                   # Optional: retry attempts\n)\n\n# Perform inference\ndata = {\"input\": \"your input data\"}\nresults = client.infer(data, model_name=\"your-model-name\")\n</code></pre>"},{"location":"extraction/nimclient/#using-environment-variables","title":"Using Environment Variables","text":"<pre><code>import os\nfrom nv_ingest_api.util.nim import create_inference_client\n\n# Use environment variables for configuration\nauth_token = os.getenv(\"NGC_API_KEY\")\ngrpc_endpoint = os.getenv(\"NIM_GRPC_ENDPOINT\", \"grpc://localhost:8001\")\nhttp_endpoint = os.getenv(\"NIM_HTTP_ENDPOINT\", \"http://localhost:8000\")\n\nclient = create_inference_client(\n    endpoints=(grpc_endpoint, http_endpoint),\n    model_interface=model_interface,\n    auth_token=auth_token\n)\n</code></pre>"},{"location":"extraction/nimclient/#creating-custom-model-interfaces","title":"Creating Custom Model Interfaces","text":"<p>To integrate a new NIM, you need to create a custom <code>ModelInterface</code> subclass that implements the required methods.</p>"},{"location":"extraction/nimclient/#basic-model-interface-template","title":"Basic Model Interface Template","text":"<pre><code>from typing import Dict, Any, List, Tuple, Optional\nimport numpy as np\nfrom nv_ingest_api.internal.primitives.nim import ModelInterface\n\nclass MyCustomModelInterface(ModelInterface):\n    \"\"\"\n    Custom model interface for My Custom NIM.\n    \"\"\"\n\n    def __init__(self, model_name: str = \"my-custom-model\"):\n        \"\"\"Initialize the model interface.\"\"\"\n        self.model_name = model_name\n\n    def name(self) -&gt; str:\n        \"\"\"Return the name of this model interface.\"\"\"\n        return \"MyCustomModel\"\n\n    def prepare_data_for_inference(self, data: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"\n        Prepare and validate input data before formatting.\n\n        Parameters\n        ----------\n        data : dict\n            Raw input data\n\n        Returns\n        -------\n        dict\n            Validated and prepared data\n        \"\"\"\n        # Validate required fields\n        if \"input_text\" not in data:\n            raise KeyError(\"Input data must include 'input_text'\")\n\n        # Ensure input is in the expected format\n        if not isinstance(data[\"input_text\"], str):\n            raise ValueError(\"input_text must be a string\")\n\n        return data\n\n    def format_input(\n        self, \n        data: Dict[str, Any], \n        protocol: str, \n        max_batch_size: int, \n        **kwargs\n    ) -&gt; Tuple[List[Any], List[Dict[str, Any]]]:\n        \"\"\"\n        Format input data for the specified protocol.\n\n        Parameters\n        ----------\n        data : dict\n            Prepared input data\n        protocol : str\n            Communication protocol (\"grpc\" or \"http\")\n        max_batch_size : int\n            Maximum batch size for processing\n        **kwargs : dict\n            Additional parameters\n\n        Returns\n        -------\n        tuple\n            (formatted_batches, batch_data_list)\n        \"\"\"\n        if protocol == \"http\":\n            return self._format_http_input(data, max_batch_size, **kwargs)\n        elif protocol == \"grpc\":\n            return self._format_grpc_input(data, max_batch_size, **kwargs)\n        else:\n            raise ValueError(\"Invalid protocol. Must be 'grpc' or 'http'\")\n\n    def _format_http_input(\n        self, \n        data: Dict[str, Any], \n        max_batch_size: int, \n        **kwargs\n    ) -&gt; Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:\n        \"\"\"Format input for HTTP protocol.\"\"\"\n        input_text = data[\"input_text\"]\n\n        # Create HTTP payload\n        payload = {\n            \"model\": kwargs.get(\"model_name\", self.model_name),\n            \"input\": input_text,\n            \"max_tokens\": kwargs.get(\"max_tokens\", 512),\n            \"temperature\": kwargs.get(\"temperature\", 0.7),\n        }\n\n        # Return as single batch\n        return [payload], [{\"original_input\": input_text}]\n\n    def _format_grpc_input(\n        self, \n        data: Dict[str, Any], \n        max_batch_size: int, \n        **kwargs\n    ) -&gt; Tuple[List[np.ndarray], List[Dict[str, Any]]]:\n        \"\"\"Format input for gRPC protocol.\"\"\"\n        input_text = data[\"input_text\"]\n\n        # Convert to numpy array for gRPC\n        text_array = np.array([[input_text.encode(\"utf-8\")]], dtype=np.object_)\n\n        return [text_array], [{\"original_input\": input_text}]\n\n    def parse_output(\n        self, \n        response: Any, \n        protocol: str, \n        data: Optional[Dict[str, Any]] = None, \n        **kwargs\n    ) -&gt; Any:\n        \"\"\"\n        Parse the raw model response.\n\n        Parameters\n        ----------\n        response : Any\n            Raw response from the model\n        protocol : str\n            Communication protocol used\n        data : dict, optional\n            Original batch data\n        **kwargs : dict\n            Additional parameters\n\n        Returns\n        -------\n        Any\n            Parsed response data\n        \"\"\"\n        if protocol == \"http\":\n            return self._parse_http_response(response)\n        elif protocol == \"grpc\":\n            return self._parse_grpc_response(response)\n        else:\n            raise ValueError(\"Invalid protocol. Must be 'grpc' or 'http'\")\n\n    def _parse_http_response(self, response: Dict[str, Any]) -&gt; str:\n        \"\"\"Parse HTTP response.\"\"\"\n        if isinstance(response, dict):\n            # Extract the generated text from response\n            if \"choices\" in response:\n                return response[\"choices\"][0].get(\"text\", \"\")\n            elif \"output\" in response:\n                return response[\"output\"]\n            else:\n                raise RuntimeError(\"Unexpected response format\")\n        return str(response)\n\n    def _parse_grpc_response(self, response: np.ndarray) -&gt; str:\n        \"\"\"Parse gRPC response.\"\"\"\n        if isinstance(response, np.ndarray):\n            # Decode bytes response\n            return response.flatten()[0].decode(\"utf-8\")\n        return str(response)\n\n    def process_inference_results(\n        self, \n        output: Any, \n        protocol: str, \n        **kwargs\n    ) -&gt; Any:\n        \"\"\"\n        Post-process the parsed inference results.\n\n        Parameters\n        ----------\n        output : Any\n            Parsed output from parse_output\n        protocol : str\n            Communication protocol used\n        **kwargs : dict\n            Additional parameters\n\n        Returns\n        -------\n        Any\n            Final processed results\n        \"\"\"\n        # Apply any final processing (e.g., filtering, formatting)\n        if isinstance(output, str):\n            return output.strip()\n        return output\n</code></pre>"},{"location":"extraction/nimclient/#real-world-examples","title":"Real-World Examples","text":""},{"location":"extraction/nimclient/#text-generation-model-interface","title":"Text Generation Model Interface","text":"<pre><code>class TextGenerationModelInterface(ModelInterface):\n    \"\"\"Interface for text generation NIMs (e.g., LLaMA, GPT-style models).\"\"\"\n\n    def name(self) -&gt; str:\n        return \"TextGeneration\"\n\n    def prepare_data_for_inference(self, data: Dict[str, Any]) -&gt; Dict[str, Any]:\n        if \"prompt\" not in data:\n            raise KeyError(\"Input data must include 'prompt'\")\n        return data\n\n    def format_input(self, data: Dict[str, Any], protocol: str, max_batch_size: int, **kwargs):\n        prompt = data[\"prompt\"]\n\n        if protocol == \"http\":\n            payload = {\n                \"model\": kwargs.get(\"model_name\", \"llama-2-7b-chat\"),\n                \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n                \"max_tokens\": kwargs.get(\"max_tokens\", 512),\n                \"temperature\": kwargs.get(\"temperature\", 0.7),\n                \"top_p\": kwargs.get(\"top_p\", 0.9),\n                \"stream\": False\n            }\n            return [payload], [{\"prompt\": prompt}]\n        else:\n            raise ValueError(\"Only HTTP protocol supported for this model\")\n\n    def parse_output(self, response: Any, protocol: str, data: Optional[Dict[str, Any]] = None, **kwargs):\n        if protocol == \"http\" and isinstance(response, dict):\n            choices = response.get(\"choices\", [])\n            if choices:\n                return choices[0].get(\"message\", {}).get(\"content\", \"\")\n        return str(response)\n\n    def process_inference_results(self, output: Any, protocol: str, **kwargs):\n        return output.strip() if isinstance(output, str) else output\n</code></pre>"},{"location":"extraction/nimclient/#image-analysis-model-interface","title":"Image Analysis Model Interface","text":"<pre><code>import base64\nfrom nv_ingest_api.util.image_processing.transforms import numpy_to_base64\n\nclass ImageAnalysisModelInterface(ModelInterface):\n    \"\"\"Interface for image analysis NIMs (e.g., vision models).\"\"\"\n\n    def name(self) -&gt; str:\n        return \"ImageAnalysis\"\n\n    def prepare_data_for_inference(self, data: Dict[str, Any]) -&gt; Dict[str, Any]:\n        if \"images\" not in data:\n            raise KeyError(\"Input data must include 'images'\")\n\n        # Ensure images is a list\n        if not isinstance(data[\"images\"], list):\n            data[\"images\"] = [data[\"images\"]]\n\n        return data\n\n    def format_input(self, data: Dict[str, Any], protocol: str, max_batch_size: int, **kwargs):\n        images = data[\"images\"]\n        prompt = data.get(\"prompt\", \"Describe this image.\")\n\n        # Convert images to base64 if needed\n        base64_images = []\n        for img in images:\n            if isinstance(img, np.ndarray):\n                base64_images.append(numpy_to_base64(img))\n            elif isinstance(img, str) and img.startswith(\"data:image\"):\n                # Already base64 encoded\n                base64_images.append(img.split(\",\")[1])\n            else:\n                base64_images.append(str(img))\n\n        # Batch images\n        batches = [base64_images[i:i + max_batch_size] \n                  for i in range(0, len(base64_images), max_batch_size)]\n\n        payloads = []\n        batch_data_list = []\n\n        for batch in batches:\n            if protocol == \"http\":\n                messages = []\n                for img_b64 in batch:\n                    messages.append({\n                        \"role\": \"user\",\n                        \"content\": f'{prompt} &lt;img src=\"data:image/png;base64,{img_b64}\" /&gt;'\n                    })\n\n                payload = {\n                    \"model\": kwargs.get(\"model_name\", \"llava-1.5-7b-hf\"),\n                    \"messages\": messages,\n                    \"max_tokens\": kwargs.get(\"max_tokens\", 512),\n                    \"temperature\": kwargs.get(\"temperature\", 0.1)\n                }\n                payloads.append(payload)\n                batch_data_list.append({\"images\": batch, \"prompt\": prompt})\n\n        return payloads, batch_data_list\n\n    def parse_output(self, response: Any, protocol: str, data: Optional[Dict[str, Any]] = None, **kwargs):\n        if protocol == \"http\" and isinstance(response, dict):\n            choices = response.get(\"choices\", [])\n            return [choice.get(\"message\", {}).get(\"content\", \"\") for choice in choices]\n        return [str(response)]\n\n    def process_inference_results(self, output: Any, protocol: str, **kwargs):\n        if isinstance(output, list):\n            return [result.strip() for result in output]\n        return output\n</code></pre>"},{"location":"extraction/nimclient/#using-nimclient-in-udfs","title":"Using NimClient in UDFs","text":""},{"location":"extraction/nimclient/#basic-udf-with-nimclient","title":"Basic UDF with NimClient","text":"<pre><code>from nv_ingest_api.internal.primitives.control_message import IngestControlMessage\nfrom nv_ingest_api.util.nim import create_inference_client\nimport os\n\ndef analyze_document_with_nim(control_message: IngestControlMessage) -&gt; IngestControlMessage:\n    \"\"\"UDF that uses a custom NIM to analyze document content.\"\"\"\n\n    # Create NIM client\n    model_interface = TextGenerationModelInterface()\n    client = create_inference_client(\n        endpoints=(\n            os.getenv(\"ANALYSIS_NIM_GRPC\", \"grpc://analysis-nim:8001\"),\n            os.getenv(\"ANALYSIS_NIM_HTTP\", \"http://analysis-nim:8000\")\n        ),\n        model_interface=model_interface,\n        auth_token=os.getenv(\"NGC_API_KEY\"),\n        infer_protocol=\"http\"\n    )\n\n    # Get the document DataFrame\n    df = control_message.get_payload()\n\n    # Process each document\n    for idx, row in df.iterrows():\n        if row.get(\"content\"):\n            # Prepare analysis prompt\n            prompt = f\"Analyze the following document content and provide a summary: {row['content'][:1000]}\"\n\n            # Perform inference\n            try:\n                results = client.infer(\n                    data={\"prompt\": prompt},\n                    model_name=\"llama-2-7b-chat\",\n                    max_tokens=256,\n                    temperature=0.3\n                )\n\n                # Add analysis to metadata\n                if results:\n                    analysis = results[0] if isinstance(results, list) else results\n                    df.at[idx, \"custom_analysis\"] = analysis\n\n            except Exception as e:\n                print(f\"NIM inference failed: {e}\")\n                df.at[idx, \"custom_analysis\"] = \"Analysis failed\"\n\n    # Update the control message with processed data\n    control_message.payload(df)\n    return control_message\n</code></pre>"},{"location":"extraction/nimclient/#advanced-udf-with-batching","title":"Advanced UDF with Batching","text":"<pre><code>def batch_image_analysis_udf(control_message: IngestControlMessage) -&gt; IngestControlMessage:\n    \"\"\"UDF that performs batched image analysis using NIM.\"\"\"\n\n    # Create image analysis client\n    model_interface = ImageAnalysisModelInterface()\n    client = create_inference_client(\n        endpoints=(\n            os.getenv(\"VISION_NIM_GRPC\", \"grpc://vision-nim:8001\"),\n            os.getenv(\"VISION_NIM_HTTP\", \"http://vision-nim:8000\")\n        ),\n        model_interface=model_interface,\n        auth_token=os.getenv(\"NGC_API_KEY\")\n    )\n\n    df = control_message.get_payload()\n\n    # Collect all images for batch processing\n    image_rows = []\n    images = []\n\n    for idx, row in df.iterrows():\n        if \"image_data\" in row and row[\"image_data\"]:\n            image_rows.append(idx)\n            images.append(row[\"image_data\"])\n\n    if images:\n        try:\n            # Batch process all images\n            results = client.infer(\n                data={\n                    \"images\": images,\n                    \"prompt\": \"Describe the content and key elements in this image.\"\n                },\n                model_name=\"llava-1.5-7b-hf\",\n                max_tokens=200\n            )\n\n            # Apply results back to DataFrame\n            for idx, result in zip(image_rows, results):\n                df.at[idx, \"image_description\"] = result\n\n        except Exception as e:\n            print(f\"Batch image analysis failed: {e}\")\n            for idx in image_rows:\n                df.at[idx, \"image_description\"] = \"Analysis failed\"\n\n    control_message.payload(df)\n    return control_message\n</code></pre>"},{"location":"extraction/nimclient/#configuration-and-best-practices","title":"Configuration and Best Practices","text":""},{"location":"extraction/nimclient/#environment-variables","title":"Environment Variables","text":"<p>Set these environment variables for your NIM endpoints:</p> <pre><code># NIM endpoints\nexport MY_NIM_GRPC_ENDPOINT=\"grpc://my-nim-service:8001\"\nexport MY_NIM_HTTP_ENDPOINT=\"http://my-nim-service:8000\"\n\n# Authentication\nexport NGC_API_KEY=\"your-ngc-api-key\"\n\n# Optional: timeouts and retries\nexport NIM_TIMEOUT=120\nexport NIM_MAX_RETRIES=5\n</code></pre>"},{"location":"extraction/nimclient/#performance-optimization","title":"Performance Optimization","text":"<ol> <li>Use gRPC when possible: Generally faster than HTTP for high-throughput scenarios</li> <li>Batch processing: Process multiple items together to reduce overhead</li> <li>Connection reuse: Create NimClient instances once and reuse them</li> <li>Appropriate timeouts: Set reasonable timeouts based on your model's response time</li> <li>Error handling: Always handle inference failures gracefully</li> </ol>"},{"location":"extraction/nimclient/#error-handling","title":"Error Handling","text":"<pre><code>def robust_nim_udf(control_message: IngestControlMessage) -&gt; IngestControlMessage:\n    \"\"\"UDF with comprehensive error handling.\"\"\"\n\n    try:\n        client = create_inference_client(\n            endpoints=(grpc_endpoint, http_endpoint),\n            model_interface=model_interface,\n            auth_token=auth_token,\n            timeout=60.0,\n            max_retries=3\n        )\n    except Exception as e:\n        print(f\"Failed to create NIM client: {e}\")\n        return control_message\n\n    df = control_message.get_payload()\n\n    for idx, row in df.iterrows():\n        try:\n            results = client.infer(data=input_data, model_name=\"my-model\")\n            df.at[idx, \"nim_result\"] = results\n        except TimeoutError:\n            print(f\"NIM request timed out for row {idx}\")\n            df.at[idx, \"nim_result\"] = \"timeout\"\n        except Exception as e:\n            print(f\"NIM inference failed for row {idx}: {e}\")\n            df.at[idx, \"nim_result\"] = \"error\"\n\n    control_message.payload(df)\n    return control_message\n</code></pre>"},{"location":"extraction/nimclient/#troubleshooting","title":"Troubleshooting","text":""},{"location":"extraction/nimclient/#common-issues","title":"Common Issues","text":"<ol> <li>Connection Errors: Verify NIM service is running and endpoints are correct</li> <li>Authentication Failures: Check NGC_API_KEY is valid and properly set</li> <li>Timeout Errors: Increase timeout values or check NIM service performance</li> <li>Format Errors: Ensure your ModelInterface formats data correctly for your NIM</li> <li>Memory Issues: Use appropriate batch sizes to avoid memory exhaustion</li> </ol>"},{"location":"extraction/nimclient/#debugging-tips","title":"Debugging Tips","text":"<pre><code>import logging\n\n# Enable debug logging\nlogging.getLogger(\"nv_ingest_api.internal.primitives.nim\").setLevel(logging.DEBUG)\n\n# Test your model interface separately\nmodel_interface = MyCustomModelInterface()\ntest_data = {\"input\": \"test\"}\n\n# Test data preparation\nprepared = model_interface.prepare_data_for_inference(test_data)\nprint(f\"Prepared data: {prepared}\")\n\n# Test input formatting\nformatted, batch_data = model_interface.format_input(prepared, \"http\", 1)\nprint(f\"Formatted input: {formatted}\")\n</code></pre>"},{"location":"extraction/nimclient/#related-topics","title":"Related Topics","text":"<ul> <li>User-Defined Functions for NeMo Retriever Extraction</li> <li>User-Defined Stages for NeMo Retriever Extraction</li> </ul>"},{"location":"extraction/notebooks/","title":"Notebooks for NeMo Retriever Extraction","text":"<p>To get started using NeMo Retriever extraction, you can try one of the ready-made notebooks that are available.</p> <p>Note</p> <p>NeMo Retriever extraction is also known as NVIDIA Ingest and nv-ingest.</p> <p>To get started with the basics, try one of the following notebooks:</p> <ul> <li>NV-Ingest: CLI Client Quick Start Guide</li> <li>NV-Ingest: Python Client Quick Start Guide</li> <li>How to add metadata to your documents and filter searches</li> <li>How to reindex a collection</li> </ul> <p>For more advanced scenarios, try one of the following notebooks:</p> <ul> <li>Build a Custom Vector Database Operator</li> <li>Try out the NVIDIA Multimodal PDF Data Extraction Blueprint</li> <li>Evaluate bo767 retrieval recall accuracy with NV-Ingest and Milvus</li> <li>Multimodal RAG with LangChain</li> <li>Multimodal RAG with LlamaIndex</li> </ul>"},{"location":"extraction/notebooks/#related-topics","title":"Related Topics","text":"<ul> <li>Prerequisites</li> <li>Support Matrix</li> <li>Deploy Without Containers (Library Mode)</li> <li>Deploy With Docker Compose (Self-Hosted)</li> <li>Deploy With Helm</li> </ul>"},{"location":"extraction/nv-ingest-python-api/","title":"Use the NeMo Retriever Extraction Python API","text":"<p>The NeMo Retriever extraction Python API provides a simple and flexible interface for processing and extracting information from various document types, including PDFs.</p> <p>Note</p> <p>NeMo Retriever extraction is also known as NVIDIA Ingest and nv-ingest.</p> <p>Tip</p> <p>There is a Jupyter notebook available to help you get started with the Python API. For more information, refer to Python Client Quick Start Guide.</p>"},{"location":"extraction/nv-ingest-python-api/#summary-of-key-methods","title":"Summary of Key Methods","text":"<p>The main class in the nv-ingest API is <code>Ingestor</code>.  The <code>Ingestor</code> class provides an interface for building, managing, and running data ingestion jobs, enabling for chainable task additions and job state tracking. </p>"},{"location":"extraction/nv-ingest-python-api/#ingestor-methods","title":"Ingestor Methods","text":"<p>The following table describes methods of the <code>Ingestor</code> class.</p> Method Description <code>caption</code> Extract captions from images within the document. <code>embed</code> Generate embeddings from extracted content. <code>extract</code> Add an extraction task (text, tables, charts, infographics). <code>files</code> Add document paths for processing. <code>ingest</code> Submit jobs and retrieve results synchronously. <code>load</code> Ensure files are locally accessible (downloads if needed). <code>save_to_disk</code> Save ingestion results to disk instead of memory. <code>store</code> Persist extracted images/structured renderings to an fsspec-compatible backend. <code>split</code> Split documents into smaller sections for processing. For more information, refer to Split Documents. <code>vdb_upload</code> Push extraction results to Milvus vector database. For more information, refer to Data Upload."},{"location":"extraction/nv-ingest-python-api/#extract-method-options","title":"Extract Method Options","text":"<p>The following table describes the <code>extract_method</code> options.</p> Value Status Description <code>audio</code> Current Extract information from audio files. <code>nemotron_parse</code> Current NVIDIA Nemotron Parse extraction. <code>ocr</code> Current Bypasses native text extraction and processes every page using the full OCR pipeline. Use this for fully scanned documents or when native text is corrupt. <code>pdfium</code> Current Uses PDFium to extract native text. This is the default. This is the fastest method but does not capture text from scanned images/pages. <code>pdfium_hybrid</code> Current A hybrid approach that uses PDFium for pages with native text and automatically switches to OCR for scanned pages. This offers a robust balance of speed and coverage for mixed documents. <code>adobe</code> Deprecated Adobe PDF Services API extraction. <code>haystack</code> Deprecated Haystack-based extraction. <code>llama_parse</code> Deprecated LlamaParse extraction. <code>tika</code> Deprecated Apache Tika extraction. <code>unstructured_io</code> Deprecated Unstructured.io API extraction. <code>unstructured_local</code> Deprecated Local Unstructured extraction."},{"location":"extraction/nv-ingest-python-api/#caption-images-and-control-reasoning","title":"Caption images and control reasoning","text":"<p>The caption task can call a VLM with optional prompt and reasoning controls:</p> <ul> <li><code>prompt</code> (string): User prompt for captioning. Defaults to <code>\"Caption the content of this image:\"</code>.</li> <li><code>reasoning</code> (boolean): Enable reasoning mode. <code>True</code> enables reasoning, <code>False</code> disables it. Defaults to <code>None</code> (service default, typically disabled).</li> </ul> <p>Note</p> <p>The <code>reasoning</code> parameter maps to the VLM's system prompt: <code>reasoning=True</code> sets the system prompt to <code>\"/think\"</code>, and <code>reasoning=False</code> sets it to <code>\"/no_think\"</code> per the [Nemotron Nano 12B v2 VL model card] (https://build.nvidia.com/nvidia/nemotron-nano-12b-v2-vl/modelcard).</p> <p>Example: <pre><code>from nv_ingest_client.client.interface import Ingestor\n\ningestor = (\n    Ingestor()\n    .files(\"path/to/doc-with-images.pdf\")\n    .extract(extract_images=True)\n    .caption(\n        prompt=\"Caption the content of this image:\",\n        reasoning=True,  # Enable reasoning\n    )\n    .ingest()\n)\n</code></pre></p>"},{"location":"extraction/nv-ingest-python-api/#track-job-progress","title":"Track Job Progress","text":"<p>For large document batches, you can enable a progress bar by setting <code>show_progress</code> to true.  Use the following code.</p> <pre><code># Return only successes\nresults = ingestor.ingest(show_progress=True)\n\nprint(len(results), \"successful documents\")\n</code></pre>"},{"location":"extraction/nv-ingest-python-api/#capture-job-failures","title":"Capture Job Failures","text":"<p>You can capture job failures by setting <code>return_failures</code> to true.  Use the following code.</p> <pre><code># Return both successes and failures\nresults, failures = ingestor.ingest(show_progress=True, return_failures=True)\n\nprint(f\"{len(results)} successful docs; {len(failures)} failures\")\n\nif failures:\n    print(\"Failures:\", failures[:1])\n</code></pre> <p>When you use the <code>vdb_upload</code> method, uploads are performed after ingestion completes.  The behavior of the upload depends on the following values of <code>return_failures</code>:</p> <ul> <li>False \u2013 If any job fails, the <code>ingest</code> method raises a runtime error and does not upload any data (all-or-nothing data upload). This is the default setting.</li> <li>True \u2013 If any jobs succeed, the results from those jobs are uploaded, and no errors are raised (partial data upload). The <code>ingest</code> method returns a failures object that contains the details for any jobs that failed. You can inspect the failures object and selectively retry or remediate the failed jobs.</li> </ul> <p>The following example uploads data to Milvus and returns any failures.</p> <pre><code>ingestor = (\n    Ingestor(client=client)\n    .files([\"/path/doc1.pdf\", \"/path/doc2.pdf\"])\n    .extract()\n    .embed()\n    .vdb_upload(collection_name=\"my_collection\", milvus_uri=\"milvus.db\")\n)\n\n# Use for large batches where you want successful chunks/pages to be committed, while collecting detailed diagnostics for failures.\nresults, failures = ingestor.ingest(return_failures=True)\n\nprint(f\"Uploaded {len(results)} successful docs; {len(failures)} failures\")\n\nif failures:\n    print(\"Failures:\", failures[:1])\n</code></pre>"},{"location":"extraction/nv-ingest-python-api/#quick-start-extracting-pdfs","title":"Quick Start: Extracting PDFs","text":"<p>The following example demonstrates how to initialize <code>Ingestor</code>, load a PDF file, and extract its contents. The <code>extract</code> method enables different types of data to be extracted.</p>"},{"location":"extraction/nv-ingest-python-api/#extract-a-single-pdf","title":"Extract a Single PDF","text":"<p>Use the following code to extract a single PDF file.</p> <pre><code>from nv_ingest_client.client.interface import Ingestor\n\n# Initialize Ingestor with a local PDF file\ningestor = Ingestor().files(\"path/to/document.pdf\")\n\n# Extract text, tables, and images\nresult = ingestor.extract().ingest()\n\nprint(result)\n</code></pre>"},{"location":"extraction/nv-ingest-python-api/#extract-multiple-pdfs","title":"Extract Multiple PDFs","text":"<p>Use the following code to process multiple PDFs at one time.</p> <pre><code>ingestor = Ingestor().files([\"path/to/doc1.pdf\", \"path/to/doc2.pdf\"])\n\n# Extract content from all PDFs\nresult = ingestor.extract().ingest()\n\nfor doc in result:\n    print(doc)\n</code></pre>"},{"location":"extraction/nv-ingest-python-api/#extract-specific-elements-from-pdfs","title":"Extract Specific Elements from PDFs","text":"<p>By default, the <code>extract</code> method extracts all supported content types.  You can customize the extraction behavior by using the following code.</p> <pre><code>ingestor = ingestor.extract(\n    extract_text=True,  # Extract text\n    text_depth=\"page\",\n    extract_tables=False,  # Skip table extraction\n    extract_charts=True,  # Extract charts\n    extract_infographics=True,  # Extract infographic images\n    extract_images=False  # Skip image extraction\n)\n</code></pre>"},{"location":"extraction/nv-ingest-python-api/#extract-non-standard-document-types","title":"Extract Non-standard Document Types","text":"<p>Use the following code to extract text from <code>.md</code>, <code>.sh</code>, and <code>.html</code> files.</p> <pre><code>ingestor = Ingestor().files([\"path/to/doc1.md\", \"path/to/doc2.html\"])\n\ningestor = ingestor.extract(\n    extract_text=True,  # Only extract text\n    extract_tables=False,\n    extract_charts=False,\n    extract_infographics=False,\n    extract_images=False\n)\n\nresult = ingestor.ingest()\n</code></pre>"},{"location":"extraction/nv-ingest-python-api/#extract-with-custom-document-type","title":"Extract with Custom Document Type","text":"<p>Use the following code to specify a custom document type for extraction.</p> <pre><code>ingestor = ingestor.extract(document_type=\"pdf\")\n</code></pre>"},{"location":"extraction/nv-ingest-python-api/#extract-office-documents-docx-and-pptx","title":"Extract Office Documents (DOCX and PPTX)","text":"<p>NeMo Retriever extraction offers the following two extraction methods for Microsoft Office documents (.docx and .pptx), to balance performance and layout fidelity:</p> <ul> <li>Native extraction</li> <li>Render as PDF</li> </ul>"},{"location":"extraction/nv-ingest-python-api/#native-extraction-default","title":"Native Extraction (Default)","text":"<p>The default methods (<code>python_docx</code> and <code>python_pptx</code>) extract content directly from the file structure. This is generally faster, but you might lose some visual layout information.</p> <pre><code># Uses default native extraction\ningestor = Ingestor().files([\"report.docx\", \"presentation.pptx\"]).extract()\n</code></pre>"},{"location":"extraction/nv-ingest-python-api/#render-as-pdf","title":"Render as PDF","text":"<p>The <code>render_as_pdf</code> method uses LibreOffice to convert the document to a PDF before extraction. We recommend this approach when preserving the visual layout is critical, or when you need to extract visual elements, such as tables and charts, that are better detected by using computer vision on a rendered page.</p> <pre><code>ingestor = Ingestor().files([\"report.docx\", \"presentation.pptx\"])\n\ningestor = ingestor.extract(\n    extract_text=True,\n    extract_tables=True,\n    extract_charts=True,\n    extract_infographics=True,\n    extract_method=\"render_as_pdf\"  # Convert to PDF first for improved visual extraction\n)\n</code></pre>"},{"location":"extraction/nv-ingest-python-api/#pdf-extraction-strategies","title":"PDF Extraction Strategies","text":"<p>NeMo Retriever extraction offers specialized strategies for PDF processing to handle various document qualities. You can select the strategy by using the following <code>extract_method</code> parameter values.  For the full list of <code>extract_method</code> options, refer to Extract Method Options.</p> <ul> <li>ocr \u2013 Bypasses native text extraction and processes every page using the full OCR pipeline. Use this for fully scanned documents or when native text is corrupt.</li> <li>pdfium \u2013 Uses PDFium to extract native text. This is the default. This is the fastest method but does not capture text from scanned images/pages.</li> <li>pdfium_hybrid \u2013 A hybrid approach that uses PDFium for pages with native text and automatically switches to OCR for scanned pages. This offers a robust balance of speed and coverage for mixed documents.</li> </ul> <pre><code>ingestor = Ingestor().files(\"mixed_content.pdf\")\n\n# Use hybrid mode for mixed digital/scanned PDFs\ningestor = ingestor.extract(\n    document_type=\"pdf\",\n    extract_method=\"pdfium_hybrid\",\n)\nresults = ingestor.ingest()\n</code></pre>"},{"location":"extraction/nv-ingest-python-api/#work-with-large-datasets-save-to-disk","title":"Work with Large Datasets: Save to Disk","text":"<p>By default, NeMo Retriever extraction stores the results from every document in system memory (RAM).  When you process a very large dataset with thousands of documents, you might encounter an Out-of-Memory (OOM) error.  The <code>save_to_disk</code> method configures the extraction pipeline to write the output for each document to a separate JSONL file on disk.</p>"},{"location":"extraction/nv-ingest-python-api/#basic-usage-save-to-a-directory","title":"Basic Usage: Save to a Directory","text":"<p>To save results to disk, simply chain the <code>save_to_disk</code> method to your ingestion task. By using <code>save_to_disk</code> the <code>ingest</code> method returns a list of <code>LazyLoadedList</code> objects,  which are memory-efficient proxies that read from the result files on disk.</p> <p>In the following example, the results are saved to a directory named <code>my_ingest_results</code>.  You are responsible for managing the created files.</p> <pre><code>ingestor = Ingestor().files(\"large_dataset/*.pdf\")\n\n# Use save_to_disk to configure the ingestor to save results to a specific directory.\n# Set cleanup=False to ensure that the directory is not deleted by any automatic process.\ningestor.save_to_disk(output_directory=\"./my_ingest_results\", cleanup=False)  # Offload results to disk to prevent OOM errors\n\n# 'results' is a list of LazyLoadedList objects that point to the new jsonl files.\nresults = ingestor.extract().ingest()\n\nprint(\"Ingestion results saved in ./my_ingest_results\")\n# You can now iterate over the results or inspect the files directly.\n</code></pre>"},{"location":"extraction/nv-ingest-python-api/#managing-disk-space-with-automatic-cleanup","title":"Managing Disk Space with Automatic Cleanup","text":"<p>When you use <code>save_to_disk</code>, NeMo Retriever extraction creates intermediate files.  For workflows where these files are temporary, NeMo Retriever extraction provides two automatic cleanup mechanisms.</p> <ul> <li> <p>Directory Cleanup with Context Manager \u2014 While not required for general use, the Ingestor can be used as a context manager (<code>with</code> statement). This enables the automatic cleanup of the entire output directory when <code>save_to_disk(cleanup=True)</code> is set (which is the default).</p> </li> <li> <p>File Purge After VDB Upload \u2013 The <code>vdb_upload</code> method includes a <code>purge_results_after_upload: bool = True</code> parameter (the default). After a successful VDB upload, this feature deletes the individual <code>.jsonl</code> files that were just uploaded.</p> </li> </ul> <p>You can also configure the output directory by using the <code>NV_INGEST_CLIENT_SAVE_TO_DISK_OUTPUT_DIRECTORY</code> environment variable.</p>"},{"location":"extraction/nv-ingest-python-api/#example-fully-automatic-cleanup","title":"Example (Fully Automatic Cleanup)","text":"<p>Fully Automatic cleanup is the recommended pattern for ingest-and-upload workflows where the intermediate files are no longer needed.  The entire process is temporary, and no files are left on disk. The following example includes automatic file purge. </p> <pre><code># After the 'with' block finishes, \n# the temporary directory and all its contents are automatically deleted.\n\nwith (\n    Ingestor()\n    .files(\"/path/to/large_dataset/*.pdf\")\n    .extract()\n    .embed()\n    .save_to_disk()  # cleanup=True is the default, enables directory deletion on exit\n    .vdb_upload()  # purge_results_after_upload=True is the default, deletes files after upload\n) as ingestor:\n    results = ingestor.ingest()\n</code></pre>"},{"location":"extraction/nv-ingest-python-api/#example-preserve-results-on-disk","title":"Example (Preserve Results on Disk)","text":"<p>In scenarios where you need to inspect or use the intermediate <code>jsonl</code> files, you can disable the cleanup features.  The following example disables automatic file purge. </p> <pre><code># After the 'with' block finishes, \n# the './permanent_results' directory and all jsonl files are preserved for inspection or other uses.\n\nwith (\n    Ingestor()\n    .files(\"/path/to/large_dataset/*.pdf\")\n    .extract()\n    .embed()\n    .save_to_disk(output_directory=\"./permanent_results\", cleanup=False)  # Specify a directory and disable directory-level cleanup\n    .vdb_upload(purge_results_after_upload=False)  # Disable automatic file purge after the VDB upload\n) as ingestor:\n    results = ingestor.ingest()\n</code></pre>"},{"location":"extraction/nv-ingest-python-api/#extract-captions-from-images","title":"Extract Captions from Images","text":"<p>The <code>caption</code> method generates image captions by using a vision-language model.  This can be used to describe images extracted from documents.</p> <p>Note</p> <p>The default model used by <code>caption</code> is <code>nvidia/llama-3.1-nemotron-nano-vl-8b-v1</code>.</p> <pre><code>ingestor = ingestor.caption()\n</code></pre> <p>To specify a different API endpoint, pass additional parameters to <code>caption</code>.</p> <pre><code>ingestor = ingestor.caption(\n    endpoint_url=\"https://integrate.api.nvidia.com/v1/chat/completions\",\n    model_name=\"nvidia/llama-3.1-nemotron-nano-vl-8b-v1\",\n    api_key=\"nvapi-\"\n)\n</code></pre>"},{"location":"extraction/nv-ingest-python-api/#caption-images-and-control-reasoning_1","title":"Caption Images and Control Reasoning","text":"<p>The caption task can call a VLM with optional prompt and system prompt overrides:</p> <ul> <li><code>caption_prompt</code> (user prompt): defaults to <code>\"Caption the content of this image:\"</code>.</li> <li><code>caption_system_prompt</code> (system prompt): defaults to <code>\"/no_think\"</code> (reasoning off). Set to <code>\"/think\"</code> to enable reasoning per the Nemotron Nano 12B v2 VL model card.</li> </ul> <p>Example: <pre><code>from nv_ingest_client.client.interface import Ingestor\n\ningestor = (\n    Ingestor()\n    .files(\"path/to/doc-with-images.pdf\")\n    .extract(extract_images=True)\n    .caption(\n        prompt=\"Caption the content of this image:\",\n        system_prompt=\"/think\",  # or \"/no_think\"\n    )\n    .ingest()\n)\n</code></pre></p>"},{"location":"extraction/nv-ingest-python-api/#extract-embeddings","title":"Extract Embeddings","text":"<p>The <code>embed</code> method in NV-Ingest generates text embeddings for document content.</p> <pre><code>ingestor = ingestor.embed()\n</code></pre> <p>Note</p> <p>By default, <code>embed</code> uses the llama-3.2-nv-embedqa-1b-v2 model.</p> <p>To use a different embedding model, such as nv-embedqa-e5-v5, specify a different <code>model_name</code> and <code>endpoint_url</code>.</p> <pre><code>ingestor = ingestor.embed(\n    endpoint_url=\"https://integrate.api.nvidia.com/v1\",\n    model_name=\"nvidia/nv-embedqa-e5-v5\",\n    api_key=\"nvapi-\"\n)\n</code></pre>"},{"location":"extraction/nv-ingest-python-api/#store-extracted-images","title":"Store Extracted Images","text":"<p>The <code>store</code> method exports decoded images (unstructured images as well as structured renderings such as tables and charts) to any fsspec-compatible URI so you can inspect or serve the generated visuals.</p> <pre><code>ingestor = ingestor.store(\n    structured=True,   # persist table/chart renderings\n    images=True,       # persist unstructured images\n    storage_uri=\"file:///workspace/data/artifacts/store/images\",  # Supports file://, s3://, etc.\n    public_base_url=\"https://assets.example.com/images\"  # Optional CDN/base URL for download links\n)\n</code></pre>"},{"location":"extraction/nv-ingest-python-api/#store-method-parameters","title":"Store Method Parameters","text":"Parameter Type Description <code>structured</code> bool Persist table and chart renderings. Default: <code>False</code> <code>images</code> bool Persist unstructured images extracted from documents. Default: <code>False</code> <code>storage_uri</code> str fsspec-compatible URI (<code>file://</code>, <code>s3://</code>, <code>gs://</code>, etc.). Defaults to server-side <code>IMAGE_STORAGE_URI</code> environment variable. <code>public_base_url</code> str Optional HTTP(S) base URL for serving stored images. When set, metadata includes public download links."},{"location":"extraction/nv-ingest-python-api/#supported-storage-backends","title":"Supported Storage Backends","text":"<p>The <code>store</code> task uses fsspec for storage, supporting multiple backends:</p> Backend URI Format Example Local filesystem <code>file://</code> <code>file:///workspace/data/images</code> Amazon S3 <code>s3://</code> <code>s3://my-bucket/extracted-images</code> Google Cloud Storage <code>gs://</code> <code>gs://my-bucket/images</code> Azure Blob Storage <code>abfs://</code> <code>abfs://container@account.dfs.core.windows.net/images</code> MinIO (S3-compatible) <code>s3://</code> <code>s3://nv-ingest/artifacts/store/images</code> (default) <p>Tip</p> <p><code>storage_uri</code> defaults to the server-side <code>IMAGE_STORAGE_URI</code> environment variable (commonly <code>s3://nv-ingest/...</code>). If you change that variable\u2014for example to a host-mounted <code>file://</code> path\u2014restart the nv-ingest runtime so the container picks up the new value.</p> <p>When <code>public_base_url</code> is provided, the metadata returned from <code>ingest()</code> surfaces that HTTP(S) link while still recording the underlying storage URI. Leave it unset when the storage endpoint itself is already publicly reachable.</p>"},{"location":"extraction/nv-ingest-python-api/#docker-volume-mounts-for-local-storage","title":"Docker Volume Mounts for Local Storage","text":"<p>When running nv-ingest via Docker and using <code>file://</code> storage URIs, the path must be within a mounted volume for files to persist on the host machine.</p> <p>By default, the <code>docker-compose.yaml</code> mounts a single volume:</p> <pre><code>volumes:\n  - ${DATASET_ROOT:-./data}:/workspace/data\n</code></pre> <p>This means:</p> Container Path Host Path Works with <code>file://</code>? <code>/workspace/data/...</code> <code>${DATASET_ROOT}/...</code> (default: <code>./data/...</code>) \u2705 Yes <code>/tmp/...</code> (container only) \u274c No - files lost on restart <code>/raid/custom/path</code> (container only) \u274c No - path not mounted <p>Example: Save to host filesystem</p> <pre><code># Files save to ./data/artifacts/images on the host\ningestor = ingestor.store(\n    structured=True,\n    images=True,\n    storage_uri=\"file:///workspace/data/artifacts/images\"\n)\n</code></pre> <p>Example: Use a custom host directory</p> <pre><code># Set DATASET_ROOT before starting services\nexport DATASET_ROOT=/raid/my-project/nv-ingest-data\ndocker compose up -d\n</code></pre> <pre><code># Now /workspace/data maps to /raid/my-project/nv-ingest-data\ningestor = ingestor.store(\n    structured=True,\n    images=True,\n    storage_uri=\"file:///workspace/data/extracted-images\"\n)\n# Files save to /raid/my-project/nv-ingest-data/extracted-images on host\n</code></pre> <p>For more information on environment variables, refer to Environment Variables.</p>"},{"location":"extraction/nv-ingest-python-api/#extract-audio","title":"Extract Audio","text":"<p>Use the following code to extract mp3 audio content.</p> <pre><code>from nv_ingest_client.client import Ingestor\n\ningestor = Ingestor().files(\"audio_file.mp3\")\n\ningestor = ingestor.extract(\n        document_type=\"mp3\",\n        extract_text=True,\n        extract_tables=False,\n        extract_charts=False,\n        extract_images=False,\n        extract_infographics=False,\n    ).split(\n        tokenizer=\"meta-llama/Llama-3.2-1B\",\n        chunk_size=150,\n        chunk_overlap=0,\n        params={\"split_source_types\": [\"mp3\"], \"hf_access_token\": \"hf_***\"}\n    )\n\nresults = ingestor.ingest()\n</code></pre>"},{"location":"extraction/nv-ingest-python-api/#related-topics","title":"Related Topics","text":"<ul> <li>Split Documents</li> <li>Troubleshoot Nemo Retriever Extraction</li> <li>Advanced Visual Parsing</li> <li>Use NeMo Retriever Extraction with Riva for Audio Processing</li> <li>Use Multimodal Embedding</li> </ul>"},{"location":"extraction/nv-ingest_cli/","title":"Use the NV-Ingest Command Line Interface","text":"<p>After you install the Python dependencies, you can use the NV-Ingest command line interface (CLI).  To use the CLI, use the <code>nv-ingest-cli</code> command.</p> <p>To check the version of the CLI that you have installed, run the following command.</p> <pre><code>nv-ingest-cli --version\n</code></pre> <p>To get a list of the current CLI commands and their options, run the following command.</p> <pre><code>nv-ingest-cli --help\n</code></pre> <p>Tip</p> <p>There is a Jupyter notebook available to help you get started with the CLI. For more information, refer to CLI Client Quick Start Guide.</p>"},{"location":"extraction/nv-ingest_cli/#examples","title":"Examples","text":"<p>Use the following code examples to submit a document to the <code>nv-ingest-ms-runtime</code> service.</p> <p>Each of the following commands can be run from the host machine, or from within the <code>nv-ingest-ms-runtime</code> container.</p> <ul> <li>Host: <code>nv-ingest-cli ...</code></li> <li>Container: <code>nv-ingest-cli ...</code></li> </ul>"},{"location":"extraction/nv-ingest_cli/#example-text-file-with-no-splitting","title":"Example: Text File With No Splitting","text":"<p>To submit a text file with no splitting, run the following code.</p> <p>Note</p> <p>You receive a response that contains a single document, which is the entire text file. The data that is returned is wrapped in the appropriate metadata structure.</p> <pre><code>nv-ingest-cli \\\n  --doc ./data/test.pdf \\\n  --client_host=localhost \\\n  --client_port=7670\n</code></pre>"},{"location":"extraction/nv-ingest_cli/#example-pdf-file-with-splitting-only","title":"Example: PDF File With Splitting Only","text":"<p>To submit a .pdf file with only a splitting task, run the following code.</p> <pre><code>nv-ingest-cli \\\n  --doc ./data/test.pdf \\\n  --output_directory ./processed_docs \\\n  --task='split' \\\n  --client_host=localhost \\\n  --client_port=7670\n</code></pre>"},{"location":"extraction/nv-ingest_cli/#example-pdf-file-with-splitting-and-extraction","title":"Example: PDF File With Splitting and Extraction","text":"<p>To submit a .pdf file with both a splitting task and an extraction task, run the following code.</p> <p>Note</p> <p>Currently, <code>split</code> only works for pdfium, nemotron-parse, and Unstructured.io.</p> <pre><code>nv-ingest-cli \\\n  --doc ./data/test.pdf \\\n  --output_directory ./processed_docs \\\n  --task='extract:{\"document_type\": \"pdf\", \"extract_method\": \"pdfium\"}' \\\n  --task='extract:{\"document_type\": \"docx\", \"extract_method\": \"python_docx\"}' \\\n  --task='split' \\\n  --client_host=localhost \\\n  --client_port=7670\n</code></pre>"},{"location":"extraction/nv-ingest_cli/#example-pdf-file-with-custom-split-page-count","title":"Example: PDF File With Custom Split Page Count","text":"<p>To submit a PDF file with a custom split page count, use the <code>--pdf_split_page_count</code> option.  This allows you to control how many pages are included in each PDF chunk during processing.</p> <p>Note</p> <p>The <code>--pdf_split_page_count</code> option requires using the V2 API (set via <code>--api_version v2</code> or environment variable <code>NV_INGEST_API_VERSION=v2</code>). It accepts values between 1 and 128 pages per chunk (default is server default, typically 32). Smaller chunks provide more parallelism but increase overhead, while larger chunks reduce overhead but limit concurrency.</p> <pre><code>nv-ingest-cli \\\n  --doc ./data/test.pdf \\\n  --output_directory ./processed_docs \\\n  --task='extract:{\"document_type\": \"pdf\", \"extract_method\": \"pdfium\", \"extract_text\": \"true\"}' \\\n  --pdf_split_page_count 64 \\\n  --api_version v2 \\\n  --client_host=localhost \\\n  --client_port=7670\n</code></pre>"},{"location":"extraction/nv-ingest_cli/#example-caption-images-with-reasoning-control","title":"Example: Caption images with reasoning control","text":"<p>To invoke image captioning and control reasoning:</p> <pre><code>nv-ingest-cli \\\n  --doc ./data/test.pdf \\\n  --task='extract:{\"document_type\": \"pdf\", \"extract_method\": \"pdfium\", \"extract_images\": \"true\"}' \\\n  --task='caption:{\"prompt\": \"Caption the content of this image:\", \"reasoning\": true}' \\\n  --client_host=localhost \\\n  --client_port=7670\n</code></pre> <ul> <li><code>reasoning</code> (boolean): Set to <code>true</code> to enable reasoning, <code>false</code> to disable it. Defaults to service default (typically disabled).</li> <li>Ensure the VLM caption profile/service is running or pointing to the public build endpoint; otherwise the caption task will be skipped.</li> </ul> <p>Alternatively, you can use an environment variable to set the API version:</p> <pre><code>export NV_INGEST_API_VERSION=v2\n\nnv-ingest-cli \\\n  --doc ./data/test.pdf \\\n  --output_directory ./processed_docs \\\n  --task='extract:{\"document_type\": \"pdf\", \"extract_method\": \"pdfium\", \"extract_text\": \"true\"}' \\\n  --pdf_split_page_count 64 \\\n  --client_host=localhost \\\n  --client_port=7670\n</code></pre>"},{"location":"extraction/nv-ingest_cli/#example-process-a-dataset","title":"Example: Process a Dataset","text":"<p>To submit a dataset for processing, run the following code.  To create a dataset, refer to Command Line Dataset Creation with Enumeration and Sampling.</p> <pre><code>nv-ingest-cli \\\n  --dataset dataset.json \\\n  --output_directory ./processed_docs \\\n  --task='extract:{\"document_type\": \"pdf\", \"extract_method\": \"pdfium\"}' \\\n  --client_host=localhost \\\n  --client_port=7670\n</code></pre> <p>Submit a PDF file with extraction tasks and upload extracted images to MinIO.</p> <pre><code>nv-ingest-cli \\\n  --doc ./data/test.pdf \\\n  --output_directory ./processed_docs \\\n  --task='extract:{\"document_type\": \"pdf\", \"extract_method\": \"pdfium\"}' \\\n  --client_host=localhost \\\n  --client_port=7670\n</code></pre>"},{"location":"extraction/nv-ingest_cli/#command-line-dataset-creation-with-enumeration-and-sampling","title":"Command Line Dataset Creation with Enumeration and Sampling","text":"<p>The <code>gen_dataset.py</code> script samples files from a specified source directory according to defined proportions and a total size target.  It offers options for caching the file list, outputting a sampled file list, and validating the output.</p> <pre><code>python ./src/util/gen_dataset.py --source_directory=./data --size=1GB --sample pdf=60 --sample txt=40 --output_file \\\n  dataset.json --validate-output\n</code></pre>"},{"location":"extraction/overview/","title":"What is NeMo Retriever Extraction?","text":"<p>NeMo Retriever extraction is a scalable, performance-oriented document content and metadata extraction microservice.  NeMo Retriever extraction uses specialized NVIDIA NIM microservices  to find, contextualize, and extract text, tables, charts and infographics that you can use in downstream generative applications.</p> <p>Note</p> <p>NeMo Retriever extraction is also known as NVIDIA Ingest and nv-ingest.</p> <p>NeMo Retriever extraction enables parallelization of splitting documents into pages where artifacts are classified (such as text, tables, charts, and infographics), extracted, and further contextualized through optical character recognition (OCR) into a well defined JSON schema.  From there, NeMo Retriever extraction can optionally manage computation of embeddings for the extracted content,  and optionally manage storing into a vector database Milvus.</p> <p>Note</p> <p>Cached and Deplot are deprecated. Instead, NeMo Retriever extraction now uses the yolox-graphic-elements NIM. With this change, you should now be able to run NeMo Retriever extraction on a single 24GB A10G or better GPU. If you want to use the old pipeline, with Cached and Deplot, use the NeMo Retriever extraction 24.12.1 release.</p>"},{"location":"extraction/overview/#what-nemo-retriever-extraction-is","title":"What NeMo Retriever Extraction Is \u2714\ufe0f","text":"<p>The following diagram shows the Nemo Retriever extraction pipeline.</p> <p></p> <p>NeMo Retriever extraction is a microservice service that does the following:</p> <ul> <li>Accept a JSON job description, containing a document payload, and a set of ingestion tasks to perform on that payload.</li> <li>Allow the results of a job to be retrieved. The result is a JSON dictionary that contains a list of metadata describing objects extracted from the base document, and processing annotations and timing/trace data.</li> <li>Support multiple methods of extraction for each document type to balance trade-offs between throughput and accuracy. For example, for .pdf documents, extraction is performed by using pdfium, nemotron-parse, Unstructured.io, and Adobe Content Extraction Services.</li> <li>Support various types of pre- and post- processing operations, including text splitting and chunking, transform and filtering, embedding generation, and image offloading to storage.</li> </ul> <p>NeMo Retriever extraction supports the following file types:</p> <ul> <li><code>avi</code> (early access)</li> <li><code>bmp</code></li> <li><code>docx</code></li> <li><code>html</code> (converted to markdown format)</li> <li><code>jpeg</code></li> <li><code>json</code> (treated as text)</li> <li><code>md</code> (treated as text)</li> <li><code>mkv</code> (early access)</li> <li><code>mov</code> (early access)</li> <li><code>mp3</code></li> <li><code>mp4</code> (early access)</li> <li><code>pdf</code></li> <li><code>png</code></li> <li><code>pptx</code></li> <li><code>sh</code> (treated as text)</li> <li><code>tiff</code></li> <li><code>txt</code></li> <li><code>wav</code></li> </ul>"},{"location":"extraction/overview/#what-nemo-retriever-extraction-isnt","title":"What NeMo Retriever Extraction Isn't \u2716\ufe0f","text":"<p>NeMo Retriever extraction does not do the following:</p> <ul> <li>Run a static pipeline or fixed set of operations on every submitted document.</li> <li>Act as a wrapper for any specific document parsing library.</li> </ul>"},{"location":"extraction/overview/#related-topics","title":"Related Topics","text":"<ul> <li>Prerequisites</li> <li>Support Matrix</li> <li>Deploy Without Containers (Library Mode)</li> <li>Deploy With Docker Compose (Self-Hosted)</li> <li>Deploy With Helm</li> <li>Notebooks</li> <li>Multimodal PDF Data Extraction</li> </ul>"},{"location":"extraction/prerequisites/","title":"Prerequisites for NeMo Retriever Extraction","text":"<p>Before you begin using NeMo Retriever extraction, ensure the following software and hardware prerequisites are met.</p> <p>Note</p> <p>NeMo Retriever extraction is also known as NVIDIA Ingest and nv-ingest.</p>"},{"location":"extraction/prerequisites/#software-requirements","title":"Software Requirements","text":"<ul> <li>Linux operating systems (Ubuntu 22.04 or later recommended)</li> <li>Docker</li> <li>Docker Compose</li> <li>Docker Buildx <code>&gt;= 0.17</code> (Compose 2.40+ enforces this)</li> <li>CUDA Toolkit (NVIDIA Driver &gt;= <code>535</code>, CUDA &gt;= <code>12.2</code>)</li> <li>NVIDIA Container Toolkit</li> <li>Conda Python environment and package manager</li> </ul> <p>Note</p> <p>You install Python later.</p>"},{"location":"extraction/prerequisites/#hardware-requirements","title":"Hardware Requirements","text":"<p>The full ingestion pipeline is designed to consume significant CPU and memory resources to achieve maximal parallelism.  Resource usage scales up to the limits of your deployed system.</p> <p>For additional hardware details, refer to Support Matrix.</p>"},{"location":"extraction/prerequisites/#recommended-production-deployment-specifications","title":"Recommended Production Deployment Specifications","text":"<ul> <li>System Memory: At least 256 GB RAM</li> <li>CPU Cores: At least 32 CPU cores</li> <li>GPU: NVIDIA GPU with at least 24 GB VRAM (e.g., A100, V100, or equivalent)</li> </ul> <p>Note</p> <p>Using less powerful systems or lower resource limits is still viable, but performance will suffer.</p>"},{"location":"extraction/prerequisites/#resource-consumption-notes","title":"Resource Consumption Notes","text":"<ul> <li>The pipeline performs runtime allocation of parallel resources based on system configuration</li> <li>Memory usage can reach up to the full system capacity for large document processing</li> <li>CPU utilization scales with the number of concurrent processing tasks</li> <li>GPU is required for image processing NIMs, embeddings, and other GPU-accelerated tasks</li> </ul>"},{"location":"extraction/prerequisites/#scaling-considerations","title":"Scaling Considerations","text":"<p>For production deployments processing large volumes of documents, consider: - Higher memory configurations for processing large PDF files or image collections - Additional CPU cores for improved parallel processing - Multiple GPUs for distributed processing workloads</p> <p>For guidance on choosing between static and dynamic scaling modes, and how to configure them in <code>docker-compose.yaml</code>, see Scaling Modes.</p>"},{"location":"extraction/prerequisites/#environment-requirements","title":"Environment Requirements","text":"<p>Ensure your deployment environment meets these specifications before running the full pipeline. Resource-constrained environments may experience performance degradation.</p>"},{"location":"extraction/prerequisites/#related-topics","title":"Related Topics","text":"<ul> <li>Support Matrix</li> <li>Troubleshooting</li> <li>Deploy Without Containers (Library Mode)</li> <li>Deploy With Docker Compose (Self-Hosted)</li> <li>Deploy With Helm</li> </ul>"},{"location":"extraction/quickstart-guide/","title":"Deploy With Docker Compose (Self-Hosted) for NeMo Retriever Extraction","text":"<p>Use this documentation to get started using NeMo Retriever extraction in self-hosted mode.</p>"},{"location":"extraction/quickstart-guide/#step-1-starting-containers","title":"Step 1: Starting Containers","text":"<p>This example demonstrates how to use the provided docker-compose.yaml to start all needed services with a few commands.</p> <p>Warning</p> <p>NIM containers on their first startup can take 10-15 minutes to pull and fully load models.</p> <p>If you prefer, you can run on Kubernetes by using our Helm chart. Also, there are additional environment variables you might want to configure.</p> <ol> <li> <p>Git clone the repo:</p> <p><code>git clone https://github.com/nvidia/nv-ingest</code></p> </li> <li> <p>Change the directory to the cloned repo</p> <p><code>cd nv-ingest</code>.</p> </li> <li> <p>Generate API keys and authenticate with NGC with the <code>docker login</code> command:</p> <pre><code># This is required to access pre-built containers and NIM microservices\n$ docker login nvcr.io\nUsername: $oauthtoken\nPassword: &lt;Your Key&gt;\n</code></pre> </li> <li> <p>Create a .env file that contains your NVIDIA Build API key.</p> <p>Note</p> <p>If you use an NGC personal key, then you should provide the same value for all keys, but you must specify each environment variable individually. In the past, you could create an API key. If you have an API key, you can still use that. For more information, refer to Generate Your NGC Keys and Environment Configuration Variables.</p> <pre><code># Container images must access resources from NGC.\n\nNGC_API_KEY=&lt;key to download containers from NGC&gt;\nNIM_NGC_API_KEY=&lt;key to download model files after containers start&gt;\n</code></pre> </li> <li> <p>Make sure NVIDIA is set as your default container runtime before running the docker compose command with the command:</p> <p><code>sudo nvidia-ctk runtime configure --runtime=docker --set-as-default</code></p> </li> <li> <p>Start core services. This example uses the retrieval profile.  For more information about other profiles, see Profile Information.</p> <p><code>docker compose --profile retrieval up</code></p> <p>Tip</p> <p>By default, we have configured log levels to be verbose. It's possible to observe service startup proceeding. You will notice a lot of log messages. Disable verbose logging by configuring <code>NIM_TRITON_LOG_VERBOSE=0</code> for each NIM in docker-compose.yaml.</p> <p>Tip</p> <p>The default configuration may not fit on a single GPU for some hardware targets. If you are running on any of the following GPUs, use a <code>docker compose</code> override file to reduce VRAM usage: - A100-SXM4-40GB - A10G Override files typically lower per-service memory allocation, batch sizes, or concurrency. This trades peak throughput for making the full pipeline runnable on the available GPU. To use an override file, include it in your <code>docker compose up</code> command by using a second <code>-f</code> flag after the base <code>docker-compose.yaml</code> file. The settings in the second file override the values that are set in the first file.</p> <p>The following example uses an override file that contains settings that are optimized for an NVIDIA A100 GPU with 40GB of VRAM. <pre><code>docker compose \\\n  -f docker-compose.yaml \\\n  -f docker-compose.a100-40gb.yaml \\\n  --profile retrieval up\n</code></pre></p> </li> <li> <p>When core services have fully started, <code>nvidia-smi</code> should show processes like the following:</p> <pre><code># If it's taking &gt; 1m for `nvidia-smi` to return, the bus will likely be busy setting up the models.\n+---------------------------------------------------------------------------------------+\n| Processes:                                                                            |\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n|        ID   ID                                                             Usage      |\n|=======================================================================================|\n|    0   N/A  N/A     80461      C   milvus                                     1438MiB |\n|    0   N/A  N/A     83791      C   tritonserver                               2492MiB |\n|    0   N/A  N/A     85605      C   tritonserver                               1896MiB |\n|    0   N/A  N/A     85889      C   tritonserver                               2824MiB |\n|    0   N/A  N/A     88253      C   tritonserver                               2824MiB |\n|    0   N/A  N/A     91194      C   tritonserver                               4546MiB |\n+---------------------------------------------------------------------------------------+\n</code></pre> </li> <li> <p>Run the command <code>docker ps</code>. You should see output similar to the following. Confirm that the status of the containers is <code>Up</code>.</p> <pre><code>CONTAINER ID  IMAGE                                            COMMAND                 CREATED         STATUS                  PORTS            NAMES\n1b885f37c991  nvcr.io/nvidia/nemo-microservices/nv-ingest:...  \"/opt/conda/envs/nv_\u2026\"  7 minutes ago   Up 7 minutes (healthy)  0.0.0.0:7670...  nv-ingest-nv-ingest-ms-runtime-1\n14ef31ed7f49  milvusdb/milvus:v2.5.3-gpu                       \"/tini -- bash -c 's\u2026\"  7 minutes ago   Up 7 minutes (healthy)  0.0.0.0:9091...  milvus-standalone\ndceaf36cc5df  otel/opentelemetry-collector-contrib:...         \"/otelcol-contrib --\u2026\"  7 minutes ago   Up 7 minutes            0.0.0.0:4317...  nv-ingest-otel-collector-1\n5bd0b48eb71b  nvcr.io/nim/nvidia/nemoretriever-graphic-ele...  \"/opt/nvidia/nvidia_\u2026\"  7 minutes ago   Up 7 minutes            0.0.0.0:8003...  nv-ingest-graphic-elements-1\ndaf878669036  nvcr.io/nim/nvidia/nemoretriever-ocr-v1:1.2.1    \"/opt/nvidia/nvidia_\u2026\"  7 minutes ago   Up 7 minutes            0.0.0.0:8009...  nv-ingest-ocr-1\n216bdf11c566  nvcr.io/nim/nvidia/nemoretriever-page-elements-v3:1.7.0  \"/opt/nvidia/nvidia_\u2026\"  7 minutes ago   Up 7 minutes            0.0.0.0:8000...  nv-ingest-page-elements-1\naee9580b0b9a  nvcr.io/nim/nvidia/llama-3.2-nv-embedqa-1b-v2:1.10.0  \"/opt/nvidia/nvidia_\u2026\"  7 minutes ago   Up 7 minutes            0.0.0.0:8012...  nv-ingest-embedding-1\n178a92bf6f7f  nvcr.io/nim/nvidia/nemoretriever-table-struc...  \"/opt/nvidia/nvidia_\u2026\"  7 minutes ago   Up 7 minutes            0.0.0.0:8006...  nv-ingest-table-structure-1\n7ddbf7690036  openzipkin/zipkin                                \"start-zipkin\"          7 minutes ago   Up 7 minutes (healthy)  9410/tcp...      nv-ingest-zipkin-1\nb73bbe0c202d  minio/minio:RELEASE.2023-03-20T20-16-18Z         \"/usr/bin/docker-ent\u2026\"  7 minutes ago   Up 7 minutes (healthy)  0.0.0.0:9000...  minio\n97fa798dbe4f  prom/prometheus:latest                           \"/bin/prometheus --w\u2026\"  7 minutes ago   Up 7 minutes            0.0.0.0:9090...  nv-ingest-prometheus-1\nf17cb556b086  grafana/grafana                                  \"/run.sh\"               7 minutes ago   Up 7 minutes            0.0.0.0:3000...  grafana-service\n3403c5a0e7be  redis/redis-stack                                \"/entrypoint.sh\"        7 minutes ago   Up 7 minutes            0.0.0.0:6379...  nv-ingest-redis-1\n</code></pre> </li> </ol>"},{"location":"extraction/quickstart-guide/#step-2-install-python-dependencies","title":"Step 2: Install Python Dependencies","text":"<p>You can interact with the NV-Ingest service from the host, or by using <code>docker exec</code> to run commands in the NV-Ingest container.</p> <p>To interact from the host, you'll need a Python environment and install the client dependencies:</p> <pre><code># conda not required but makes it easy to create a fresh Python environment\nconda create --name nv-ingest-dev python=3.12.11\nconda activate nv-ingest-dev\npip install nv-ingest==26.1.2 nv-ingest-api==26.1.2 nv-ingest-client==26.1.2\n</code></pre> <p>Tip</p> <p>To confirm that you have activated your Conda environment, run <code>which pip</code> and <code>which python</code>, and confirm that you see <code>nvingest</code> in the result. You can do this before any pip or python command that you run.</p> <p>Note</p> <p>Interacting from the host depends on the appropriate port being exposed from the nv-ingest container to the host as defined in docker-compose.yaml. If you prefer, you can disable exposing that port and interact with the NV-Ingest service directly from within its container. To interact within the container run <code>docker exec -it nv-ingest-nv-ingest-ms-runtime-1 bash</code>. You'll be in the <code>/workspace</code> directory with <code>DATASET_ROOT</code> from the .env file mounted at <code>./data</code>. The pre-activated <code>nv_ingest_runtime</code> conda environment has all the Python client libraries pre-installed and you should see <code>(nv_ingest_runtime) root@aba77e2a4bde:/workspace#</code>. From the bash prompt above, you can run the nv-ingest-cli and Python examples described below. Because various service URIs default to <code>localhost</code>, running within the NV-Ingest container will also require URIs to be manually specified in order for services to be accessed between containers on the internal Docker network. See the code below for an example specifying <code>milvus_uri</code>.</p>"},{"location":"extraction/quickstart-guide/#step-3-ingesting-documents","title":"Step 3: Ingesting Documents","text":"<p>You can submit jobs programmatically in Python or using the NV-Ingest CLI.</p> <p>In the following examples, we do text, chart, table, and image extraction.</p> <ul> <li>extract_text \u2014 Uses PDFium to find and extract text from pages.</li> <li>extract_images \u2014 Uses PDFium to extract images.</li> <li>extract_tables \u2014 Uses object detection family of NIMs to find tables and charts, and NemoRetriever OCR for table extraction.</li> <li>extract_charts \u2014 Enables or disables chart extraction, also based on the object detection NIM family.</li> </ul>"},{"location":"extraction/quickstart-guide/#in-python","title":"In Python","text":"<p>Tip</p> <p>For more Python examples, refer to NV-Ingest: Python Client Quick Start Guide.</p> <p> <pre><code>import logging, os, time\nfrom nv_ingest_client.client import Ingestor, NvIngestClient\nfrom nv_ingest_client.util.process_json_files import ingest_json_results_to_blob\nclient = NvIngestClient(                                                                         \n    message_client_port=7670,                                                               \n    message_client_hostname=\"localhost\"        \n)                                                                 \n# do content extraction from files                               \ningestor = (\n    Ingestor(client=client)\n    .files(\"data/multimodal_test.pdf\")\n    .extract(             \n        extract_text=True,\n        extract_tables=True,\n        extract_charts=True,\n        extract_images=True,\n        table_output_format=\"markdown\",\n        extract_infographics=True,\n        # extract_method=\"nemotron_parse\", # Slower, but maximally accurate, especially for PDFs with pages that are scanned images\n        text_depth=\"page\"\n    ).embed()\n    .vdb_upload(\n        collection_name=\"test\",\n        sparse=False,\n        # for llama-3.2 embedder, use 1024 for e5-v5\n        dense_dim=2048,\n        # milvus_uri=\"http://milvus:19530\"  # When running from within a container, the URI to the Milvus service is specified using the internal Docker network.\n    )\n)\n\nprint(\"Starting ingestion..\")\nt0 = time.time()\n\n# Return both successes and failures\n# Use for large batches where you want successful chunks/pages to be committed, while collecting detailed diagnostics for failures.\nresults, failures = ingestor.ingest(show_progress=True, return_failures=True)\n\n# Return only successes\n# results = ingestor.ingest(show_progress=True)\n\nt1 = time.time()\nprint(f\"Total time: {t1-t0} seconds\")\n\n# results blob is directly inspectable\nprint(ingest_json_results_to_blob(results[0]))\n\nif failures:\n    print(f\"There were {len(failures)} failures. Sample: {failures[0]}\")\n</code></pre></p> <p>Note</p> <p>For advanced visual parsing in self-hosted mode, uncomment <code>extract_method=\"nemotron_parse\"</code> in the previous code. For more information, refer to Advanced Visual Parsing.</p> <p>The output looks similar to the following.</p> <pre><code>Starting ingestion..\n1 records to insert to milvus\nlogged 8 records\nTotal time: 5.479151725769043 seconds\nThis chart shows some gadgets, and some very fictitious costs. Gadgets and their cost   Chart 1 - Hammer - Powerdrill - Bluetooth speaker - Minifridge - Premium desk fan Dollars $- - $20.00 - $40.00 - $60.00 - $80.00 - $100.00 - $120.00 - $140.00 - $160.00 Cost\nTable 1\n| This table describes some animals, and some activities they might be doing in specific locations. | This table describes some animals, and some activities they might be doing in specific locations. | This table describes some animals, and some activities they might be doing in specific locations. |\n| Animal | Activity | Place |\n| Giraffe | Driving a car | At the beach |\n| Lion | Putting on sunscreen | At the park |\n| Cat | Jumping onto a laptop | In a home office |\n| Dog | Chasing a squirrel | In the front yard |\nTestingDocument\nA sample document with headings and placeholder text\nIntroduction\nThis is a placeholder document that can be used for any purpose. It contains some \nheadings and some placeholder text to fill the space. The text is not important and contains \nno real value, but it is useful for testing. Below, we will have some simple tables and charts \nthat we can use to confirm Ingest is working as expected.\nTable 1\nThis table describes some animals, and some activities they might be doing in specific \nlocations.\nAnimal Activity Place\nGira@e Driving a car At the beach\nLion Putting on sunscreen At the park\nCat Jumping onto a laptop In a home o@ice\nDog Chasing a squirrel In the front yard\nChart 1\nThis chart shows some gadgets, and some very fictitious costs.\nimage_caption:[]\nimage_caption:[]\nBelow,is a high-quality picture of some shapes          Picture\nTable 2\n| This table shows some popular colors that cars might come in | This table shows some popular colors that cars might come in | This table shows some popular colors that cars might come in | This table shows some popular colors that cars might come in |\n| Car | Color1 | Color2 | Color3 |\n| Coupe | White | Silver | Flat Gray |\n| Sedan | White | Metallic Gray | Matte Gray |\n| Minivan | Gray | Beige | Black |\n| Truck | Dark Gray | Titanium Gray | Charcoal |\n| Convertible | Light Gray | Graphite | Slate Gray |\nSection One\nThis is the first section of the document. It has some more placeholder text to show how \nthe document looks like. The text is not meant to be meaningful or informative, but rather to \ndemonstrate the layout and formatting of the document.\n\u2022 This is the first bullet point\n\u2022 This is the second bullet point\n\u2022 This is the third bullet point\nSection Two\nThis is the second section of the document. It is more of the same as we\u2019ve seen in the rest \nof the document. The content is meaningless, but the intent is to create a very simple \nsmoke test to ensure extraction is working as intended. This will be used in CI as time goes \non to ensure that changes we make to the library do not negatively impact our accuracy.\nTable 2\nThis table shows some popular colors that cars might come in.\nCar Color1 Color2 Color3\nCoupe White Silver Flat Gray\nSedan White Metallic Gray Matte Gray\nMinivan Gray Beige Black\nTruck Dark Gray Titanium Gray Charcoal\nConvertible Light Gray Graphite Slate Gray\nPicture\nBelow, is a high-quality picture of some shapes.\nimage_caption:[]\nimage_caption:[]\nThis chart shows some average frequency ranges for speaker drivers. Frequency Ranges ofSpeaker Drivers   Tweeter - Midrange - Midwoofer - Subwoofer Chart2 Hertz (log scale) 1 - 10 - 100 - 1000 - 10000 - 100000 FrequencyRange Start (Hz) - Frequency Range End (Hz)\nChart 2\nThis chart shows some average frequency ranges for speaker drivers.\nConclusion\nThis is the conclusion of the document. It has some more placeholder text, but the most \nimportant thing is that this is the conclusion. As we end this document, we should have \nbeen able to extract 2 tables, 2 charts, and some text including 3 bullet points.\nimage_caption:[]\n</code></pre>"},{"location":"extraction/quickstart-guide/#using-the-nv-ingest-cli","title":"Using the <code>nv-ingest-cli</code>","text":"<p>Tip</p> <p>There is a Jupyter notebook available to help you get started with the CLI. For more information, refer to CLI Client Quick Start Guide.</p> <p> <pre><code>nv-ingest-cli \\\n  --doc ./data/multimodal_test.pdf \\\n  --output_directory ./processed_docs \\\n  --task='extract:{\"document_type\": \"pdf\", \"extract_method\": \"pdfium\", \"extract_tables\": \"true\", \"extract_images\": \"true\", \"extract_charts\": \"true\"}' \\\n  --client_host=localhost \\\n  --client_port=7670\n</code></pre></p> <p>You should see output that indicates the document processing status followed by a breakdown of time spent during job execution.</p> <pre><code>None of PyTorch, TensorFlow &gt;= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n[nltk_data] Downloading package punkt_tab to\n[nltk_data]     /raid/jdyer/miniforge3/envs/nv-ingest-\n[nltk_data]     dev/lib/python3.10/site-\n[nltk_data]     packages/llama_index/core/_static/nltk_cache...\n[nltk_data]   Package punkt_tab is already up-to-date!\nINFO:nv_ingest_client.nv_ingest_cli:Processing 1 documents.\nINFO:nv_ingest_client.nv_ingest_cli:Output will be written to: ./processed_docs\nProcessing files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:02&lt;00:00,  2.34s/file, pages_per_sec=1.28]\nINFO:nv_ingest_client.cli.util.processing:message_broker_task_source: Avg: 2.39 ms, Median: 2.39 ms, Total Time: 2.39 ms, Total % of Trace Computation: 0.06%\nINFO:nv_ingest_client.cli.util.processing:broker_source_network_in: Avg: 9.51 ms, Median: 9.51 ms, Total Time: 9.51 ms, Total % of Trace Computation: 0.25%\nINFO:nv_ingest_client.cli.util.processing:job_counter: Avg: 1.47 ms, Median: 1.47 ms, Total Time: 1.47 ms, Total % of Trace Computation: 0.04%\nINFO:nv_ingest_client.cli.util.processing:job_counter_channel_in: Avg: 0.46 ms, Median: 0.46 ms, Total Time: 0.46 ms, Total % of Trace Computation: 0.01%\nINFO:nv_ingest_client.cli.util.processing:metadata_injection: Avg: 3.52 ms, Median: 3.52 ms, Total Time: 3.52 ms, Total % of Trace Computation: 0.09%\nINFO:nv_ingest_client.cli.util.processing:metadata_injection_channel_in: Avg: 0.16 ms, Median: 0.16 ms, Total Time: 0.16 ms, Total % of Trace Computation: 0.00%\nINFO:nv_ingest_client.cli.util.processing:pdf_content_extractor: Avg: 475.64 ms, Median: 163.77 ms, Total Time: 2378.21 ms, Total % of Trace Computation: 62.73%\nINFO:nv_ingest_client.cli.util.processing:pdf_content_extractor_channel_in: Avg: 0.31 ms, Median: 0.31 ms, Total Time: 0.31 ms, Total % of Trace Computation: 0.01%\nINFO:nv_ingest_client.cli.util.processing:image_content_extractor: Avg: 0.67 ms, Median: 0.67 ms, Total Time: 0.67 ms, Total % of Trace Computation: 0.02%\nINFO:nv_ingest_client.cli.util.processing:image_content_extractor_channel_in: Avg: 0.21 ms, Median: 0.21 ms, Total Time: 0.21 ms, Total % of Trace Computation: 0.01%\nINFO:nv_ingest_client.cli.util.processing:docx_content_extractor: Avg: 0.46 ms, Median: 0.46 ms, Total Time: 0.46 ms, Total % of Trace Computation: 0.01%\nINFO:nv_ingest_client.cli.util.processing:docx_content_extractor_channel_in: Avg: 0.20 ms, Median: 0.20 ms, Total Time: 0.20 ms, Total % of Trace Computation: 0.01%\nINFO:nv_ingest_client.cli.util.processing:pptx_content_extractor: Avg: 0.68 ms, Median: 0.68 ms, Total Time: 0.68 ms, Total % of Trace Computation: 0.02%\nINFO:nv_ingest_client.cli.util.processing:pptx_content_extractor_channel_in: Avg: 0.46 ms, Median: 0.46 ms, Total Time: 0.46 ms, Total % of Trace Computation: 0.01%\nINFO:nv_ingest_client.cli.util.processing:audio_data_extraction: Avg: 1.08 ms, Median: 1.08 ms, Total Time: 1.08 ms, Total % of Trace Computation: 0.03%\nINFO:nv_ingest_client.cli.util.processing:audio_data_extraction_channel_in: Avg: 0.20 ms, Median: 0.20 ms, Total Time: 0.20 ms, Total % of Trace Computation: 0.01%\nINFO:nv_ingest_client.cli.util.processing:dedup_images: Avg: 0.42 ms, Median: 0.42 ms, Total Time: 0.42 ms, Total % of Trace Computation: 0.01%\nINFO:nv_ingest_client.cli.util.processing:dedup_images_channel_in: Avg: 0.42 ms, Median: 0.42 ms, Total Time: 0.42 ms, Total % of Trace Computation: 0.01%\nINFO:nv_ingest_client.cli.util.processing:filter_images: Avg: 0.59 ms, Median: 0.59 ms, Total Time: 0.59 ms, Total % of Trace Computation: 0.02%\nINFO:nv_ingest_client.cli.util.processing:filter_images_channel_in: Avg: 0.57 ms, Median: 0.57 ms, Total Time: 0.57 ms, Total % of Trace Computation: 0.02%\nINFO:nv_ingest_client.cli.util.processing:table_data_extraction: Avg: 240.75 ms, Median: 240.75 ms, Total Time: 481.49 ms, Total % of Trace Computation: 12.70%\nINFO:nv_ingest_client.cli.util.processing:table_data_extraction_channel_in: Avg: 0.38 ms, Median: 0.38 ms, Total Time: 0.38 ms, Total % of Trace Computation: 0.01%\nINFO:nv_ingest_client.cli.util.processing:chart_data_extraction: Avg: 300.54 ms, Median: 299.94 ms, Total Time: 901.62 ms, Total % of Trace Computation: 23.78%\nINFO:nv_ingest_client.cli.util.processing:chart_data_extraction_channel_in: Avg: 0.23 ms, Median: 0.23 ms, Total Time: 0.23 ms, Total % of Trace Computation: 0.01%\nINFO:nv_ingest_client.cli.util.processing:infographic_data_extraction: Avg: 0.77 ms, Median: 0.77 ms, Total Time: 0.77 ms, Total % of Trace Computation: 0.02%\nINFO:nv_ingest_client.cli.util.processing:infographic_data_extraction_channel_in: Avg: 0.25 ms, Median: 0.25 ms, Total Time: 0.25 ms, Total % of Trace Computation: 0.01%\nINFO:nv_ingest_client.cli.util.processing:caption_ext: Avg: 0.55 ms, Median: 0.55 ms, Total Time: 0.55 ms, Total % of Trace Computation: 0.01%\nINFO:nv_ingest_client.cli.util.processing:caption_ext_channel_in: Avg: 0.51 ms, Median: 0.51 ms, Total Time: 0.51 ms, Total % of Trace Computation: 0.01%\nINFO:nv_ingest_client.cli.util.processing:embed_text: Avg: 1.21 ms, Median: 1.21 ms, Total Time: 1.21 ms, Total % of Trace Computation: 0.03%\nINFO:nv_ingest_client.cli.util.processing:embed_text_channel_in: Avg: 0.21 ms, Median: 0.21 ms, Total Time: 0.21 ms, Total % of Trace Computation: 0.01%\nINFO:nv_ingest_client.cli.util.processing:store_embedding_minio: Avg: 0.32 ms, Median: 0.32 ms, Total Time: 0.32 ms, Total % of Trace Computation: 0.01%\nINFO:nv_ingest_client.cli.util.processing:store_embedding_minio_channel_in: Avg: 1.18 ms, Median: 1.18 ms, Total Time: 1.18 ms, Total % of Trace Computation: 0.03%\nINFO:nv_ingest_client.cli.util.processing:message_broker_task_sink_channel_in: Avg: 0.42 ms, Median: 0.42 ms, Total Time: 0.42 ms, Total % of Trace Computation: 0.01%\nINFO:nv_ingest_client.cli.util.processing:No unresolved time detected. Trace times account for the entire elapsed duration.\nINFO:nv_ingest_client.cli.util.processing:Processed 1 files in 2.34 seconds.\nINFO:nv_ingest_client.cli.util.processing:Total pages processed: 3\nINFO:nv_ingest_client.cli.util.processing:Throughput (Pages/sec): 1.28\nINFO:nv_ingest_client.cli.util.processing:Throughput (Files/sec): 0.43\n</code></pre>"},{"location":"extraction/quickstart-guide/#step-4-inspecting-and-consuming-results","title":"Step 4: Inspecting and Consuming Results","text":"<p>After the ingestion steps above have been completed, you should be able to find the <code>text</code> and <code>image</code> subfolders inside your processed docs folder. Each will contain JSON-formatted extracted content and metadata.</p> <p>When processing has completed, you'll have separate result files for text and image data: <pre><code>ls -R processed_docs/\n</code></pre> <pre><code>processed_docs/:\nimage  structured  text\n\nprocessed_docs/image:\nmultimodal_test.pdf.metadata.json\n\nprocessed_docs/structured:\nmultimodal_test.pdf.metadata.json\n\nprocessed_docs/text:\nmultimodal_test.pdf.metadata.json\n</code></pre></p> <p>For the full metadata definitions, refer to Content Metadata. </p> <p>We also provide a script for inspecting extracted images.</p> <p>First, install <code>tkinter</code> by running the following code. Choose the code for your OS.</p> <ul> <li> <p>For Ubuntu/Debian Linux:</p> <pre><code>sudo apt-get update\nsudo apt-get install python3-tk\n</code></pre> </li> <li> <p>For Fedora/RHEL Linux:</p> <pre><code>sudo dnf install python3-tkinter\n</code></pre> </li> <li> <p>For macOS using Homebrew:</p> <pre><code>brew install python-tk\n</code></pre> </li> </ul> <p>Then, run the following command to execute the script for inspecting the extracted image:</p> <pre><code>python src/util/image_viewer.py --file_path ./processed_docs/image/multimodal_test.pdf.metadata.json\n</code></pre> <p>Tip</p> <p>Beyond inspecting the results, you can read them into things like llama-index or langchain retrieval pipelines. Also, checkout our demo using a retrieval pipeline on build.nvidia.com to query over document content pre-extracted with NV-Ingest.</p>"},{"location":"extraction/quickstart-guide/#profile-information","title":"Profile Information","text":"<p>The values that you specify in the <code>--profile</code> option of your <code>docker compose up</code> command are explained in the following table.  You can specify multiple <code>--profile</code> options.</p> Profile Type Description <code>retrieval</code> Core Enables the embedding NIM and (GPU accelerated) Milvus. <code>audio</code> Advanced Use Riva for processing audio files. For more information, refer to Audio Processing. <code>nemotron-parse</code> Advanced Use nemotron-parse, which adds state-of-the-art text and table extraction. For more information, refer to Advanced Visual Parsing. <code>vlm</code> Advanced Use llama 3.1 Nemotron 8B Vision for experimental image captioning of unstructured images."},{"location":"extraction/quickstart-guide/#specify-mig-slices-for-nim-models","title":"Specify MIG slices for NIM models","text":"<p>When you deploy NV-Ingest with NIM models on MIG\u2011enabled GPUs, MIG device slices are requested and scheduled through the <code>values.yaml</code> file for the corresponding NIM microservice. For IBM Content-Aware Storage (CAS) deployments, this allows NV-Ingest NIM pods to land only on nodes that expose the desired MIG profiles raw.githubusercontent.\u200b</p> <p>To target a specific MIG profile\u2014for example, a 3g.20gb slice on an A100, which is a hardware-partitioned virtual GPU instance that gives your workload a fixed mid-sized share of the A100\u2019s compute plus 20 GB of dedicated GPU memory and behaves like a smaller independent GPU\u2014for a given NIM, configure the <code>resources</code> and <code>nodeSelector</code> under that NIM\u2019s values path in <code>values.yaml</code>.</p> <p>The following example shows the pattern. Paths vary by NIM, such as <code>nvingest.nvidiaNim.nemoretrieverPageElements</code> instead of the generic <code>nvingest.nim</code> placeholder. For details refer to catalog.ngc.nvidia\u200b.</p> <p>Set <code>resources.requests</code> and <code>resources.limits</code> to the name of the MIG resource that you want (for example, <code>nvidia.com/mig-3g.20gb</code>).</p> <p><pre><code>nvingest:\n  nvidiaNim:\n    nemoretrieverPageElements:\n      modelName: \"meta/llama3-8b-instruct\"        # Example NIM model\n      resources:\n        limits:\n          nvidia.com/mig-3g.20gb: 1               # MIG profile resource\n        requests:\n          nvidia.com/mig-3g.20gb: 1\n      nodeSelector:\n        nvidia.com/gpu.product: A100-SXM4-40GB-MIG-3g.20gb\n</code></pre> Key points: * Use the appropriate NIM\u2011specific values path (for example, <code>nvingest.nvidiaNim.nemoretrieverPageElements.resources</code>) rather than the generic <code>nvingest.nim</code> placeholder. * Set <code>resources.requests</code> and <code>resources.limits</code> to the desired MIG resource name (for example, <code>nvidia.com/mig-3g.20gb</code>). * Use <code>nodeSelector</code> (or tolerations/affinity, if you prefer) to target nodes labeled with the corresponding MIG\u2011enabled GPU product (for example, <code>nvidia.com/gpu.product: A100-SXM4-40GB-MIG-3g.20gb</code>).</p> <p>This syntax and structure can be repeated for each NIM model used by CAS, ensuring that each NV-Ingest NIM pod is mapped to the correct MIG slice type and scheduled onto compatible nodes.</p> <p>Important</p> <p>Advanced features require additional GPU support and disk space. For more information, refer to Support Matrix.</p>"},{"location":"extraction/quickstart-guide/#related-topics","title":"Related Topics","text":"<ul> <li>Troubleshoot</li> <li>Prerequisites</li> <li>Support Matrix</li> <li>Deploy Without Containers (Library Mode)</li> <li>Deploy With Helm</li> <li>Notebooks</li> <li>Multimodal PDF Data Extraction</li> </ul>"},{"location":"extraction/quickstart-library-mode/","title":"Deploy Without Containers (Library Mode) for NeMo Retriever Extraction","text":"<p>NeMo Retriever extraction is typically deployed as a cluster of containers for robust, scalable production use. </p> <p>Note</p> <p>NeMo Retriever extraction is also known as NVIDIA Ingest and nv-ingest.</p> <p>In addition, you can use library mode, which is intended for the following cases:</p> <ul> <li>Local development</li> <li>Experimentation and testing</li> <li>Small-scale workloads, such as workloads of fewer than 100 documents</li> </ul> <p>By default, library mode depends on NIMs that are hosted on build.nvidia.com.  In library mode you launch the main pipeline service directly within a Python process,  while all other services (such as embedding and storage) are hosted remotely in the cloud.</p> <p>To get started using library mode, you need the following:</p> <ul> <li>Linux operating systems (Ubuntu 22.04 or later recommended) or MacOS</li> <li>Python 3.12</li> <li>We strongly advise using an isolated Python virtual env, such as provided by uv or conda</li> </ul>"},{"location":"extraction/quickstart-library-mode/#step-1-prepare-your-environment","title":"Step 1: Prepare Your Environment","text":"<p>Use the following procedure to prepare your environment.</p> <ol> <li> <p>Run the following code to create your NV Ingest Conda environment.</p> <pre><code>   uv venv --python 3.12 nvingest &amp;&amp; \\\n     source nvingest/bin/activate &amp;&amp; \\\n     uv pip install nv-ingest==26.1.2 nv-ingest-api==26.1.2 nv-ingest-client==26.1.2 milvus-lite==2.4.12\n</code></pre> <p>Tip</p> <p>To confirm that you have activated your Conda environment, run <code>which python</code> and confirm that you see <code>nvingest</code> in the result. You can do this before any python command that you run.</p> </li> <li> <p>Set or create a .env file that contains your NVIDIA Build API key and other environment variables.</p> <p>Note</p> <p>If you have an NGC API key, you can use it here. For more information, refer to Generate Your NGC Keys and Environment Configuration Variables.</p> <ul> <li> <p>To set your variables, use the following code.</p> <p><pre><code>export NVIDIA_API_KEY=nvapi-&lt;your key&gt;\n</code></pre>     - To add your variables to a .env file, include the following.</p> <pre><code>NVIDIA_API_KEY=nvapi-&lt;your key&gt;\n</code></pre> </li> </ul> </li> </ol>"},{"location":"extraction/quickstart-library-mode/#step-2-ingest-documents","title":"Step 2: Ingest Documents","text":"<p>You can submit jobs programmatically by using Python.</p> <p>Tip</p> <p>For more Python examples, refer to NV-Ingest: Python Client Quick Start Guide.</p> <p>If you have a very high number of CPUs, and see the process hang without progress,  we recommend that you use <code>taskset</code> to limit the number of CPUs visible to the process.  Use the following code.</p> <pre><code>taskset -c 0-3 python your_ingestion_script.py\n</code></pre> <p>On a 4 CPU core low end laptop, the following code should take about 10 seconds.</p> <pre><code>import time\n\nfrom nv_ingest.framework.orchestration.ray.util.pipeline.pipeline_runners import run_pipeline\nfrom nv_ingest_client.client import Ingestor, NvIngestClient\nfrom nv_ingest_api.util.message_brokers.simple_message_broker import SimpleClient\nfrom nv_ingest_client.util.process_json_files import ingest_json_results_to_blob\n\ndef main():\n    # Start the pipeline subprocess for library mode\n    run_pipeline(block=False, disable_dynamic_scaling=True, run_in_subprocess=True)\n\n    client = NvIngestClient(\n        message_client_allocator=SimpleClient,\n        message_client_port=7671,\n        message_client_hostname=\"localhost\",\n    )\n\n    # gpu_cagra accelerated indexing is not available in milvus-lite\n    # Provide a filename for milvus_uri to use milvus-lite\n    milvus_uri = \"milvus.db\"\n    collection_name = \"test\"\n    sparse = False\n\n    # do content extraction from files\n    ingestor = (\n        Ingestor(client=client)\n        .files(\"data/multimodal_test.pdf\")\n        .extract(\n            extract_text=True,\n            extract_tables=True,\n            extract_charts=True,\n            extract_images=True,\n            table_output_format=\"markdown\",\n            extract_infographics=True,\n            # extract_method=\"nemotron_parse\", #Slower, but maximally accurate, especially for PDFs with pages that are scanned images\n            text_depth=\"page\",\n        )\n        .embed()\n        .vdb_upload(\n            collection_name=collection_name,\n            milvus_uri=milvus_uri,\n            sparse=sparse,\n            # for llama-3.2 embedder, use 1024 for e5-v5\n            dense_dim=2048,\n        )\n    )\n\n    print(\"Starting ingestion..\")\n    t0 = time.time()\n\n    # Return both successes and failures\n    # Use for large batches where you want successful chunks/pages to be committed, while collecting detailed diagnostics for failures.\n    results, failures = ingestor.ingest(show_progress=True, return_failures=True)\n\n    # Return only successes\n    # results = ingestor.ingest(show_progress=True)\n\n    t1 = time.time()\n    print(f\"Total time: {t1 - t0} seconds\")\n\n    # results blob is directly inspectable\n    if results:\n        print(ingest_json_results_to_blob(results[0]))\n\n    # (optional) Review any failures that were returned\n    if failures:\n        print(f\"There were {len(failures)} failures. Sample: {failures[0]}\")\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>Note</p> <p>For advanced visual parsing with library mode, uncomment <code>extract_method=\"nemotron_parse\"</code> in the previous code. For more information, refer to Advanced Visual Parsing.</p> <p>You can see the extracted text that represents the content of the ingested test document.</p> <pre><code>Starting ingestion..\nTotal time: 9.243880033493042 seconds\n\nTestingDocument\nA sample document with headings and placeholder text\nIntroduction\nThis is a placeholder document that can be used for any purpose. It contains some \nheadings and some placeholder text to fill the space. The text is not important and contains \nno real value, but it is useful for testing. Below, we will have some simple tables and charts \nthat we can use to confirm Ingest is working as expected.\nTable 1\nThis table describes some animals, and some activities they might be doing in specific \nlocations.\nAnimal Activity Place\nGira@e Driving a car At the beach\nLion Putting on sunscreen At the park\nCat Jumping onto a laptop In a home o@ice\nDog Chasing a squirrel In the front yard\nChart 1\nThis chart shows some gadgets, and some very fictitious costs.\n\n... document extract continues ...\n</code></pre>"},{"location":"extraction/quickstart-library-mode/#step-3-query-ingested-content","title":"Step 3: Query Ingested Content","text":"<p>To query for relevant snippets of the ingested content, and use them with an LLM to generate answers, use the following code.</p> <pre><code>import os\nfrom openai import OpenAI\nfrom nv_ingest_client.util.milvus import nvingest_retrieval\n\nmilvus_uri = \"milvus.db\"\ncollection_name = \"test\"\nsparse=False\n\nqueries = [\"Which animal is responsible for the typos?\"]\n\nretrieved_docs = nvingest_retrieval(\n    queries,\n    collection_name,\n    milvus_uri=milvus_uri,\n    hybrid=sparse,\n    top_k=1,\n)\n\n# simple generation example\nextract = retrieved_docs[0][0][\"entity\"][\"text\"]\nclient = OpenAI(\n  base_url = \"https://integrate.api.nvidia.com/v1\",\n  api_key = os.environ[\"NVIDIA_API_KEY\"]\n)\n\nprompt = f\"Using the following content: {extract}\\n\\n Answer the user query: {queries[0]}\"\nprint(f\"Prompt: {prompt}\")\ncompletion = client.chat.completions.create(\n  model=\"nvidia/llama-3.1-nemotron-nano-vl-8b-v1\",\n  messages=[{\"role\":\"user\",\"content\": prompt}],\n)\nresponse = completion.choices[0].message.content\n\nprint(f\"Answer: {response}\")\n</code></pre> <pre><code>Prompt: Using the following content: Table 1\n| This table describes some animals, and some activities they might be doing in specific locations. | This table describes some animals, and some activities they might be doing in specific locations. | This table describes some animals, and some activities they might be doing in specific locations. |\n| Animal | Activity | Place |\n| Giraffe | Driving a car | At the beach |\n| Lion | Putting on sunscreen | At the park |\n| Cat | Jumping onto a laptop | In a home office |\n| Dog | Chasing a squirrel | In the front yard |\n\n Answer the user query: Which animal is responsible for the typos?\nAnswer: A clever query!\n\nBased on the provided Table 1, I'd make an educated inference to answer your question. Since the activities listed are quite unconventional for the respective animals (e.g., a giraffe driving a car, a lion putting on sunscreen), it's likely that the table is using humor or hypothetical scenarios.\n\nGiven this context, the question \"Which animal is responsible for the typos?\" is probably a tongue-in-cheek inquiry, as there's no direct information in the table about typos or typing activities.\n\nHowever, if we were to make a playful connection, we could look for an animal that's:\n\n1. Typically found in a setting where typing might occur (e.g., an office).\n2. Engaging in an activity that could potentially lead to typos (e.g., interacting with a typing device).\n\nBased on these loose criteria, I'd jokingly point to:\n\n**Cat** as the potential culprit, since it's:\n        * Located \"In a home office\"\n        * Engaged in \"Jumping onto a laptop\", which could theoretically lead to accidental keystrokes or typos if the cat were to start \"walking\" on the keyboard!\n\nPlease keep in mind that this response is purely humorous and interpretative, as the table doesn't explicitly mention typos or provide a straightforward answer to the question.\n</code></pre>"},{"location":"extraction/quickstart-library-mode/#logging-configuration","title":"Logging Configuration","text":"<p>Nemo Retriever extraction uses Ray for logging.  For details, refer to Configure Ray Logging.</p> <p>By default, library mode runs in quiet mode to minimize startup noise.  Quiet mode automatically configures the following environment variables.</p> Variable Quiet Mode Value Description <code>INGEST_RAY_LOG_LEVEL</code> <code>PRODUCTION</code> Sets Ray logging to ERROR level to reduce noise. <code>RAY_ACCEL_ENV_VAR_OVERRIDE_ON_ZERO</code> <code>0</code> Silences Ray accelerator warnings <code>OTEL_SDK_DISABLED</code> <code>true</code> Disables OpenTelemetry trace export errors <p>If you want to see detailed startup logs for debugging, use one of the following options:</p> <ul> <li> <p>Set <code>quiet=False</code> when you run the pipeline as shown following.</p> <pre><code>run_pipeline(block=False, disable_dynamic_scaling=True, run_in_subprocess=True, quiet=False)\n</code></pre> </li> <li> <p>Set the environment variables manually before you run the pipeline as shown following.</p> <pre><code>export INGEST_RAY_LOG_LEVEL=DEVELOPMENT  # or DEBUG for maximum verbosity\n</code></pre> </li> </ul>"},{"location":"extraction/quickstart-library-mode/#library-mode-communication-and-advanced-examples","title":"Library Mode Communication and Advanced Examples","text":"<p>Communication in library mode is handled through a simplified, 3-way handshake message broker called <code>SimpleBroker</code>.</p> <p>Attempting to run a library-mode process co-located with a Docker Compose deployment does not work by default.  The Docker Compose deployment typically creates a firewall rule or port mapping that captures traffic to port <code>7671</code>, which prevents the <code>SimpleBroker</code> from receiving messages.  Always ensure that you use library mode in isolation, without an active containerized deployment listening on the same port.</p>"},{"location":"extraction/quickstart-library-mode/#example-launch_libmode_servicepy","title":"Example <code>launch_libmode_service.py</code>","text":"<p>This example launches the pipeline service in a subprocess,  and keeps it running until it is interrupted (for example, by pressing <code>Ctrl+C</code>).  It listens for ingestion requests on port <code>7671</code> from an external client.</p> <pre><code>import logging\nimport os\n\nfrom nv_ingest.framework.orchestration.ray.util.pipeline.pipeline_runners import run_pipeline\nfrom nv_ingest_api.util.logging.configuration import configure_logging as configure_local_logging\n\n# Configure the logger\nlogger = logging.getLogger(__name__)\n\nlocal_log_level = os.getenv(\"INGEST_LOG_LEVEL\", \"DEFAULT\")\nif local_log_level in (\"DEFAULT\",):\n    local_log_level = \"INFO\"\n\nconfigure_local_logging(local_log_level)\n\n\ndef main():\n    \"\"\"\n    Launch the libmode pipeline service using the embedded default configuration.\n    \"\"\"\n    try:\n        # Start pipeline and block until interrupted\n        # Note: stdout/stderr cannot be passed when run_in_subprocess=True (not picklable)\n        # Use quiet=False to see verbose startup logs\n        _ = run_pipeline(\n            block=True,\n            disable_dynamic_scaling=True,\n            run_in_subprocess=True,\n        )\n    except KeyboardInterrupt:\n        logger.info(\"Keyboard interrupt received. Shutting down...\")\n    except Exception as e:\n        logger.error(f\"An unexpected error occurred: {e}\", exc_info=True)\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"extraction/quickstart-library-mode/#example-launch_libmode_and_run_ingestorpy","title":"Example <code>launch_libmode_and_run_ingestor.py</code>","text":"<p>This example starts the pipeline service in-process,  and immediately runs an ingestion client against it in the same parent process.</p> <pre><code>import logging\nimport os\nimport time\n\nfrom nv_ingest.framework.orchestration.ray.util.pipeline.pipeline_runners import run_pipeline\nfrom nv_ingest_api.util.logging.configuration import configure_logging as configure_local_logging\nfrom nv_ingest_api.util.message_brokers.simple_message_broker import SimpleClient\nfrom nv_ingest_client.client import Ingestor\nfrom nv_ingest_client.client import NvIngestClient\n\n# Configure the logger\nlogger = logging.getLogger(__name__)\n\nlocal_log_level = os.getenv(\"INGEST_LOG_LEVEL\", \"INFO\")\nif local_log_level in (\"DEFAULT\",):\n    local_log_level = \"INFO\"\n\nconfigure_local_logging(local_log_level)\n\n\ndef run_ingestor():\n    \"\"\"\n    Set up and run the ingestion process to send traffic against the pipeline.\n    \"\"\"\n    logger.info(\"Setting up Ingestor client...\")\n    client = NvIngestClient(\n        message_client_allocator=SimpleClient, message_client_port=7671, message_client_hostname=\"localhost\"\n    )\n\n    ingestor = (\n        Ingestor(client=client)\n        .files(\"./data/multimodal_test.pdf\")\n        .extract(\n            extract_text=True,\n            extract_tables=True,\n            extract_charts=True,\n            extract_images=True,\n            table_output_format=\"markdown\",\n            extract_infographics=False,\n            text_depth=\"page\",\n        )\n        .split(chunk_size=1024, chunk_overlap=150)\n        .embed()\n    )\n\n    try:\n        results, _ = ingestor.ingest(show_progress=False, return_failures=True)\n        logger.info(\"Ingestion completed successfully.\")\n    except Exception as e:\n        logger.error(f\"Ingestion failed: {e}\")\n        raise\n\n    print(\"\\nIngest done.\")\n    print(f\"Got {len(results)} results.\")\n\n\ndef main():\n    \"\"\"\n    Launch the libmode pipeline service and run the ingestor against it.\n    Uses the embedded default libmode pipeline configuration.\n    \"\"\"\n    pipeline = None\n    try:\n        # Start pipeline in subprocess\n        # Note: stdout/stderr cannot be passed when run_in_subprocess=True (not picklable)\n        # Use quiet=False to see verbose startup logs\n        pipeline = run_pipeline(\n            block=False,\n            disable_dynamic_scaling=True,\n            run_in_subprocess=True,\n        )\n        time.sleep(10)\n        run_ingestor()\n        # Run other code...\n    except KeyboardInterrupt:\n        logger.info(\"Keyboard interrupt received. Shutting down...\")\n    except Exception as e:\n        logger.error(f\"Error running pipeline: {e}\")\n    finally:\n        if pipeline:\n            pipeline.stop()\n            logger.info(\"Shutting down pipeline...\")\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"extraction/quickstart-library-mode/#the-run_pipeline-function-reference","title":"The <code>run_pipeline</code> Function Reference","text":"<p>The <code>run_pipeline</code> function is the main entry point to start the Nemo Retriever Extraction pipeline.  It can run in-process or as a subprocess.</p> <p>The <code>run_pipeline</code> function accepts the following parameters.</p> Parameter Type Default Required? Description pipeline_config PipelineConfigSchema \u2014 Yes A configuration object that specifies how the pipeline should be constructed. run_in_subprocess bool False Yes <code>True</code> to launch the pipeline in a separate Python subprocess. <code>False</code> to run in the current process. block bool True Yes <code>True</code> to run the pipeline synchronously. The function returns after it finishes. <code>False</code> to return an interface for external pipeline control. disable_dynamic_scaling bool None No <code>True</code> to disable autoscaling regardless of global settings. <code>None</code> to use the global default behavior. dynamic_memory_threshold float None No A value between <code>0.0</code> and <code>1.0</code>. If dynamic scaling is enabled, triggers autoscaling when memory usage crosses this threshold. stdout TextIO None No Redirect the subprocess <code>stdout</code> to a file or stream. If <code>None</code>, defaults to <code>/dev/null</code>. stderr TextIO None No Redirect subprocess <code>stderr</code> to a file or stream. If <code>None</code>, defaults to <code>/dev/null</code>. libmode bool True No <code>True</code> to load the default library mode pipeline configuration when <code>ingest_config</code> is <code>None</code>. quiet bool None No <code>True</code> to suppress verbose startup logs (PRODUCTION preset). <code>None</code> defaults to <code>True</code> when <code>libmode=True</code>. Set to <code>False</code> for verbose output. <p>The <code>run_pipeline</code> function returns the following values, depending on the parameters that you set:</p> <ul> <li>run_in_subprocess=False and block=True  \u2014 The function returns a <code>float</code> that represents the elapsed time in seconds.</li> <li>run_in_subprocess=False and block=False \u2014 The function returns a <code>RayPipelineInterface</code> object.</li> <li>run_in_subprocess=True  and block=True  \u2014 The function returns <code>0.0</code>.</li> <li>run_in_subprocess=True  and block=False \u2014 The function returns a <code>RayPipelineInterface</code> object.</li> </ul> <p>The <code>run_pipeline</code> throws the following errors:</p> <ul> <li>RuntimeError \u2014 A subprocess failed to start, or exited with error.</li> <li>Exception \u2014 Any other failure during pipeline setup or execution.</li> </ul>"},{"location":"extraction/quickstart-library-mode/#related-topics","title":"Related Topics","text":"<ul> <li>Prerequisites</li> <li>Support Matrix</li> <li>Deploy With Docker Compose (Self-Hosted)</li> <li>Deploy With Helm</li> <li>Notebooks</li> <li>Multimodal PDF Data Extraction</li> </ul>"},{"location":"extraction/ray-logging/","title":"Configure Ray Logging","text":"<p>NeMo Retriever extraction uses Ray for logging.  You can use environment variables for fine-grained control over Ray's logging behavior.  In addition, NeMo Retriever extraction provides preset configurations that you can use to quickly update Ray logging behavior.</p> <p>Important</p> <p>You must set environment variables before you initialize the pipeline, and you must restart the pipeline if you change variable values.</p>"},{"location":"extraction/ray-logging/#quick-start-use-preset-configurations","title":"Quick Start - Use Preset Configurations","text":"<p>To get started quickly, use one of the NeMo Retriever extraction package-level preset variables.  Run the code below that corresponds to your use case; production, development, or debugging.  The log levels are explained following.</p> <p>Tip</p> <p>After you set a preset configuration, you can also override individual variables.</p> <pre><code># Production deployment - minimal logging, maximum performance\nexport INGEST_RAY_LOG_LEVEL=PRODUCTION\n\n# Development work (default) - balanced logging\nexport INGEST_RAY_LOG_LEVEL=DEVELOPMENT  \n\n# Debugging issues - maximum logging and visibility\nexport INGEST_RAY_LOG_LEVEL=DEBUG\n</code></pre>"},{"location":"extraction/ray-logging/#production-log-level","title":"PRODUCTION Log Level","text":"<p>The <code>PRODUCTION</code> log level is optimized for production deployments with minimal logging overhead. </p> <ul> <li>Storage Limit \u2013 10GB total (1GB \u00d7 10 files)</li> <li>Performance Impact \u2013 ~5% CPU reduction, ~200MB memory savings in large clusters</li> </ul> <p>This log level uses the following settings:</p> <ul> <li>Log Level \u2013 ERROR only</li> <li>Log to Driver \u2013 Disabled (worker logs stay in worker files)</li> <li>Import Warnings \u2013 Disabled</li> <li>Usage Stats \u2013 Disabled  </li> <li>Storage \u2013 10GB total (1GB \u00d7 10 files)</li> <li>Deduplication \u2013 Enabled</li> <li>Encoding \u2013 TEXT</li> </ul>"},{"location":"extraction/ray-logging/#development-log-level","title":"DEVELOPMENT Log Level","text":"<p>The <code>DEVELOPMENT</code> log level is a balanced configuration for development work,  and is the default log level.</p> <ul> <li>Storage Limit \u2013 20GB total (1GB \u00d7 20 files)  </li> <li>Performance Impact \u2013 Balanced performance and visibility</li> </ul> <p>This log level uses the following settings:</p> <ul> <li>Log Level \u2013 INFO</li> <li>Log to Driver \u2013 Enabled</li> <li>Import Warnings \u2013 Enabled</li> <li>Usage Stats \u2013 Enabled</li> <li>Storage \u2013 20GB total (1GB \u00d7 20 files) </li> <li>Deduplication \u2013 Enabled</li> <li>Encoding \u2013 TEXT</li> </ul>"},{"location":"extraction/ray-logging/#debug-log-level","title":"DEBUG Log Level","text":"<p>The <code>DEBUG</code> log level provides maximum visibility for troubleshooting issues. </p> <ul> <li>Storage Limit \u2013 20GB total (512MB \u00d7 40 files)</li> <li>Performance Impact \u2013 ~10% CPU overhead for detailed logging, higher memory usage</li> </ul> <p>This log level uses the following settings:</p> <ul> <li>Log Level \u2013 DEBUG</li> <li>Log to Driver \u2013 Enabled</li> <li>Import Warnings \u2013 Enabled</li> <li>Usage Stats \u2013 Enabled</li> <li>Storage \u2013 20GB total (512MB \u00d7 40 files)</li> <li>Deduplication \u2013 Disabled (see all duplicate messages)</li> <li>Encoding \u2013 JSON with function names and line numbers</li> </ul>"},{"location":"extraction/ray-logging/#configuration-reference","title":"Configuration Reference","text":"<p>The following are the environment variables that you can set to control Ray logging behavior.  If you specify an invalid value, the variable reverts to the default value with a warning message.</p> Variable Type Description Valid Values Default <code>INGEST_RAY_LOG_LEVEL</code> NeMo Retriever extraction preset Set multiple Ray logging variables to optimize for specific use cases. <code>PRODUCTION</code>, <code>DEVELOPMENT</code>, <code>DEBUG</code> <code>DEVELOPMENT</code> <code>RAY_DEDUP_LOGS</code> Log flow control Specify whether to log multiple instances of repeated events or to combine into a single entry. 1 to combine repeated messages (for example, <code>[repeated 5x]</code>). <code>0</code>, <code>1</code> <code>1</code> <code>RAY_DISABLE_IMPORT_WARNING</code> Ray internal logging <code>1</code> to suppresses <code>Ray X.Y.Z started</code> message and other warnings during initialization. <code>0</code>, <code>1</code> <code>0</code> <code>RAY_LOG_TO_DRIVER</code> Log flow control <code>true</code>to log worker messages in the main process. <code>false</code> to log worker messages in worker log files. <code>true</code>, <code>false</code> <code>true</code> <code>RAY_LOGGING_ADDITIONAL_ATTRS</code> Core logging control Add Python logger fields like function names, line numbers to each log entry. Comma-separated list (empty) <code>RAY_LOGGING_ENCODING</code> Core logging control Specify the format for log messages. <code>TEXT</code>, <code>JSON</code> <code>TEXT</code> <code>RAY_LOGGING_LEVEL</code> Core logging control Specify what events to log. <code>DEBUG</code> to log all Ray internals. <code>WARNING</code> to log only significant events. <code>DEBUG</code>, <code>INFO</code>, <code>WARNING</code>, <code>ERROR</code>, <code>CRITICAL</code> <code>INFO</code> <code>RAY_LOGGING_ROTATE_BACKUP_COUNT</code> File rotation Specify the number of old log files retained. Total storage = (count + 1) \u00d7 file size. Integer <code>19</code> <code>RAY_LOGGING_ROTATE_BYTES</code> File rotation Specify the log file size before Ray creates a new log file. Use this to prevent unbounded disk usage. Bytes <code>1073741824</code> (1GB) <code>RAY_USAGE_STATS_ENABLED</code> Ray internal logging <code>1</code> to enable telemetry collection and related log messages. <code>0</code> to disable. <code>0</code>, <code>1</code> <code>1</code>"},{"location":"extraction/ray-logging/#configuration-examples","title":"Configuration Examples","text":""},{"location":"extraction/ray-logging/#use-a-preset-with-a-manual-override","title":"Use a Preset With A Manual Override","text":"<p>The following example uses the <code>DEVELOPMENT</code> preset and then overrides the <code>RAY_LOGGING_LEVEL</code> behavior.</p> <pre><code>export INGEST_RAY_LOG_LEVEL=DEVELOPMENT  # Use the DEVELOPMENT preset\nexport RAY_LOGGING_LEVEL=WARNING         # Override just the log level\n</code></pre>"},{"location":"extraction/ray-logging/#log-verbosity-control","title":"Log Verbosity Control","text":"<p>By default, Ray generates significant logging output.  The following example configures Ray to reduce log volume.</p> <pre><code>export RAY_DISABLE_IMPORT_WARNING=1  # Suppress Ray initialization warnings\nexport RAY_LOGGING_LEVEL=WARNING     # Suppress informational messages, show only warnings and errors\nexport RAY_LOG_TO_DRIVER=false       # Prevent worker logs from appearing in driver process output\n</code></pre>"},{"location":"extraction/ray-logging/#minimal-logging-legacy","title":"Minimal Logging (Legacy)","text":"<p>The following example minimizes logging.  Only critical errors are logged.  Worker logs are isolated.  This reduces log volume by approximately 95%.</p> <p>Tip</p> <p>You can achieve the same effect by setting the <code>INGEST_RAY_LOG_LEVEL</code> to <code>PRODUCTION</code>.</p> <pre><code>export RAY_LOGGING_LEVEL=ERROR\nexport RAY_LOG_TO_DRIVER=false\nexport RAY_DISABLE_IMPORT_WARNING=1\nexport RAY_DEDUP_LOGS=1\n</code></pre>"},{"location":"extraction/ray-logging/#structured-logging-for-analysis","title":"Structured Logging for Analysis","text":"<p>The following example results in machine-parseable JSON with metadata for log aggregation systems.</p> <pre><code>export INGEST_RAY_LOG_LEVEL=DEVELOPMENT\nexport RAY_LOGGING_ENCODING=JSON\nexport RAY_LOGGING_ADDITIONAL_ATTRS=name,funcName,lineno,thread,process\n</code></pre>"},{"location":"extraction/ray-logging/#set-custom-storage-limits","title":"Set Custom Storage Limits","text":"<p>The following example automatically cleans up files when logs exceed 5GB. The oldest files are removed first.</p> <pre><code># 5GB total log storage (500MB \u00d7 10 files)\nexport RAY_LOGGING_ROTATE_BYTES=524288000\nexport RAY_LOGGING_ROTATE_BACKUP_COUNT=9\n</code></pre>"},{"location":"extraction/ray-logging/#log-output-examples","title":"Log Output Examples","text":""},{"location":"extraction/ray-logging/#info-level-default","title":"INFO level (Default)","text":"<pre><code>2024-01-15 10:30:15,123 INFO worker.py:1234 -- Task task_id=abc123 started\n2024-01-15 10:30:15,124 INFO worker.py:1235 -- Processing batch size=100\n2024-01-15 10:30:15,125 INFO worker.py:1236 -- Task task_id=abc123 completed\n</code></pre>"},{"location":"extraction/ray-logging/#warning-level","title":"WARNING level","text":"<pre><code>2024-01-15 10:30:20,456 WARNING worker.py:1240 -- Task retry attempt 2/3\n2024-01-15 10:30:25,789 ERROR worker.py:1245 -- Task failed: Connection timeout\n</code></pre>"},{"location":"extraction/ray-logging/#json-encoding-debug-preset","title":"JSON encoding (DEBUG preset)","text":"<pre><code>{\n    \"asctime\": \"2024-01-15 10:30:15,123\", \n    \"levelname\": \"INFO\", \n    \"filename\": \"worker.py\", \n    \"lineno\": 1234, \n    \"message\": \"Task started\", \n    \"name\": \"ray.worker\", \n    \"funcName\": \"execute_task\", \n    \"job_id\": \"01000000\", \n    \"worker_id\": \"abc123\", \n    \"task_id\": \"def456\"}\n</code></pre>"},{"location":"extraction/ray-logging/#related-topics","title":"Related Topics","text":"<ul> <li>Environment Variables</li> </ul>"},{"location":"extraction/releasenotes-nv-ingest/","title":"Release Notes for NeMo Retriever Extraction","text":"<p>This documentation contains the release notes for NeMo Retriever extraction.</p> <p>Note</p> <p>NeMo Retriever extraction is also known as NVIDIA Ingest and nv-ingest.</p>"},{"location":"extraction/releasenotes-nv-ingest/#release-2601-2612","title":"Release 26.01 (26.1.2)","text":"<p>The NeMo Retriever extraction 26.01 release adds new hardware and software support, and other improvements.</p> <p>To upgrade the Helm Charts for this version, refer to NV-Ingest Helm Charts.</p>"},{"location":"extraction/releasenotes-nv-ingest/#highlights","title":"Highlights","text":"<p>This release contains the following key changes:</p> <ul> <li>Added functional support for H200 NVL. For details, refer to Support Matrix.</li> <li>All Helm deployments for Kubernetes now use NVIDIA NIM Operator. For details, refer to NV-Ingest Helm Charts. </li> <li>Updated RIVA NIM to version 1.4.0. For details, refer to Extract Speech.</li> <li>Updated VLM NIM to nemotron-nano-12b-v2-vl. For details, refer to Extract Captions from Images.</li> <li>Added VLM caption prompt customization parameters, including reasoning control. For details, refer to Caption Images and Control Reasoning.</li> <li>Added support for the nemotron-parse model which replaces the nemoretriever-parse model. For details, refer to Advanced Visual Parsing.</li> <li>Support is now deprecated for paddleocr.</li> <li>The <code>meta-llama/Llama-3.2-1B</code> tokenizer is now pre-downloaded so that you can run token-based splitting without making a network request. For details, refer to Split Documents.</li> <li>For scanned PDFs, added specialized extraction strategies. For details, refer to PDF Extraction Strategies.</li> <li>Added support for LanceDB. For details, refer to Upload to a Custom Data Store.</li> <li>The V2 API is now available and is the default processing pipeline. The response format remains backwards-compatible. You can enable the v2 API by using <code>message_client_kwargs={\"api_version\": \"v2\"}</code>.For details, refer to API Reference.</li> <li>Large PDFs are now automatically split into chunks and processed in parallel, delivering faster ingestion for long documents. For details, refer to PDF Pre-Splitting.</li> <li>Issues maintaining extraction quality while processing very large files are now resolved with the V2 API. For details, refer to V2 API Guide.</li> <li>Updated the embedding task to support embedding on custom content fields like the results of summarization functions. For details, refer to Use the Python API.</li> <li>User-defined function summarization is now using <code>nemotron-mini-4b-instruct</code> which provides significant speed improvements. For details, refer to User-defined Functions and NV-Ingest UDF Examples.</li> <li>In the <code>Ingestor.extract</code> method, the defaults for <code>extract_text</code> and <code>extract_images</code> are now set to <code>true</code> for consistency with <code>extract_tables</code> and <code>extract_charts</code>. For details, refer to Use the Python API.</li> <li>The <code>table-structure</code> profile is no longer available. The table-structure profile is now part of the default profile. For details, refer to Profile Information.</li> <li>New documentation Why Throughput Is Dataset-Dependent.</li> <li>New documentation Add User-defined Stages.</li> <li>New documentation Add User-defined Functions.</li> <li>New documentation Resource Scaling Modes.</li> <li>New documentation NimClient Usage.</li> <li>New documentation Use the API (V2).</li> </ul>"},{"location":"extraction/releasenotes-nv-ingest/#fixed-known-issues","title":"Fixed Known Issues","text":"<p>The following are the known issues that are fixed in this version:</p> <ul> <li>A10G support is restored. To use A10G hardware, use release 26.1.2 or later. For details, refer to Support Matrix.</li> <li>L40S support is restored. To use L40S hardware, use release 26.1.2 or later. For details, refer to Support Matrix.</li> <li>The page number field in the content metadata now starts at 1 instead of 0 so each page number is no longer off by one from what you would expect. For details, refer to Content Metadata.</li> <li>Support for batches that include individual files greater than approximately 400MB is restored. This includes audio files and pdfs.</li> </ul>"},{"location":"extraction/releasenotes-nv-ingest/#all-known-issues","title":"All Known Issues","text":"<p>The following are the known issues for NeMo Retriever extraction:</p> <ul> <li> <p>Advanced visual parsing is not supported on RTX Pro 6000, B200, or H200 NVL. For details, refer to Advanced Visual Parsing and Support Matrix.</p> </li> <li> <p>The Page Elements NIM (<code>nemoretriever-page-elements-v3:1.7.0</code>) may intermittently fail during inference under high-concurrency workloads. This happens when Triton\u2019s dynamic batching combines requests that exceed the model\u2019s maximum batch size, a situation more commonly seen in multi-GPU setups or large ingestion runs. In these cases, extraction fails for the impacted documents. A correction is planned for <code>nemoretriever-page-elements-v3:1.7.1</code>.</p> </li> </ul>"},{"location":"extraction/releasenotes-nv-ingest/#release-notes-for-previous-versions","title":"Release Notes for Previous Versions","text":"<p>| 26.1.1 | 25.9.0  | 25.6.3  | 25.6.2  | 25.4.2  | 25.3.0  | 24.12.1  | 24.12.0  |</p>"},{"location":"extraction/releasenotes-nv-ingest/#related-topics","title":"Related Topics","text":"<ul> <li>Prerequisites</li> <li>Deploy Without Containers (Library Mode)</li> <li>Deploy With Docker Compose (Self-Hosted)</li> <li>Deploy With Helm</li> </ul>"},{"location":"extraction/scaling-modes/","title":"Resource Scaling Modes for NeMo Retriever Extraction","text":"<p>This guide covers how resource scaling modes work across stages in NeMo Retriever extraction, and how to configure it with docker-compose.</p> <ul> <li>Static scaling: Each pipeline stage runs a fixed number of replicas based on heuristics (memory-aware). Good for consistent latency; higher steady-state memory usage.</li> <li>Dynamic scaling: Only the source stage is fixed; other stages scale up/down based on observed resource pressure. Better memory efficiency; may briefly pause to spin replicas back up after idle periods.</li> </ul> <p>Note</p> <p>NeMo Retriever extraction is also known as NVIDIA Ingest and nv-ingest.</p>"},{"location":"extraction/scaling-modes/#when-to-choose-which","title":"When to choose which","text":"<ul> <li>Choose Static when latency consistency and warm pipelines matter more than memory minimization.</li> <li>Choose Dynamic when memory headroom is constrained or workloads are bursty/idle for long periods.</li> </ul>"},{"location":"extraction/scaling-modes/#configure-docker-compose","title":"Configure (docker-compose)","text":"<p>Edit <code>services &gt; nv-ingest-ms-runtime &gt; environment</code> in <code>docker-compose.yaml</code>.</p>"},{"location":"extraction/scaling-modes/#select-mode","title":"Select mode","text":"<ul> <li>Dynamic (default)</li> <li><code>INGEST_DISABLE_DYNAMIC_SCALING=false</code></li> <li> <p><code>INGEST_DYNAMIC_MEMORY_THRESHOLD=0.80</code> (fraction of memory; worker scaling reacts around this level)</p> </li> <li> <p>Static</p> </li> <li><code>INGEST_DISABLE_DYNAMIC_SCALING=true</code></li> <li>Optionally set a static memory threshold:<ul> <li><code>INGEST_STATIC_MEMORY_THRESHOLD=0.85</code> (fraction of total memory reserved for static replicas)</li> </ul> </li> </ul> <p>Example (Static):</p> <pre><code>services:\n  nv-ingest-ms-runtime:\n    environment:\n      - INGEST_DISABLE_DYNAMIC_SCALING=true\n      - INGEST_STATIC_MEMORY_THRESHOLD=0.85\n</code></pre> <p>Example (Dynamic):</p> <pre><code>services:\n  nv-ingest-ms-runtime:\n    environment:\n      - INGEST_DISABLE_DYNAMIC_SCALING=false\n      - INGEST_DYNAMIC_MEMORY_THRESHOLD=0.80\n</code></pre>"},{"location":"extraction/scaling-modes/#pipeline-config-mapping","title":"Pipeline config mapping","text":"<ul> <li><code>pipeline.disable_dynamic_scaling</code> \u21d0 <code>INGEST_DISABLE_DYNAMIC_SCALING</code></li> <li><code>pipeline.dynamic_memory_threshold</code> \u21d0 <code>INGEST_DYNAMIC_MEMORY_THRESHOLD</code></li> <li><code>pipeline.static_memory_threshold</code> \u21d0 <code>INGEST_STATIC_MEMORY_THRESHOLD</code></li> </ul>"},{"location":"extraction/scaling-modes/#trade-offs-recap","title":"Trade-offs recap","text":"<ul> <li>Dynamic</li> <li>Pros: Better memory efficiency; stages scale down when idle; can force scale-down under spikes.</li> <li> <p>Cons: After long idle, stages may scale to 0 replicas causing brief warm-up latency when work resumes.</p> </li> <li> <p>Static</p> </li> <li>Pros: Stable, predictable latency; stages remain hot.</li> <li>Cons: Higher baseline memory usage over time.</li> </ul>"},{"location":"extraction/scaling-modes/#sources-of-memory-utilization","title":"Sources of memory utilization","text":"<ul> <li>Workload size and concurrency</li> <li>More in\u2011flight jobs create more objects (pages, images, tables, charts) and large artifacts (for example, embeddings).</li> <li>Example: 1 MB text file \u2192 paragraphs with 20% overlap \u2192 4k\u2011dim embeddings base64\u2011encoded to JSON<ul> <li>Assumptions: ~600 bytes per paragraph. 20% overlap \u21d2 effective step \u2248 480 bytes. Chunks \u2248 1,000,000 / 480 \u2248 2,083.</li> <li>Per\u2011embedding size: 4,096 dims \u00d7 4 bytes (float32) = 16,384 bytes; base64 expansion \u00d7 4/3 \u2248 21,845 bytes (\u224821.3 KB).</li> <li>Total embeddings payload: \u2248 2,083 \u00d7 21.3 KB \u2248 45 MB, excluding JSON keys/metadata.</li> <li>Takeaway: a 1 MB source can yield \u227340\u00d7 memory just for embeddings, before adding extracted text, images, or other artifacts.</li> </ul> </li> <li>Example: PDF rendering and extracted images (A4 @ 72 DPI)<ul> <li>Rendering a page is a large in\u2011memory buffer; each extracted sub\u2011image adds more, and base64 inflates size.</li> <li>Page pixels \u2248 8.27\u00d772 by 11.69\u00d772 \u2248 595\u00d7842 \u2248 0.50 MP.</li> <li>RGB (3 bytes/pixel) \u2248 1.5 MB per page buffer; RGBA (4 bytes/pixel) \u2248 2.0 MB.</li> <li>Ten 1024\u00d71024 RGB crops \u2248 3.0 MB each in memory \u2192 base64 (+33%) \u2248 4.0 MB each \u21d2 ~40 MB just for crops (JSON not included).</li> <li>If you also base64 the full page image, expect another ~33% over the raw byte size (compression varies by format).</li> </ul> </li> <li>Library behavior</li> <li>Components like PyArrow may retain memory longer than expected (delayed free).</li> <li>Queues and payloads</li> <li>Base64\u2011encoded, fragmented documents in Redis consume memory proportional to concurrent jobs, clients, and drain speed.</li> </ul>"},{"location":"extraction/scaling-modes/#where-to-look-in-docker-compose","title":"Where to look in docker-compose","text":"<p>Open <code>docker-compose.yaml</code> and locate:</p> <ul> <li><code>services &gt; nv-ingest-ms-runtime &gt; environment</code>:</li> <li><code>INGEST_DISABLE_DYNAMIC_SCALING</code></li> <li><code>INGEST_DYNAMIC_MEMORY_THRESHOLD</code></li> <li><code>INGEST_STATIC_MEMORY_THRESHOLD</code></li> </ul>"},{"location":"extraction/scaling-modes/#related-topics","title":"Related Topics","text":"<ul> <li>Prerequisites</li> <li>Support Matrix</li> <li>Troubleshooting</li> </ul>"},{"location":"extraction/support-matrix/","title":"Support Matrix for NeMo Retriever Extraction","text":"<p>Before you begin using NeMo Retriever extraction, ensure that you have the hardware for your use case.</p> <p>Note</p> <p>NeMo Retriever extraction is also known as NVIDIA Ingest and nv-ingest.</p>"},{"location":"extraction/support-matrix/#core-and-advanced-pipeline-features","title":"Core and Advanced Pipeline Features","text":"<p>The Nemo Retriever extraction core pipeline features run on a single A10G or better GPU.  The core pipeline features include the following:</p> <ul> <li>llama3.2-nv-embedqa-1b-v2 \u2014 Embedding model for converting text chunks into vectors.</li> <li>nemoretriever-page-elements-v3 \u2014 Detects and classifies images on a page as a table, chart or infographic.</li> <li>nemoretriever-table-structure-v1 \u2014 Detects rows, columns, and cells within a table to preserve table structure and convert to Markdown format. </li> <li>nemoretriever-graphic-elements-v1 \u2014 Detects graphic elements within chart images such as titles, legends, axes, and numerical values. </li> <li>nemoretriever-ocr-v1 \u2014 Image OCR model to detect and extract text from images.</li> <li>retrieval \u2014 Enables embedding and indexing into Milvus.</li> </ul> <p>Advanced features require additional GPU support and disk space.  This includes the following:</p> <ul> <li>Audio extraction \u2014 Use Riva for processing audio files. For more information, refer to Audio Processing.</li> <li>Advanced visual parsing \u2014 Use nemotron-parse, which adds state-of-the-art text and table extraction. For more information, refer to Advanced Visual Parsing .</li> <li>VLM image captioning \u2014 Use nemotron-nano-12b-v2-vl for experimental image captioning of unstructured images. For more information, refer to Extract Captions from Images.</li> <li>Reranker \u2014 Use llama-3.2-nv-rerankqa-1b-v2 for improved retrieval accuracy.</li> </ul>"},{"location":"extraction/support-matrix/#hardware-requirements","title":"Hardware Requirements","text":"<p>NeMo Retriever extraction supports the following GPU hardware.</p> <ul> <li>RTX Pro 6000 Blackwell Server Edition</li> <li>DGX B200</li> <li>H200 NVL</li> <li>H100 Tensor Core GPU</li> <li>A100 Tensor Core GPU</li> <li>A10G Tensor Core GPU</li> <li>L40S</li> </ul> <p>The following are the hardware requirements to run NeMo Retriever extraction.</p> Feature GPU Option RTX Pro 6000 B200 H200 NVL H100 A100 80GB A100 40GB A10G L40S GPU Memory 96GB 180GB 141GB 80GB 80GB 40GB 24GB 48GB Core Features Total GPUs 1 1 1 1 1 1 1 1 Core Features Total Disk Space ~150GB ~150GB ~150GB ~150GB ~150GB ~150GB ~150GB ~150GB Audio Additional Dedicated GPUs 1 1 1 1 1 1 1 1 Audio Additional Disk Space ~37GB ~37GB ~37GB ~37GB ~37GB ~37GB ~37GB ~37GB nemotron-parse Additional Dedicated GPUs Not supported Not supported Not supported 1 1 1 1 1 nemotron-parse Additional Disk Space Not supported Not supported Not supported ~16GB ~16GB ~16GB ~16GB ~16GB VLM Additional Dedicated GPUs 1 1 1 1 1 Not supported Not supported 1 VLM Additional Disk Space ~16GB ~16GB ~16GB ~16GB ~16GB Not supported Not supported ~16GB Reranker With Core Pipeline Yes Yes Yes Yes Yes No* No* No* Reranker Standalone (recall only) Yes Yes Yes Yes Yes Yes Yes Yes <p>* GPUs with less than 80GB VRAM cannot run the reranker concurrently with the core pipeline.  To perform recall testing with the reranker on these GPUs, shut down the core pipeline NIM microservices  and run only the embedder, reranker, and your vector database.</p>"},{"location":"extraction/support-matrix/#related-topics","title":"Related Topics","text":"<ul> <li>Prerequisites</li> <li>Release Notes</li> <li>NVIDIA NIM for Vision Language Models Support Matrix</li> <li>NVIDIA NVIDIA Riva Support Matrix</li> </ul>"},{"location":"extraction/telemetry/","title":"Telemetry with NeMo Retriever Extraction","text":"<p>You can view telemetry data for NeMo Retriever extraction.</p> <p>Note</p> <p>NeMo Retriever extraction is also known as NVIDIA Ingest and nv-ingest.</p>"},{"location":"extraction/telemetry/#opentelemetry","title":"OpenTelemetry","text":"<p>After OpenTelemetry and Zipkin are running, you can open your browser to explore traces: </p> <ul> <li>Docker \u2014 Use http://$YOUR_DOCKER_HOST:9411/zipkin/ </li> <li>Kubernetes \u2014 Use http://$YOUR_K8S_OTEL_POD:9411/zipkin/</li> </ul> <p></p>"},{"location":"extraction/telemetry/#prometheus","title":"Prometheus","text":"<p>After Prometheus is running, you can open your browser to explore metrics: </p> <ul> <li>Docker \u2014 Use http://$YOUR_DOCKER_HOST:9090/zipkin/</li> <li>Kubernetes \u2014 Use http://$YOUR_K8S_OTEL_POD:9090/zipkin/</li> </ul> <p></p>"},{"location":"extraction/throughput-is-dataset-dependent/","title":"Why Throughput Is Dataset-Dependent","text":"<p>A single headline metric can drastically misrepresent system efficiency.  The amount of compute that you need to process a dataset depends far more on its content and how your pipeline operates than on its disk size.  This documentation explains why, and offers you better ways to measure and report throughput.</p> <p>Some common throughput measures, and their problems, include the following:</p> <ul> <li>TB/day, GB/hour, MB/s \u2013 Useful for capacity planning for storage and network, and the cost of data movement or archival. A weak proxy for compute due to compression and encoding differences.</li> <li>docs/min (documents per minute) \u2013 Easy to understand, but documents vary wildly in length and complexity.</li> <li>pages/sec (pages per second) \u2013 Usually correlates with work batching (sets-of-pages from PDFs). Varies with per-page complexity and modality mix.</li> <li>images/sec \u2013 Relevant when image transforms dominate. Sensitive to resolution.</li> <li>tokens/sec \u2013 Useful for LLM/VLM text-heavy stages. Ignores non-text work.</li> <li>elements/sec (tables/sec, charts/sec, OCR pages/sec) \u2013 Stage-specific and informative. Must be paired with prevalence (how many elements per page).</li> </ul>"},{"location":"extraction/throughput-is-dataset-dependent/#summary","title":"Summary","text":"<ul> <li>Disk size is not a reflection of expected processing time. Content complexity and enabled tasks dominate actual compute cost.</li> <li>Pages/sec is generally better than data-size-over-time metrics because it correlates more with work units, but it is still imperfect.</li> <li>Report throughput alongside dataset characteristics and stage-level metrics for meaningful, reproducible comparisons.</li> </ul>"},{"location":"extraction/throughput-is-dataset-dependent/#example-use-cases","title":"Example Use Cases","text":"<p>The following two datasets can yield the reverse ranking if you evaluate by data-size-over-time versus by pages/sec:</p> <ul> <li>Complex-but-small \u2013 A 1000-page PDF where each page contains dense tables and charts. The PDF may be small on disk (vector text, compressed graphics) yet very expensive to process (table detection, OCR, structure reconstruction, chart parsing).</li> <li>Large-but-simple \u2013 A 1000-page PDF with one large image per page. The file may be huge on disk (high-DPI scans) but comparatively fast to process if your pipeline mostly routes images without heavy analysis.</li> </ul>"},{"location":"extraction/throughput-is-dataset-dependent/#what-drives-processing-cost","title":"What Drives Processing Cost","text":"<p>The following factors drive processing cost. </p> <p>[!IMPORTANT] None of the following factors correlate with file size.</p> <ul> <li>Content modality and tasks enabled</li> <li>Text OCR vs. native text extraction</li> <li>Table structure detection and reconstruction</li> <li>Chart detection and text extraction</li> <li>Image captioning or vision-language models</li> <li> <p>Embedding generation and vector storage</p> </li> <li> <p>Content density and complexity per page</p> </li> <li>Number of elements (tables, figures, charts, text blocks)</li> <li>Layout complexity (nested tables, merged cells, multi-column text)</li> <li> <p>Languages, scripts, and fonts (OCR difficulty)</p> </li> <li> <p>Resolution and quality</p> </li> <li>DPI for scanned pages (I/O and pre-processing cost)</li> <li> <p>Compression artifacts vs. vector graphics</p> </li> <li> <p>Pipeline configuration</p> </li> <li>Which stages are turned on/off</li> <li>Model choices (accuracy vs. speed trade-offs)</li> <li> <p>Batch sizes, concurrency, hardware placement</p> </li> <li> <p>System factors</p> </li> <li>Warm-up vs. steady state</li> <li>I/O bandwidth and storage latency</li> <li>Network latency to inference services</li> </ul>"},{"location":"extraction/throughput-is-dataset-dependent/#why-data-size-over-time-is-misleading","title":"Why data-size-over-time Is Misleading","text":"<p>Use data-size-over-time metrics for storage and network planning, not for compute efficiency. </p> <p>The following are examples of why data-size-over-time metrics are misleading:</p> <ul> <li>Compression breaks the proxy</li> <li>Highly compressible vector PDFs may be tiny yet compute-heavy.</li> <li> <p>Scanned images may be huge but require minimal analysis.</p> </li> <li> <p>Format dependency</p> </li> <li> <p>Two datasets with identical content can have wildly different byte sizes due to encoding/format.</p> </li> <li> <p>Incentivizes the wrong optimizations</p> </li> <li> <p>Encourages selecting \u201cbig-byte\u201d but easy datasets to inflate data-size-over-time without improving true efficiency.</p> </li> <li> <p>Not portable across stages</p> </li> <li> <p>Bytes are not additive across pipeline stages (and often increase or decrease as formats change).</p> </li> <li> <p>Hard to reproduce</p> </li> <li>Data-size-over-time varies wildly with dataset encoding choices, not just system performance.</li> </ul>"},{"location":"extraction/throughput-is-dataset-dependent/#why-pagessec-is-better-but-imperfect","title":"Why Pages/sec Is Better (But Imperfect)","text":"<p>When you report pages/sec, you should also report dataset characterization. </p> <p>The following are some reasons why pages/sec is better than data-size-over-time metrics:</p> <ul> <li>Closer to the work unit</li> <li>Pipelines commonly schedule and process sets-of-pages from PDFs to saturate pipeline resources.</li> <li>Normalizes away compression and file format</li> <li>A page is a page regardless of on-disk bytes.</li> </ul> <p>However, pages/sec is still imperfect because of the following:</p> <ul> <li>Page complexity varies</li> <li>Pages with many tables/charts/figures or dense text cost more than blank or simple pages.</li> <li>Modality mix differs</li> <li>OCR-heavy pages vs. native text pages drive very different compute paths.</li> <li>Resolution matters</li> <li>High-DPI scans require more I/O and pre-processing.</li> </ul>"},{"location":"extraction/throughput-is-dataset-dependent/#example-interpreting-the-two-1000-page-pdfs","title":"Example: Interpreting the Two 1000-Page PDFs","text":"<p>The supposedly fast dataset by data-size-over-time can be the slow one by pages/sec, and vice versa.  Only context-rich reporting avoids this trap.</p> <p>The following are reasons why:</p> <ul> <li>Complex tables + charts per page (small file size)</li> <li>Data-size-over-time appears low due to tiny bytes, but compute is high \u2192 pages/sec and stage-level metrics reveal true cost.</li> <li>Expect lower pages/sec and lower tables/sec/charts/sec to dominate.</li> <li>Single large image per page (large file size)</li> <li>Data-size-over-time appears high due to big bytes, but compute can be low \u2192 fast pages/sec.</li> <li>If table/chart stages are skipped, stage-level numbers show negligible table/chart work.</li> </ul>"},{"location":"extraction/throughput-is-dataset-dependent/#practical-tips-for-fair-comparisons","title":"Practical Tips for Fair Comparisons","text":"<p>The following are practical tips for fair comparisons:</p> <ul> <li>Separate warm-up from steady-state measurements.</li> <li>Fix the pipeline configuration and model versions for a given comparison.</li> <li>Keep concurrency and resource limits identical across runs.</li> <li>Provide dataset characterization alongside throughput numbers.</li> </ul>"},{"location":"extraction/troubleshoot/","title":"Troubleshoot NeMo Retriever Extraction","text":"<p>Use this documentation to troubleshoot issues that arise when you use NeMo Retriever extraction.</p> <p>Note</p> <p>NeMo Retriever extraction is also known as NVIDIA Ingest and nv-ingest.</p>"},{"location":"extraction/troubleshoot/#cant-process-long-non-language-text-strings","title":"Can't process long, non-language text strings","text":"<p>NeMo Retriever extraction is designed to process language and language-length strings.  If you submit a document that contains extremely long, or non-language text strings,  such as a DNA sequence, errors or unexpected results occur.</p>"},{"location":"extraction/troubleshoot/#cant-process-malformed-input-files","title":"Can't process malformed input files","text":"<p>When you run a job you might see errors similar to the following:</p> <ul> <li>Failed to process the message</li> <li>Failed to extract image</li> <li>File may be malformed</li> <li>Failed to format paragraph</li> </ul> <p>These errors can occur when your input file is malformed.  Verify or fix the format of your input file, and try resubmitting your job.</p>"},{"location":"extraction/troubleshoot/#cant-start-new-thread-error","title":"Can't start new thread error","text":"<p>In rare cases, when you run a job you might an see an error similar to <code>can't start new thread</code>.  This error occurs when the maximum number of processes available to a single user is too low. To resolve the issue, set or raise the maximum number of processes (<code>-u</code>) by using the ulimit command. Before you change the <code>-u</code> setting, consider the following:</p> <ul> <li>Apply the <code>-u</code> setting directly to the user (or the Docker container environment) that runs your ingest service.</li> <li>For <code>-u</code> we recommend 10,000 as a baseline, but you might need to raise or lower it based on your actual usage and system configuration.</li> </ul> <pre><code>ulimit -u 10,000\n</code></pre>"},{"location":"extraction/troubleshoot/#out-of-memory-oom-error-when-processing-large-datasets","title":"Out-of-Memory (OOM) Error when Processing Large Datasets","text":"<p>When you process a very large dataset with thousands of documents, you might encounter an Out-of-Memory (OOM) error.  This happens because, by default, NeMo Retriever extraction stores the results from every document in system memory (RAM).  If the total size of the results exceeds the available memory, the process fails.</p> <p>To resolve this issue, use the <code>save_to_disk</code> method.  For details, refer to Working with Large Datasets: Saving to Disk.</p>"},{"location":"extraction/troubleshoot/#embedding-service-fails-to-start-with-an-unsupported-batch-size-error","title":"Embedding service fails to start with an unsupported batch size error","text":"<p>On certain hardware, for example RTX 6000,  the embedding service might fail to start and you might see an error similar to the following.</p> <pre><code>ValueError: Configured max_batch_size (30) is larger than the model''s supported max_batch_size (3).\n</code></pre> <p>If you are using hardware where the embedding NIM uses the ONNX model profile,  you must set <code>EMBEDDER_BATCH_SIZE=3</code> in your environment.  You can set the variable in your .env file or directly in your environment.</p>"},{"location":"extraction/troubleshoot/#extract-method-nemotron-parse-doesnt-support-image-files","title":"Extract method nemotron-parse doesn't support image files","text":"<p>Currently, extraction with Nemotron parse doesn't support image files, only scanned PDFs.  To work around this issue, convert image files to PDFs before you use <code>extract_method=\"nemotron_parse\"</code>.</p>"},{"location":"extraction/troubleshoot/#too-many-open-files-error","title":"Too many open files error","text":"<p>In rare cases, when you run a job you might an see an error similar to <code>too many open files</code> or <code>max open file descriptor</code>.  This error occurs when the open file descriptor limit for your service user account is too low. To resolve the issue, set or raise the maximum number of open file descriptors (<code>-n</code>) by using the ulimit command. Before you change the <code>-n</code> setting, consider the following:</p> <ul> <li>Apply the <code>-n</code> setting directly to the user (or the Docker container environment) that runs your ingest service.</li> <li>For <code>-n</code> we recommend 10,000 as a baseline, but you might need to raise or lower it based on your actual usage and system configuration.</li> </ul> <pre><code>ulimit -n 10,000\n</code></pre>"},{"location":"extraction/troubleshoot/#triton-server-info-messages-incorrectly-logged-as-errors","title":"Triton server INFO messages incorrectly logged as errors","text":"<p>Sometimes messages are incorrectly logged as errors, when they are information.  When this happens, you can ignore the errors, and treat the messages as information.  For example, you might see log messages that look similar to the following.</p> <pre><code>ERROR 2025-04-24 22:49:44.266 nimutils.py:68] tritonserver: /usr/local/lib/libcurl.so.4: ...\nERROR 2025-04-24 22:49:44.268 nimutils.py:68] I0424 22:49:44.265292 98 cache_manager.cc:480] \"Create CacheManager with cache_dir: '/opt/tritonserver/caches'\"\nERROR 2025-04-24 22:49:44.431 nimutils.py:68] I0424 22:49:44.431796 98 pinned_memory_manager.cc:277] \"Pinned memory pool is created at '0x7f8e4a000000' with size 268435456\"\nERROR 2025-04-24 22:49:44.432 nimutils.py:68] I0424 22:49:44.432036 98 cuda_memory_manager.cc:107] \"CUDA memory pool is created on device 0 with size 67108864\"\nERROR 2025-04-24 22:49:44.433 nimutils.py:68] I0424 22:49:44.433448 98 model_config_utils.cc:753] \"Server side auto-completed config: \"\nERROR 2025-04-24 22:49:44.433 nimutils.py:68] name: \"yolox\"\nERROR 2025-04-24 22:49:44.433 nimutils.py:68] platform: \"tensorrt_plan\"\nERROR 2025-04-24 22:49:44.433 nimutils.py:68] max_batch_size: 32\nERROR 2025-04-24 22:49:44.433 nimutils.py:68] input {\nERROR 2025-04-24 22:49:44.433 nimutils.py:68] name: \"input\"\nERROR 2025-04-24 22:49:44.433 nimutils.py:68] data_type: TYPE_FP32\nERROR 2025-04-24 22:49:44.433 nimutils.py:68] dims: 3\nERROR 2025-04-24 22:49:44.433 nimutils.py:68] dims: 1024\nERROR 2025-04-24 22:49:44.433 nimutils.py:68] dims: 1024\nERROR 2025-04-24 22:49:44.433 nimutils.py:68] }\nERROR 2025-04-24 22:49:44.433 nimutils.py:68] output {\nERROR 2025-04-24 22:49:44.434 nimutils.py:68] name: \"output\"\nERROR 2025-04-24 22:49:44.434 nimutils.py:68] data_type: TYPE_FP32\nERROR 2025-04-24 22:49:44.434 nimutils.py:68] dims: 21504\nERROR 2025-04-24 22:49:44.434 nimutils.py:68] dims: 9\nERROR 2025-04-24 22:49:44.434 nimutils.py:68] }\n...\n</code></pre>"},{"location":"extraction/troubleshoot/#related-topics","title":"Related Topics","text":"<ul> <li>Support Matrix</li> <li>Prerequisites</li> <li>Deploy Without Containers (Library Mode)</li> <li>Deploy With Docker Compose (Self-Hosted)</li> <li>Deploy With Helm</li> </ul>"},{"location":"extraction/user-defined-functions/","title":"Add User-Defined Functions to NeMo Retriever Extraction","text":"<p>User-Defined Functions (UDFs) allow you to inject custom processing logic into the NeMo Retriever extraction pipeline at specific stages.  This guide covers how to write, validate, and submit UDFs using both the CLI and the Python client interface.</p> <p>Note</p> <p>NeMo Retriever extraction is also known as NVIDIA Ingest and nv-ingest.</p>"},{"location":"extraction/user-defined-functions/#quickstart","title":"Quickstart","text":""},{"location":"extraction/user-defined-functions/#1-write-your-udf","title":"1. Write Your UDF","text":"<p>Create a Python function that accepts an <code>IngestControlMessage</code> and returns a modified <code>IngestControlMessage</code>:</p> <pre><code>from nv_ingest_api.internal.primitives.ingest_control_message import IngestControlMessage\n\ndef my_custom_processor(control_message: IngestControlMessage) -&gt; IngestControlMessage:\n    \"\"\"Add custom metadata to all documents.\"\"\"\n    # Get the DataFrame payload\n    df = control_message.payload()\n\n    # Add custom metadata to each row\n    for idx, row in df.iterrows():\n        if 'metadata' in row and isinstance(row['metadata'], dict):\n            # Add your custom field\n            df.at[idx, 'metadata']['custom_field'] = 'my_custom_value'\n\n    # Update the payload with modified DataFrame\n    control_message.payload(df)\n    return control_message\n</code></pre>"},{"location":"extraction/user-defined-functions/#2-submit-via-cli","title":"2. Submit via CLI","text":"<p>The CLI supports all UDF function specification formats. Here are examples of each:</p>"},{"location":"extraction/user-defined-functions/#inline-function-string","title":"Inline Function String","text":"<pre><code># Submit inline UDF function\nnv-ingest-cli \\\n    --doc /path/to/document.pdf \\\n    --output-directory ./output \\\n    --task 'udf:{\"udf_function\": \"def my_processor(control_message): print(\\\"Processing...\\\"); return control_message\", \"udf_function_name\": \"my_processor\", \"target_stage\": \"text_embedder\", \"run_before\": true}'\n</code></pre>"},{"location":"extraction/user-defined-functions/#module-path-with-colon-recommended","title":"Module Path with Colon (Recommended)","text":"<pre><code># Submit UDF from importable module (preserves all imports and context)\nnv-ingest-cli \\\n    --doc /path/to/document.pdf \\\n    --output-directory ./output \\\n    --task 'udf:{\"udf_function\": \"my_package.processors:enhance_metadata\", \"target_stage\": \"text_embedder\", \"run_after\": true}'\n</code></pre>"},{"location":"extraction/user-defined-functions/#file-path","title":"File Path","text":"<pre><code># Submit UDF from file path\nnv-ingest-cli \\\n    --doc /path/to/document.pdf \\\n    --output-directory ./output \\\n    --task 'udf:{\"udf_function\": \"my_file.py:my_custom_processor\", \"target_stage\": \"text_embedder\", \"run_before\": true}'\n</code></pre>"},{"location":"extraction/user-defined-functions/#legacy-import-path-limited","title":"Legacy Import Path (Limited)","text":"<pre><code># Submit UDF using legacy dot notation (function only, no imports)\nnv-ingest-cli \\\n    --doc /path/to/document.pdf \\\n    --output-directory ./output \\\n    --task 'udf:{\"udf_function\": \"my_package.processors.basic_processor\", \"target_stage\": \"text_embedder\", \"run_after\": true}'\n</code></pre>"},{"location":"extraction/user-defined-functions/#3-submit-via-python-client","title":"3. Submit via Python Client","text":"<pre><code>from nv_ingest_client.client.interface import Ingestor\n\n# Create an Ingestor instance with default client\ningestor = Ingestor()\n\n# Add documents and configure UDF to run before text embedding\nresults = ingestor.files(\"/path/to/document.pdf\") \\\n    .extract() \\\n    .udf(\n        udf_function=\"my_file.py:my_custom_processor\",\n        target_stage=\"text_embedder\",\n        run_before=True\n    ) \\\n    .embed() \\\n    .ingest()\n\n# Alternative: UDF to run after text embedding stage\nresults = ingestor.files(\"/path/to/document.pdf\") \\\n    .extract() \\\n    .embed() \\\n    .udf(\n        udf_function=\"my_file.py:my_custom_processor\", \n        target_stage=\"text_embedder\",\n        run_after=True\n    ) \\\n    .ingest()\n\n# Using phase-based targeting (legacy approach)\nresults = ingestor.files(\"/path/to/document.pdf\") \\\n    .extract() \\\n    .udf(\n        udf_function=\"my_file.py:my_custom_processor\",\n        phase=\"embed\"  # or phase=4\n    ) \\\n    .embed() \\\n    .ingest()\n</code></pre>"},{"location":"extraction/user-defined-functions/#comprehensive-documentation","title":"Comprehensive Documentation","text":""},{"location":"extraction/user-defined-functions/#understanding-ingestcontrolmessage-icm","title":"Understanding IngestControlMessage (ICM)","text":"<p>The <code>IngestControlMessage</code> is the primary data structure that flows through the NV-Ingest pipeline. Your UDF receives an ICM and must return a (potentially modified) ICM.</p>"},{"location":"extraction/user-defined-functions/#key-icm-methods","title":"Key ICM Methods","text":"<pre><code># Get the pandas DataFrame payload\ndf = control_message.payload()\n\n# Update the payload with a modified DataFrame\ncontrol_message.payload(modified_df)\n\n# Access metadata\nmetadata = control_message.get_metadata(\"some_key\")\ncontrol_message.set_metadata(\"some_key\", \"some_value\")\n\n# Get tasks (advanced usage)\ntasks = control_message.get_tasks()\n</code></pre>"},{"location":"extraction/user-defined-functions/#understanding-the-dataframe-payload","title":"Understanding the DataFrame Payload","text":"<p>The DataFrame payload contains the extracted content and metadata for processing. Each row represents a piece of content (text, image, table, etc.).</p>"},{"location":"extraction/user-defined-functions/#core-dataframe-columns","title":"Core DataFrame Columns","text":"Column Type Description <code>document_type</code> <code>str</code> Type of document (pdf, docx, txt, etc.) <code>source_type</code> <code>str</code> Source type identifier <code>source_file</code> <code>str</code> Path or identifier of the source file <code>id</code> <code>str</code> Unique identifier for this content piece <code>metadata</code> <code>dict</code> Rich metadata structure (see below) <code>content</code> <code>str</code> The actual extracted content"},{"location":"extraction/user-defined-functions/#example-dataframe-access","title":"Example DataFrame Access","text":"<pre><code>def process_text_content(control_message: IngestControlMessage) -&gt; IngestControlMessage:\n    df = control_message.payload()\n\n    for idx, row in df.iterrows():\n        # Access core fields\n        doc_type = row['document_type']\n        content = row['content']\n        metadata = row['metadata']\n\n        # Modify content or metadata\n        if doc_type == 'pdf' and len(content) &gt; 1000:\n            # Add a summary for long PDF content\n            df.at[idx, 'metadata']['content_summary'] = content[:200] + \"...\"\n\n    control_message.payload(df)\n    return control_message\n</code></pre>"},{"location":"extraction/user-defined-functions/#metadata-structure","title":"Metadata Structure","text":"<p>The <code>metadata</code> field in each DataFrame row contains a rich, nested structure with information about the content. The metadata follows a standardized schema with the following top-level keys:</p> <p>\u26a0\ufe0f Important Notes: - Full Modification Access: UDFs can modify any field defined by the MetadataSchema. However, corrupting or removing data expected by downstream stages may cause job failures. - Custom Content Field: The top-level <code>metadata</code> object includes an unvalidated <code>custom_content</code> dictionary where you can place any valid, JSON-serializable data for your custom use cases.</p>"},{"location":"extraction/user-defined-functions/#top-level-metadata-structure","title":"Top-Level Metadata Structure","text":"<pre><code>{\n    \"content\": \"The extracted text content\",\n    \"custom_content\": {},  # Your custom JSON-serializable data goes here\n    \"content_metadata\": {\n        \"type\": \"text|image|audio|structured\",\n        \"page_number\": 1,\n        \"description\": \"Content description\",\n        \"hierarchy\": {...},  # Content hierarchy information\n        \"subtype\": \"\",\n        # ... other content-specific fields\n    },\n    \"source_metadata\": {\n        \"source_id\": \"unique_source_identifier\",\n        \"source_name\": \"document.pdf\",\n        \"source_type\": \"pdf\",\n        \"source_location\": \"/path/to/document.pdf\",\n        \"collection_id\": \"\",\n        \"date_created\": \"2024-01-01T00:00:00\",\n        \"last_modified\": \"2024-01-01T00:00:00\",\n        \"summary\": \"\",\n        \"partition_id\": -1,\n        \"access_level\": 0\n    },\n    \"text_metadata\": {  # Present when content_metadata.type == \"text\"\n        \"text_type\": \"document\",\n        \"summary\": \"\",\n        \"keywords\": \"\",\n        \"language\": \"unknown\",\n        \"text_location\": [0, 0, 0, 0],\n        \"text_location_max_dimensions\": [0, 0, 0, 0]\n    },\n    \"image_metadata\": {  # Present when content_metadata.type == \"image\"\n        \"image_type\": \"png\",\n        \"structured_image_type\": \"none\",\n        \"caption\": \"\",\n        \"text\": \"\",\n        \"image_location\": [0, 0, 0, 0],\n        \"image_location_max_dimensions\": [0, 0],\n        \"uploaded_image_url\": \"\",\n        \"width\": 0,\n        \"height\": 0\n    },\n    \"audio_metadata\": {  # Present when content_metadata.type == \"audio\"\n        \"audio_type\": \"wav\",\n        \"audio_transcript\": \"\"\n    },\n    \"error_metadata\": null,  # Contains error information if processing failed\n    \"debug_metadata\": {}     # Arbitrary debug information\n}\n</code></pre>"},{"location":"extraction/user-defined-functions/#example-metadata-manipulation","title":"Example Metadata Manipulation","text":"<pre><code>def enhance_metadata(control_message: IngestControlMessage) -&gt; IngestControlMessage:\n    df = control_message.payload()\n\n    for idx, row in df.iterrows():\n        metadata = row['metadata']\n\n        # Add custom fields to source metadata\n        if 'source_metadata' in metadata:\n            metadata['source_metadata']['custom_processing_date'] = datetime.now().isoformat()\n            metadata['source_metadata']['custom_processor_version'] = \"1.0.0\"\n\n        # Add content-specific enhancements\n        if metadata.get('content_metadata', {}).get('type') == 'text':\n            content = metadata.get('content', '')\n            # Add word count\n            metadata['text_metadata']['word_count'] = len(content.split())\n\n        # Update the row\n        df.at[idx, 'metadata'] = metadata\n\n    control_message.payload(df)\n    return control_message\n</code></pre> <p>\ud83d\udcd6 For detailed metadata schema documentation, see: metadata_documentation.md</p>"},{"location":"extraction/user-defined-functions/#udf-targeting","title":"UDF Targeting","text":"<p>UDFs can be executed at different stages of the pipeline by specifying the <code>target_stage</code> parameter. The following stages are available in the default pipeline configuration:</p>"},{"location":"extraction/user-defined-functions/#available-pipeline-stages","title":"Available Pipeline Stages","text":"<p>Pre processing Stages (Phase 0): - <code>metadata_injector</code> - Metadata injection stage</p> <p>Extraction Stages (Phase 1): - <code>pdf_extractor</code> - PDF content extraction - <code>audio_extractor</code> - Audio content extraction - <code>docx_extractor</code> - DOCX document extraction - <code>pptx_extractor</code> - PowerPoint presentation extraction - <code>image_extractor</code> - Image content extraction - <code>html_extractor</code> - HTML document extraction - <code>infographic_extractor</code> - Infographic content extraction - <code>table_extractor</code> - Table structure extraction - <code>chart_extractor</code> - Chart and graphic extraction</p> <p>Post-processing Stages (Phase 2):</p> <p>Note: There are currently no Phase 2 stages in the default pipeline. This phase is reserved for future use and may include stages for content validation, quality assessment, or intermediate processing steps between extraction and mutation phases.</p> <p>Mutation Stages (Phase 3): - <code>image_filter</code> - Image filtering and validation - <code>image_dedup</code> - Image deduplication</p> <p>Transform Stages (Phase 4): - <code>text_splitter</code> - Text chunking and splitting - <code>image_caption</code> - Image captioning and description - <code>text_embedder</code> - Text embedding generation</p> <p>Storage Stages (Phase 5): - <code>image_storage</code> - Image storage and management - <code>embedding_storage</code> - Embedding storage and indexing - <code>broker_response</code> - Response message handling - <code>otel_tracer</code> - OpenTelemetry tracing</p> <p>Note: For the complete and up-to-date list of pipeline stages, see the default_pipeline.yaml configuration file.</p>"},{"location":"extraction/user-defined-functions/#target-stage-selection-examples","title":"Target Stage Selection Examples","text":"<pre><code># CLI examples for different target stages\nnv-ingest-cli --doc file.pdf --task 'udf:{\"udf_function\": \"processor.py:validate_input\", \"target_stage\": \"pdf_extractor\", \"run_before\": true}'\nnv-ingest-cli --doc file.pdf --task 'udf:{\"udf_function\": \"processor.py:extract_custom\", \"target_stage\": \"text_embedder\", \"run_after\": true}'\nnv-ingest-cli --doc file.pdf --task 'udf:{\"udf_function\": \"processor.py:enhance_output\", \"target_stage\": \"embedding_storage\", \"run_before\": true}'\n</code></pre> <pre><code># Python client examples\ningestor = Ingestor()\n\ningestor.udf(udf_function=\"processor.py:validate_input\", target_stage=\"pdf_extractor\", run_before=True\n    ).udf(udf_function=\"processor.py:extract_custom\", target_stage=\"text_embedder\", run_after=True\n    ).udf(udf_function=\"processor.py:enhance_output\", target_stage=\"embedding_storage\", run_before=True)\n</code></pre>"},{"location":"extraction/user-defined-functions/#udf-function-requirements","title":"UDF Function Requirements","text":""},{"location":"extraction/user-defined-functions/#signature-requirements","title":"Signature Requirements","text":"<p>Your UDF function must:</p> <ol> <li>Accept exactly one parameter named <code>control_message</code> with type annotation <code>IngestControlMessage</code></li> <li>Return an <code>IngestControlMessage</code></li> <li>Have proper type annotations</li> </ol> <pre><code># \u2705 Correct signature\ndef my_udf(control_message: IngestControlMessage) -&gt; IngestControlMessage:\n    # Your processing logic\n    return control_message\n\n# \u274c Incorrect - missing type annotations\ndef my_udf(control_message):\n    return control_message\n\n# \u274c Incorrect - wrong parameter name\ndef my_udf(message: IngestControlMessage) -&gt; IngestControlMessage:\n    return message\n\n# \u274c Incorrect - multiple parameters\ndef my_udf(control_message: IngestControlMessage, config: dict) -&gt; IngestControlMessage:\n    return control_message\n</code></pre>"},{"location":"extraction/user-defined-functions/#essential-patterns","title":"Essential Patterns","text":"<p>Always return the control message: <pre><code>def my_udf(control_message: IngestControlMessage) -&gt; IngestControlMessage:\n    # ... your processing ...\n    return control_message  # Don't forget this!\n</code></pre></p> <p>Update the payload after DataFrame modifications: <pre><code>def my_udf(control_message: IngestControlMessage) -&gt; IngestControlMessage:\n    df = control_message.payload()\n\n    # Modify the DataFrame\n    df['new_column'] = 'new_value'\n\n    # IMPORTANT: Update the payload\n    control_message.payload(df)\n\n    return control_message\n</code></pre></p>"},{"location":"extraction/user-defined-functions/#udf-function-specification-formats","title":"UDF Function Specification Formats","text":"<p>NV-Ingest supports four different formats for specifying UDF functions:</p>"},{"location":"extraction/user-defined-functions/#1-inline-function-string","title":"1. Inline Function String","text":"<p>Define your function directly as a string:</p> <pre><code>udf_function = \"\"\"\ndef my_custom_processor(control_message):\n    # Your processing logic here\n    payload = control_message.payload()\n    # Modify the payload as needed\n    return control_message\n\"\"\"\n</code></pre>"},{"location":"extraction/user-defined-functions/#2-module-path-with-colon-recommended","title":"2. Module Path with Colon (Recommended)","text":"<p>Reference a function from an importable module while preserving all imports and context:</p> <pre><code># Format: 'module.submodule:function_name'\nudf_function = \"my_package.processors.text_utils:enhance_metadata\"\n</code></pre> <p>Benefits: - \u2705 Preserves all module-level imports (<code>import pandas as pd</code>, etc.) - \u2705 Includes helper functions and variables the UDF depends on - \u2705 Maintains full execution context - \u2705 Best for complex UDFs with dependencies</p>"},{"location":"extraction/user-defined-functions/#3-file-path","title":"3. File Path","text":"<p>Reference a function from a specific Python file:</p> <pre><code># With function name: 'path/to/file.py:function_name'\nudf_function = \"/path/to/my_udfs.py:my_custom_processor\"\n\n# Without function name (assumes 'process' function)\nudf_function = \"/path/to/my_udfs.py\"\n</code></pre> <p>Benefits: - \u2705 Preserves all file-level imports and context - \u2705 Works with files not in Python path - \u2705 Good for standalone UDF files</p>"},{"location":"extraction/user-defined-functions/#4-legacy-import-path-limited","title":"4. Legacy Import Path (Limited)","text":"<p>Reference a function using dot notation (legacy format):</p> <pre><code># Format: 'module.submodule.function_name'\nudf_function = \"my_package.processors.text_utils.enhance_metadata\"\n</code></pre> <p>Limitations: - \u26a0\ufe0f Only extracts the function source code - \u26a0\ufe0f Does NOT include module imports or dependencies - \u26a0\ufe0f May fail if function depends on imports - \u26a0\ufe0f Use format #2 instead for better reliability</p>"},{"location":"extraction/user-defined-functions/#recommendation","title":"Recommendation","text":"<p>Use format #2 (Module Path with Colon) for most use cases as it provides the best balance of functionality and reliability by preserving the complete execution context your UDF needs.</p>"},{"location":"extraction/user-defined-functions/#udf-function-specification-examples","title":"UDF Function Specification Examples","text":"<pre><code># CLI usage\n--task 'udf:{\"udf_function\": \"path/to/my_processors.py:process_documents\"}'\n</code></pre> <pre><code># Python client usage\ningestor.udf(udf_function=\"my_package.processors.text_utils:enhance_metadata\")\n</code></pre>"},{"location":"extraction/user-defined-functions/#integrating-with-nvidia-nims","title":"Integrating with NVIDIA NIMs","text":"<p>NVIDIA Inference Microservices (NIMs) provide powerful AI capabilities that can be seamlessly integrated into your UDFs. The <code>NimClient</code> class offers a unified interface for connecting to and using NIMs within the NV-Ingest pipeline.</p>"},{"location":"extraction/user-defined-functions/#quick-nim-integration","title":"Quick NIM Integration","text":"<pre><code>from nv_ingest_api.internal.primitives.control_message import IngestControlMessage\nfrom nv_ingest_api.util.nim import create_inference_client\nfrom nv_ingest_api.internal.primitives.nim.model_interface.vlm import VLMModelInterface\nimport os\n\ndef document_analysis_with_nim(control_message: IngestControlMessage) -&gt; IngestControlMessage:\n    \"\"\"UDF that uses a NIM to analyze document content.\"\"\"\n\n    # Create NIM client for text analysis\n    model_interface = VLMModelInterface()\n    client = create_inference_client(\n        endpoints=(\n            os.getenv(\"ANALYSIS_NIM_GRPC\", \"grpc://analysis-nim:8001\"),\n            os.getenv(\"ANALYSIS_NIM_HTTP\", \"http://analysis-nim:8000\")\n        ),\n        model_interface=model_interface,\n        auth_token=os.getenv(\"NGC_API_KEY\"),\n        infer_protocol=\"http\"\n    )\n\n    df = control_message.payload()\n\n    for idx, row in df.iterrows():\n        if row.get(\"content\"):\n            try:\n                # Perform NIM inference\n                results = client.infer(\n                    data={\n                        \"base64_images\": [row.get(\"image_data\", \"\")],\n                        \"prompt\": f\"Analyze this document: {row['content'][:500]}\"\n                    },\n                    model_name=\"llava-1.5-7b-hf\"\n                )\n\n                # Add analysis to DataFrame\n                df.at[idx, \"nim_analysis\"] = results[0] if results else \"No analysis\"\n\n            except Exception as e:\n                print(f\"NIM inference failed: {e}\")\n                df.at[idx, \"nim_analysis\"] = \"Analysis failed\"\n\n    control_message.payload(df)\n    return control_message\n</code></pre>"},{"location":"extraction/user-defined-functions/#environment-configuration","title":"Environment Configuration","text":"<p>Set these environment variables for your NIM endpoints:</p> <pre><code># NIM service endpoints\nexport ANALYSIS_NIM_GRPC=\"grpc://your-nim-service:8001\"\nexport ANALYSIS_NIM_HTTP=\"http://your-nim-service:8000\"\n\n# Authentication (if required)\nexport NGC_API_KEY=\"your-ngc-api-key\"\n</code></pre>"},{"location":"extraction/user-defined-functions/#available-nim-interfaces","title":"Available NIM Interfaces","text":"<p>NV-Ingest provides several pre-built model interfaces:</p> <ul> <li>VLMModelInterface: Vision-Language Models for image analysis and captioning</li> <li>EmbeddingModelInterface: Text embedding generation</li> <li>OCRModelInterface: Optical Character Recognition</li> <li>YoloxModelInterface: Object detection and page element extraction</li> </ul>"},{"location":"extraction/user-defined-functions/#creating-custom-nim-integrations","title":"Creating Custom NIM Integrations","text":"<p>For detailed guidance on creating custom NIM integrations, including:</p> <ul> <li>Custom ModelInterface implementation</li> <li>Protocol handling (gRPC vs HTTP)</li> <li>Batch processing optimization</li> <li>Error handling and debugging</li> <li>Performance best practices</li> </ul> <p>See the comprehensive NimClient Usage Guide.</p>"},{"location":"extraction/user-defined-functions/#error-handling","title":"Error Handling","text":"<p>The NV-Ingest system automatically catches all exceptions that occur within UDF execution. If your UDF fails for any reason, the system will:</p> <ol> <li>Annotate the job with appropriate error information</li> <li>Mark the job as failed</li> <li>Return the failed job to you with error details</li> <li>Failures that are not caught by the system, or unhandled exceptions (segfaults) from acceleration libraries may leave the pipeline in an unstable state </li> </ol> <p>You do not need to implement extensive error handling within your UDF - focus on your core processing logic and let the system handle failures gracefully.</p>"},{"location":"extraction/user-defined-functions/#performance-considerations","title":"Performance Considerations","text":"<p>UDFs execute within the NV-Ingest pipeline and can significantly impact overall system performance and stability. Understanding these considerations is crucial for maintaining optimal pipeline throughput and reliability.</p>"},{"location":"extraction/user-defined-functions/#pipeline-impact","title":"Pipeline Impact","text":"<p>Global Slowdown on Congested Stages: - UDFs run synchronously within pipeline stages, blocking other processing until completion - Heavy-weight UDFs on high-traffic stages (e.g., <code>text_embedder</code>, <code>pdf_extractor</code>) can create bottlenecks - A single slow UDF can reduce throughput for the entire pipeline - Consider the stage's typical workload when designing UDF complexity</p> <p>Stage Selection Strategy: <pre><code># \u274c Avoid heavy processing on high-throughput stages\ningestor.udf(\n    udf_function=\"heavy_ml_processing.py:complex_analysis\",\n    target_stage=\"text_embedder\",  # High-traffic stage - will slow everything down\n    run_before=True\n)\n\n# \u2705 Better: Use less congested stages or run after processing\ningestor.udf(\n    udf_function=\"heavy_ml_processing.py:complex_analysis\", \n    target_stage=\"embedding_storage\",  # Lower-traffic stage\n    run_before=True\n)\n</code></pre></p>"},{"location":"extraction/user-defined-functions/#memory-management","title":"Memory Management","text":"<p>Memory Consumption: - UDFs share memory with the pipeline worker processes - Excessive memory usage can trigger out-of-memory (OOM) kills - Large DataFrame modifications can cause memory spikes - Memory leaks in UDFs accumulate over time and destabilize workers</p> <p>Best Practices: <pre><code># \u274c Memory-intensive operations\ndef memory_heavy_udf(control_message: IngestControlMessage) -&gt; IngestControlMessage:\n    df = control_message.payload()\n\n    # Creates large temporary objects\n    large_temp_data = [expensive_computation(row) for row in df.itertuples()]\n\n    # Multiple DataFrame copies\n    df_copy1 = df.copy()\n    df_copy2 = df.copy()\n    df_copy3 = df.copy()\n\n    return control_message\n\n# \u2705 Memory-efficient approach\ndef memory_efficient_udf(control_message: IngestControlMessage) -&gt; IngestControlMessage:\n    df = control_message.payload()\n\n    try:\n        # Load model once and reuse (consider caching)\n        model = get_cached_model()\n\n        # Batch processing with error handling\n        batch_results = []\n        for i in range(0, len(df), chunk_size):\n            chunk = df.iloc[i:i+chunk_size]\n            # Process chunk in-place\n            for idx in chunk.index:\n                df.at[idx, 'new_field'] = efficient_computation(df.at[idx, 'content'])\n\n        df['result'] = batch_results\n\n    except Exception as e:\n        logger.error(f\"UDF failed: {e}\")\n        # Return original message on failure\n        return control_message\n    finally:\n        # Explicit cleanup if needed\n        cleanup_resources()\n\n    control_message.payload(df)\n    return control_message\n</code></pre></p>"},{"location":"extraction/user-defined-functions/#computational-complexity","title":"Computational Complexity","text":"<p>CPU-Intensive Operations: - Complex algorithms can monopolize CPU resources - Long-running computations block the pipeline stage - Consider computational complexity relative to document size</p> <p>I/O Operations: - File system access, network requests, and database queries add latency - Synchronous I/O blocks the entire pipeline stage - External service dependencies introduce failure points</p> <pre><code># \u274c Blocking I/O in UDF\ndef blocking_io_udf(control_message: IngestControlMessage) -&gt; IngestControlMessage:\n    df = control_message.payload()\n\n    for idx, row in df.iterrows():\n        # Blocks pipeline for each external call\n        result = requests.get(f\"https://api.example.com/process/{row['id']}\")\n        df.at[idx, 'external_data'] = result.json()\n\n    control_message.payload(df)\n    return control_message\n\n# \u2705 Batch processing with timeouts\ndef efficient_io_udf(control_message: IngestControlMessage) -&gt; IngestControlMessage:\n    df = control_message.payload()\n\n    # Batch requests and set reasonable timeouts\n    ids = df['id'].tolist()\n    try:\n        response = requests.post(\n            \"https://api.example.com/batch_process\",\n            json={\"ids\": ids},\n            timeout=5.0  # Prevent hanging\n        )\n        results = response.json()\n\n        # Update DataFrame efficiently\n        df['external_data'] = df['id'].map(results.get)\n\n    except requests.RequestException as e:\n        logger.warning(f\"External API failed: {e}\")\n        df['external_data'] = None  # Graceful fallback\n\n    control_message.payload(df)\n    return control_message\n</code></pre>"},{"location":"extraction/user-defined-functions/#system-stability-risks","title":"System Stability Risks","text":"<p>Segmentation Faults: - Native library crashes (C extensions) can kill worker processes - Segfaults may leave the pipeline in an unstable state - Worker restarts cause job failures and processing delays</p> <p>Resource Exhaustion: - File descriptor leaks from unclosed resources - Thread pool exhaustion from concurrent operations</p> <p>Common Stability Issues: <pre><code># \u274c Potential stability risks\ndef risky_udf(control_message: IngestControlMessage) -&gt; IngestControlMessage:\n    \"\"\"UDF with potential stability risks.\"\"\"\n    logger = logging.getLogger(__name__)\n\n    try:\n        df = control_message.get_payload()\n        logger.info(f\"Processing {len(df)} documents\")\n\n        # Load model repeatedly (memory intensive)\n        model = load_large_ml_model()\n\n        # Native library calls without error handling\n        for idx, row in df.iterrows():\n            result = unsafe_native_function(row['content'])  # Could segfault\n            df.at[idx, 'result'] = result\n\n        logger.info(\"UDF processing completed successfully\")\n        control_message.payload(df)\n        return control_message\n\n    except Exception as e:\n        logger.error(f\"UDF failed: {e}\", exc_info=True)\n        # Return original message on failure\n        return control_message\n    finally:\n        # No explicit cleanup\n        pass\n\n# \u2705 Stable approach with resource management\ndef stable_udf(control_message: IngestControlMessage) -&gt; IngestControlMessage:\n    \"\"\"UDF with proper resource management.\"\"\"\n    logger = logging.getLogger(__name__)\n\n    try:\n        df = control_message.get_payload()\n        logger.info(f\"Processing {len(df)} documents\")\n\n        # Load model once and reuse (consider caching)\n        model = get_cached_model()\n\n        # Batch processing with error handling\n        batch_results = []\n        for i in range(0, len(df), chunk_size):\n            chunk = df.iloc[i:i+chunk_size]\n            # Process chunk in-place\n            for idx in chunk.index:\n                df.at[idx, 'new_field'] = efficient_computation(df.at[idx, 'content'])\n\n        df['result'] = batch_results\n\n        logger.info(\"UDF processing completed successfully\")\n        control_message.payload(df)\n        return control_message\n\n    except Exception as e:\n        logger.error(f\"UDF failed: {e}\", exc_info=True)\n        # Return original message on failure\n        return control_message\n    finally:\n        # Explicit cleanup if needed\n        cleanup_resources()\n</code></pre></p>"},{"location":"extraction/user-defined-functions/#performance-monitoring","title":"Performance Monitoring","text":"<p>Key Metrics to Monitor: - UDF execution time per document - Memory usage during UDF execution - Pipeline stage throughput before/after UDF deployment - Worker process restart frequency - Job failure rates</p> <p>Profiling UDFs: <pre><code>import time\nimport psutil\nimport logging\n\ndef profiled_udf(control_message: IngestControlMessage) -&gt; IngestControlMessage:\n    \"\"\"UDF with profiling.\"\"\"\n    logger = logging.getLogger(__name__)\n\n    start_time = time.time()\n    start_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB\n\n    df = control_message.payload()\n\n    # Your UDF logic here\n    # ... processing ...\n\n    end_time = time.time()\n    end_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB\n\n    execution_time = end_time - start_time\n    memory_delta = end_memory - start_memory\n\n    if execution_time &gt; 5.0:  # Log slow UDFs\n        logger.warning(f\"Slow UDF execution: {execution_time:.2f}s\")\n\n    if memory_delta &gt; 100:  # Log high memory usage\n        logger.warning(f\"High memory usage: {memory_delta:.2f}MB\")\n\n    control_message.payload(df)\n    return control_message\n</code></pre></p>"},{"location":"extraction/user-defined-functions/#recommendations","title":"Recommendations","text":"<p>Development Guidelines: 1. Profile Early: Test UDFs with realistic data volumes 2. Optimize for Stage: Consider the target stage's typical workload 3. Limit Complexity: Keep UDFs focused and lightweight 4. Handle Errors: Implement graceful fallbacks for external dependencies 5. Monitor Impact: Track pipeline performance after UDF deployment</p> <p>Production Deployment: 1. Gradual Rollout: Deploy UDFs to a subset of documents first 2. Resource Limits: Set appropriate memory and CPU limits 3. Monitoring: Implement alerting for performance degradation 4. Rollback Plan: Have a quick way to disable problematic UDFs</p> <p>When to Avoid UDFs: - For simple metadata additions that could be done post-processing - When external service dependencies are unreliable - For computationally expensive operations that could be batched offline - When the processing logic changes frequently</p> <p>Remember: UDFs are powerful but should be used judiciously. Poor UDF design can significantly impact the entire pipeline's performance and stability.</p>"},{"location":"extraction/user-defined-functions/#debugging-and-testing","title":"Debugging and Testing","text":""},{"location":"extraction/user-defined-functions/#global-udf-control","title":"Global UDF Control","text":"<p>You can globally disable all UDF processing using an environment variable:</p> <pre><code># Disable all UDF execution across the entire pipeline\nexport INGEST_DISABLE_UDF_PROCESSING=1\n</code></pre> <p>When to Use: Setting <code>INGEST_DISABLE_UDF_PROCESSING</code> to any non-empty value will disable all UDF processing across the entire pipeline. This is useful for:</p> <ul> <li>Debugging: Isolate pipeline issues by removing UDF interference</li> <li>Performance Testing: Measure baseline pipeline throughput without UDF overhead</li> <li>Emergency Situations: Quickly disable UDFs causing instability or crashes</li> <li>Maintenance: Temporary bypass during troubleshooting or system updates</li> <li>Rollback: Quick way to disable problematic UDFs in production</li> </ul> <p>Behavior: When disabled, all UDF tasks remain in control messages but are not executed. The pipeline runs normally without any UDF processing overhead, allowing you to verify that issues are UDF-related.</p> <pre><code># Examples of values that disable UDF processing\nexport INGEST_DISABLE_UDF_PROCESSING=1\nexport INGEST_DISABLE_UDF_PROCESSING=true\nexport INGEST_DISABLE_UDF_PROCESSING=disable\nexport INGEST_DISABLE_UDF_PROCESSING=any_non_empty_value\n\n# UDF processing is enabled (default behavior)\nunset INGEST_DISABLE_UDF_PROCESSING\n# OR\nexport INGEST_DISABLE_UDF_PROCESSING=\"\"\n</code></pre>"},{"location":"extraction/user-defined-functions/#testing-udfs","title":"Testing UDFs","text":"<p>When developing and testing UDFs, consider these approaches:</p>"},{"location":"extraction/user-defined-functions/#local-testing","title":"Local Testing","text":"<p>Test your UDF functions in isolation before deploying them to the pipeline:</p> <pre><code>import pandas as pd\nfrom nv_ingest_api.internal.primitives.ingest_control_message import IngestControlMessage\n\ndef test_my_udf():\n    # Create test data\n    test_df = pd.DataFrame({\n        'content': ['test document 1', 'test document 2'],\n        'metadata': [{'source': 'test1'}, {'source': 'test2'}]\n    })\n\n    # Create control message\n    control_message = IngestControlMessage()\n    control_message.payload(test_df)\n\n    # Test your UDF\n    result = my_custom_processor(control_message)\n\n    # Verify results\n    result_df = result.get_payload()\n    print(result_df)\n    assert 'custom_field' in result_df.iloc[0]['metadata']\n\n# Run the test\ntest_my_udf()\n</code></pre>"},{"location":"extraction/user-defined-functions/#pipeline-integration-testing","title":"Pipeline Integration Testing","text":"<p>Test UDFs in a controlled pipeline environment:</p> <ol> <li>Start with small datasets to verify basic functionality</li> <li>Use the disable flag to compare pipeline behavior with/without UDFs</li> <li>Monitor resource usage during UDF execution</li> <li>Test error scenarios to ensure graceful failure handling</li> </ol>"},{"location":"extraction/user-defined-functions/#common-debugging-techniques","title":"Common Debugging Techniques","text":"<pre><code>import logging\n\ndef debug_udf(control_message: IngestControlMessage) -&gt; IngestControlMessage:\n    \"\"\"UDF with comprehensive debugging.\"\"\"\n    logger = logging.getLogger(__name__)\n\n    try:\n        df = control_message.get_payload()\n        logger.info(f\"Processing {len(df)} documents\")\n\n        # Log input data structure\n        logger.debug(f\"DataFrame columns: {df.columns.tolist()}\")\n        logger.debug(f\"Sample row: {df.iloc[0].to_dict()}\")\n\n        # Your processing logic here\n        for idx, row in df.iterrows():\n            logger.debug(f\"Processing row {idx}: {row.get('content', '')[:50]}...\")\n            # ... your logic ...\n\n        logger.info(\"UDF processing completed successfully\")\n        control_message.payload(df)\n        return control_message\n\n    except Exception as e:\n        logger.error(f\"UDF failed: {e}\", exc_info=True)\n        # Return original message on failure\n        return control_message\n</code></pre>"},{"location":"extraction/user-defined-functions/#related-topics","title":"Related Topics","text":"<ul> <li>NV-Ingest UDF Examples</li> <li>User-Defined Stages for NeMo Retriever Extraction</li> <li>NimClient Usage</li> </ul>"},{"location":"extraction/user-defined-stages/","title":"Add User-defined Stages to Your NeMo Retriever Extraction Pipeline","text":"<p>This documentation demonstrates how to add user-defined stages to your NeMo Retriever extraction pipeline. You can directly import a function, or use a string module path, and include robust signature validation.  By following these steps,  your Lambda stages are robust, signature-validated, plug-and-play for your RayPipeline,  and operate on a well-defined DataFrame payload and metadata structure.</p> <p>Note</p> <p>NeMo Retriever extraction is also known as NVIDIA Ingest and nv-ingest.</p> <p>To add user-defined stages to your pipeline, you need the following:</p> <ul> <li> <p>A callable function \u2014 Your function must have the following exact signature. For more information, refer to .</p> <pre><code>def my_fn(control_message: IngestControlMessage, stage_config: MyConfig) -&gt; IngestControlMessage:\n</code></pre> </li> <li> <p>A DataFrame payload \u2014 The <code>control_message.payload</code> field must be a pandas.DataFrame. For more information, refer to Create a DataFrame Payload.</p> </li> <li> <p>Valid metadata \u2014 The <code>metadata</code> field must conform to the nv-ingest metadata schema. For more information, refer to Update and Validate Metadata.</p> </li> </ul>"},{"location":"extraction/user-defined-stages/#create-a-lambda-function-and-config","title":"Create a Lambda Function and Config","text":"<p>Your function must have the following exact signature. </p> <pre><code>def my_fn(control_message: IngestControlMessage, stage_config: MyConfig) -&gt; IngestControlMessage:\n...\n</code></pre> <ul> <li>The first parameter is named <code>control_message</code> and is an <code>IngestControlMessage</code>.</li> <li>The second parameter is named <code>stage_config</code> and must be a subclass of <code>pydantic.BaseModel</code>.</li> <li>The return value is an <code>IngestControlMessage</code>.</li> </ul> <p>The following example demonstrates how to create a valid Lambda function and configuration.</p> <pre><code>import pandas as pd\nfrom pydantic import BaseModel\nfrom nv_ingest_api.internal.primitives.ingest_control_message import IngestControlMessage\nfrom nv_ingest_api.internal.schemas.meta.metadata_schema import validate_metadata\n\n# Config schema for your stage\nclass MyToyConfig(BaseModel):\n  set_processed: bool = True\n\ndef toy_stage_fn(control_message: IngestControlMessage, stage_config: MyToyConfig) -&gt; IngestControlMessage:\n  df = control_message.payload()\n\n  # Set 'processed' flag in the allowed 'content_metadata' dict (if present)\n  def update_metadata(meta):\n    meta = dict(meta)\n\n    # Only update if 'content_metadata' exists and is a dict\n    if \"content_metadata\" in meta and isinstance(meta[\"content_metadata\"], dict):\n      meta[\"content_metadata\"] = dict(meta[\"content_metadata\"])  # ensure copy\n      meta[\"content_metadata\"][\"processed\"] = stage_config.set_processed\n    validate_metadata(meta)\n    return meta\n\n  df[\"metadata\"] = df[\"metadata\"].apply(update_metadata)\n  control_message.payload(df)\n  return control_message\n</code></pre>"},{"location":"extraction/user-defined-stages/#create-a-dataframe-payload","title":"Create a DataFrame Payload","text":"<p>The <code>control_message.payload</code> field must be a pandas.DataFrame with the following columns.</p> <ul> <li><code>document_type</code></li> <li><code>source_id</code></li> <li><code>job_id</code></li> <li><code>metadata</code></li> </ul> <p>The following example demonstrates an input payload (before the stage runs) as a DataFrame with valid metadata.</p> <pre><code>import pandas as pd\n\ndf = pd.DataFrame([\n  {\n    \"document_type\": \"invoice\",\n    \"source_id\": \"A123\",\n    \"job_id\": \"job-001\",\n    \"metadata\": {\n      \"content\": \"example\",\n      \"content_metadata\": {\"type\": \"pdf\"},\n      \"source_metadata\": {\"source_id\": \"A123\", \"source_type\": \"pdf\"},\n    },\n  },\n  {\n    \"document_type\": \"report\",\n    \"source_id\": \"B456\",\n    \"job_id\": \"job-002\",\n    \"metadata\": {\n      \"content\": \"another\",\n      \"content_metadata\": {\"type\": \"pdf\"},\n      \"source_metadata\": {\"source_id\": \"B456\", \"source_type\": \"pdf\"},\n    }\n  }\n])\n</code></pre>"},{"location":"extraction/user-defined-stages/#add-a-stage-function-is-imported","title":"Add a Stage (Function is Imported)","text":"<p>If your function is already imported in Python,  use the following code to add your user-defined stage to the pipeline.</p> <pre><code>config = MyToyConfig(flag_field=\"my_flag\")\n\npipeline.add_stage(\n  name=\"toy_stage\",\n  stage_actor=toy_stage_fn,\n  config=config,\n  min_replicas=1,\n  max_replicas=2,\n)\n</code></pre>"},{"location":"extraction/user-defined-stages/#add-a-stage-function-is-defined-in-a-module","title":"Add a Stage (Function is Defined in a Module)","text":"<p>If your function is defined in a module,  use the following code to add your user-defined stage to the pipeline.  In this example, the function is defined in <code>my_project.stages:toy_stage_fn</code>.</p> <pre><code>config = MyToyConfig(flag_field=\"has_been_processed\")\n\npipeline.add_stage(\n  name=\"toy_stage\",\n  stage_actor=\"my_project.stages:toy_stage_fn\",\n  config=config,\n  min_replicas=1,\n  max_replicas=2,\n)\n</code></pre> <p>When the pipeline runs it does the following:</p> <ul> <li>Import and validate the function (using <code>resolve_callable_from_path</code>).</li> <li>Automatically wrap it as a Ray stage.</li> <li>Enforce the signature and parameter naming rules.</li> </ul>"},{"location":"extraction/user-defined-stages/#update-and-validate-metadata","title":"Update and Validate Metadata","text":"<p>The <code>metadata</code> column in each row is a dictionary (JSON object),  and must conform to the nv-ingest metadata schema. </p> <p>After you change any metadata, you can validate it by using the <code>validate_metadata</code> function  as demonstrated in the following code example.</p> <pre><code>from nv_ingest_api.internal.schemas.meta.metadata_schema import validate_metadata\n\ndef edit_metadata(control_message: IngestControlMessage, stage_config: MyToyConfig) -&gt; IngestControlMessage:\n  df = control_message.payload()\n\n  def ensure_valid(meta):\n    meta = dict(meta)\n    # Only update an allowed nested metadata field\n    if \"content_metadata\" in meta and isinstance(meta[\"content_metadata\"], dict):\n      meta[\"content_metadata\"] = dict(meta[\"content_metadata\"])\n      meta[\"content_metadata\"][\"checked\"] = True\n    validate_metadata(meta)\n    return meta\n\n  df[\"metadata\"] = df[\"metadata\"].apply(ensure_valid)\n  control_message.payload(df)\n  return control_message\n</code></pre>"},{"location":"extraction/user-defined-stages/#troubleshoot-validation-failures","title":"Troubleshoot Validation Failures","text":"<p>The following are some examples of reasons that Lambda functions are invalid and fail validation.</p>"},{"location":"extraction/user-defined-stages/#wrong-parameter-names","title":"Wrong parameter names","text":"<p>The following Lambda function fails validation because the parameter names are incorrect.  You should see an error message similar to <code>TypeError: Expected parameter names: 'control_message', 'config'</code>.</p> <pre><code>#Incorrect example, do not use\ndef bad_fn(msg: IngestControlMessage, cfg: MyToyConfig) -&gt; IngestControlMessage:\n...\n</code></pre>"},{"location":"extraction/user-defined-stages/#missing-type-annotations","title":"Missing type annotations","text":"<p>The following Lambda function fails validation because the parameter and return types are missing. You should see an error message similar to <code>TypeError</code>.</p> <pre><code>#Incorrect example, do not use\ndef bad_fn(control_message, stage_config):\n...\n</code></pre>"},{"location":"extraction/user-defined-stages/#best-practices","title":"Best Practices","text":"<p>Use the following best practices to avoid validation failures.</p> <ul> <li>Always use explicit type annotations and the required parameter names (<code>control_message</code>, <code>stage_config</code>).</li> <li>Your config can be any subclass of <code>pydantic.BaseModel</code>.</li> <li>Any errors in signature validation are raised with a clear message during pipeline construction.</li> <li>You can use <code>validate_metadata(meta)</code> to assert compliance after metadata changes.</li> </ul>"},{"location":"extraction/user-defined-stages/#minimal-complete-example","title":"Minimal Complete Example","text":"<p>The  following example adds user-defined stages to your NeMo Retriever extraction pipeline. </p> <ol> <li> <p>The following code creates a function for a user-defined stage.</p> <pre><code># my_pipeline/stages.py\nfrom pydantic import BaseModel\nfrom nv_ingest_api.internal.primitives.ingest_control_message import IngestControlMessage\nfrom nv_ingest_api.internal.schemas.meta.metadata_schema import validate_metadata\n\nclass DoubleConfig(BaseModel):\nmultiply_by: int = 2\n\ndef double_amount(control_message: IngestControlMessage, stage_config: DoubleConfig) -&gt; IngestControlMessage:\ndf = control_message.payload()\n\n# Suppose the metadata for each row includes 'amount' under 'content_metadata'\ndef double_meta(meta):\n    meta = dict(meta)\n    if \"content_metadata\" in meta and isinstance(meta[\"content_metadata\"], dict):\n    cm = dict(meta[\"content_metadata\"])\n    if \"amount\" in cm and isinstance(cm[\"amount\"], (int, float)):\n        cm[\"amount\"] *= stage_config.multiply_by\n    meta[\"content_metadata\"] = cm\n    validate_metadata(meta)\n    return meta\n\ndf[\"metadata\"] = df[\"metadata\"].apply(double_meta)\ncontrol_message.payload(df)\nreturn control_message\n</code></pre> </li> <li> <p>The following code adds the user-defined stage to the pipeline.</p> <ul> <li> <p>(Option 1)  For a function that is defined in the module my_pipeline.stages.</p> <pre><code>from my_pipeline.stages import double_amount, DoubleConfig\n\npipeline.add_stage(\nname=\"doubler\",\nstage_actor=\"my_pipeline.stages:double_amount\",\nconfig=DoubleConfig(multiply_by=3),\nmin_replicas=1,\nmax_replicas=2,\n)\n</code></pre> </li> <li> <p>(Option 2) For a function that you have already imported.</p> <pre><code>pipeline.add_stage(\nname=\"doubler\",\nstage_actor=double_amount,\nconfig=DoubleConfig(multiply_by=3),\nmin_replicas=1,\nmax_replicas=2,\n)\n</code></pre> </li> </ul> </li> </ol>"},{"location":"extraction/user-defined-stages/#related-topics","title":"Related Topics","text":"<ul> <li>User-Defined Functions for NeMo Retriever Extraction</li> <li>NimClient Usage</li> </ul>"},{"location":"extraction/v2-api-guide/","title":"NeMo Retriever Extraction V2 API Guide: PDF Pre Splitting","text":"<p>TL;DR: V2 API automatically splits large PDFs into chunks for faster parallel processing.</p> <p>Python: Enable with <code>message_client_kwargs={\"api_version\": \"v2\"}</code> and configure chunk size with <code>.pdf_split_config(pages_per_chunk=64)</code>.</p> <p>CLI: Use <code>--api_version v2 --pdf_split_page_count 64</code></p>"},{"location":"extraction/v2-api-guide/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Quick Start - Get running in 5 minutes</li> <li>Configuration Guide - All configuration options</li> <li>How It Works - Architecture overview</li> <li>Migration from V1 - Upgrade existing code</li> </ol>"},{"location":"extraction/v2-api-guide/#quick-start","title":"Quick Start","text":""},{"location":"extraction/v2-api-guide/#what-is-v2-api","title":"What is V2 API?","text":"<p>The V2 API automatically splits large PDFs into smaller chunks before processing, enabling:</p> <ul> <li>Higher throughput - 1.3-1.5x faster for large documents</li> <li>Better parallelization - Distribute work across Ray workers</li> <li>Configurable chunk sizes - Tune to your infrastructure (1-128 pages)</li> </ul>"},{"location":"extraction/v2-api-guide/#minimal-example","title":"Minimal Example","text":"<pre><code>from nv_ingest_client.client import Ingestor\n\n# Two-step configuration\ningestor = Ingestor(\n    message_client_hostname=\"http://localhost\",\n    message_client_port=7670,\n    message_client_kwargs={\"api_version\": \"v2\"}  # \u2190 Step 1: Enable V2\n)\n\n# Run with optional chunk size override\nresults = ingestor.files([\"large_document.pdf\"]) \\\n    .extract(extract_text=True, extract_tables=True) \\\n    .pdf_split_config(pages_per_chunk=64) \\  # \u2190 Step 2: Configure splitting\n    .ingest()\n\nprint(f\"Processed {results['metadata']['total_pages']} pages\")\n</code></pre>"},{"location":"extraction/v2-api-guide/#cli-usage","title":"CLI Usage","text":"<pre><code>nv-ingest-cli \\\n  --api_version v2 \\\n  --pdf_split_page_count 64 \\\n  --doc large_document.pdf \\\n  --task 'extract:{\"document_type\":\"pdf\", \"extract_text\":true}' \\\n  --output_directory ./results\n</code></pre> <p>That's it! PDFs larger than 64 pages will be automatically split and processed in parallel.</p>"},{"location":"extraction/v2-api-guide/#configuration-guide","title":"Configuration Guide","text":""},{"location":"extraction/v2-api-guide/#two-required-settings","title":"Two Required Settings","text":"Setting Purpose How to Set API Version Route requests to V2 endpoints <code>message_client_kwargs={\"api_version\": \"v2\"}</code> Chunk Size Pages per chunk (optional) <code>.pdf_split_config(pages_per_chunk=N)</code>"},{"location":"extraction/v2-api-guide/#configuration-priority-chain","title":"Configuration Priority Chain","text":"<p>The chunk size is resolved in this order:</p> <pre><code>1. Client Override (HIGHEST)     \u2192 .pdf_split_config(pages_per_chunk=64)\n2. Server Environment Variable   \u2192 PDF_SPLIT_PAGE_COUNT=64 in .env\n3. Hardcoded Default (FALLBACK)  \u2192 32 pages\n</code></pre> <p>Client override always wins - useful for per-request tuning.</p>"},{"location":"extraction/v2-api-guide/#option-1-client-side-configuration-recommended","title":"Option 1: Client-Side Configuration (Recommended)","text":"<pre><code># Full control over chunk size per request\ningestor = Ingestor(\n    message_client_kwargs={\"api_version\": \"v2\"}\n).files(pdf_files) \\\n .extract(...) \\\n .pdf_split_config(pages_per_chunk=64)  # Client override\n</code></pre> <p>Pros: - \u2705 Different chunk sizes for different workloads - \u2705 No server config changes needed - \u2705 Clear intent in code</p> <p>Cons: - \u274c Must specify in every request</p>"},{"location":"extraction/v2-api-guide/#option-2-server-side-default","title":"Option 2: Server-Side Default","text":"<p>Set a cluster-wide default via Docker Compose <code>.env</code>:</p> <pre><code># .env file\nPDF_SPLIT_PAGE_COUNT=64\n</code></pre> <pre><code># docker-compose.yaml (already configured)\nservices:\n  nv-ingest-ms-runtime:\n    environment:\n      - PDF_SPLIT_PAGE_COUNT=${PDF_SPLIT_PAGE_COUNT:-32}\n</code></pre> <p>Pros: - \u2705 Set once, applies to all clients - \u2705 Different defaults per environment (dev/staging/prod) - \u2705 Clients don't need to specify</p> <p>Cons: - \u274c Requires server restart to change - \u274c Less flexible than client override</p>"},{"location":"extraction/v2-api-guide/#option-3-use-the-default","title":"Option 3: Use the Default","text":"<p>Simply enable V2 without configuring chunk size:</p> <pre><code># Uses default 32 pages per chunk\ningestor = Ingestor(\n    message_client_kwargs={\"api_version\": \"v2\"}\n).files(pdf_files).extract(...).ingest()\n</code></pre>"},{"location":"extraction/v2-api-guide/#configuration-matrix","title":"Configuration Matrix","text":"Client Config Server Env Var Effective Chunk Size Use Case <code>.pdf_split_config(64)</code> Not set 64 Client controls everything <code>.pdf_split_config(128)</code> <code>PDF_SPLIT_PAGE_COUNT=32</code> 128 Client override wins Not set <code>PDF_SPLIT_PAGE_COUNT=48</code> 48 Server default applies Not set Not set 32 Hardcoded fallback"},{"location":"extraction/v2-api-guide/#choosing-chunk-size","title":"Choosing Chunk Size","text":"<p>Note: We are developing an auto-tuning system that will automatically select chunk sizes based on document characteristics, available resources, and historical performance. This will eliminate manual tuning for most use cases.</p> <p>Smaller chunks (16-32 pages): - \u2705 Maximum parallelism - \u2705 Lower GPU memory per worker - \u274c More overhead from splitting/aggregation - Best for: Limited GPU memory, many available workers</p> <p>Medium chunks (32-64 pages): - \u2705 Balanced parallelism and overhead - \u2705 Good for most workloads - Best for: General use (recommended starting point)</p> <p>Larger chunks (64-128 pages): - \u2705 Minimal overhead - \u274c Less parallelism - Best for: Very large datasets, fewer workers</p> <p>Very large chunks (128+ pages): - \u274c Limited parallel benefits - Best for: Testing or when splitting overhead is problematic</p> <p>Valid range: 1-128 pages (server enforces with clamping)</p>"},{"location":"extraction/v2-api-guide/#how-it-works","title":"How It Works","text":""},{"location":"extraction/v2-api-guide/#architecture-flow","title":"Architecture Flow","text":"<pre><code>Client                    API Layer (V2)              Ray Workers\n  \u2502                            \u2502                           \u2502\n  \u2502   1. Submit PDF            \u2502                           \u2502\n  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba \u2502                           \u2502\n  \u2502   (200 pages)              \u2502                           \u2502\n  \u2502                            \u2502   2. Split into chunks    \u2502\n  \u2502                            \u2502   (64 pages each)         \u2502\n  \u2502                            \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                   \u2502\n  \u2502                            \u2502       \u2502 Chunk 1 (1-64)    \u2502\n  \u2502                            \u2502       \u2502 Chunk 2 (65-128)  \u2502\n  \u2502                            \u2502       \u2502 Chunk 3 (129-192) \u2502\n  \u2502                            \u2502       \u2514 Chunk 4 (193-200) \u2502\n  \u2502                            \u2502                           \u2502\n  \u2502                            \u2502   3. Process in parallel  \u2502\n  \u2502                            \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba\u2502\n  \u2502                            \u2502                           \u2502 Worker A \u2192 Chunk 1\n  \u2502                            \u2502                           \u2502 Worker B \u2192 Chunk 2\n  \u2502                            \u2502                           \u2502 Worker C \u2192 Chunk 3\n  \u2502                            \u2502                           \u2502 Worker D \u2192 Chunk 4\n  \u2502                            \u2502                           \u2502\n  \u2502   4. Fetch result          \u2502   5. Aggregate all chunks \u2502\n  \u2502 \u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n  \u2502   (all chunks combined)    \u2502   (ordered by page)       \u2502\n</code></pre>"},{"location":"extraction/v2-api-guide/#submission-phase","title":"Submission Phase","text":"<p>When you submit a PDF:</p> <ol> <li>Page Count Check: Server reads PDF metadata to get total pages</li> <li>Split Decision: If <code>page_count &gt; pages_per_chunk</code>, trigger splitting</li> <li>Chunk Creation: Use <code>pypdfium2</code> to split PDF into page ranges</li> <li>Subjob Generation: Create subjobs with deterministic UUIDs</li> <li>Redis Storage: Store parent\u2192subjob mapping with metadata</li> <li>Queue Submission: Submit all chunks to Ray task queue</li> </ol> <p>Example: <pre><code>Original: document.pdf (200 pages)\nConfig: pages_per_chunk=64\n\nChunks created:\n- document.pdf#pages_1-64    (64 pages)\n- document.pdf#pages_65-128  (64 pages)\n- document.pdf#pages_129-192 (64 pages)\n- document.pdf#pages_193-200 (8 pages)\n</code></pre></p>"},{"location":"extraction/v2-api-guide/#processing-phase","title":"Processing Phase","text":"<p>Each chunk is processed independently:</p> <ul> <li>Parallel execution across available Ray workers</li> <li>Full pipeline runs on each chunk (extraction, embedding, etc.)</li> <li>Per-chunk telemetry emitted to trace/annotations</li> <li>Results stored in Redis with subjob IDs</li> </ul>"},{"location":"extraction/v2-api-guide/#fetch-phase","title":"Fetch Phase","text":"<p>When you fetch results:</p> <ol> <li>Parent Check: API checks if job has subjobs</li> <li>State Verification: Checks all subjob states in parallel (batched)</li> <li>Wait if Needed: Returns 202 if any chunks still processing</li> <li>Fetch All Results: Fetches all subjob results in parallel (batched)</li> <li>Aggregate Data: Combines all chunk data in original page order</li> <li>Compute Metrics: Calculates parent-level trace aggregations</li> <li>Return Response: Single unified response with all chunks</li> </ol>"},{"location":"extraction/v2-api-guide/#response-structure","title":"Response Structure","text":"<p>Small PDFs (\u2264 chunk size): <pre><code>{\n  \"data\": [...],\n  \"trace\": {...},\n  \"annotations\": {...},\n  \"metadata\": {\n    \"total_pages\": 15\n  }\n}\n</code></pre></p> <p>Large PDFs (split into chunks): <pre><code>{\n  \"data\": [...],  // All chunks combined in page order\n  \"status\": \"success\",\n  \"trace\": {\n    // Parent-level aggregated metrics\n    \"trace::entry::pdf_extractor\": 1000,\n    \"trace::exit::pdf_extractor\": 5000,\n    \"trace::resident_time::pdf_extractor\": 800\n  },\n  \"annotations\": {...},  // Merged from all chunks\n  \"metadata\": {\n    \"parent_job_id\": \"abc-123\",\n    \"total_pages\": 200,\n    \"pages_per_chunk\": 64,\n    \"original_source_id\": \"document.pdf\",\n    \"subjobs_failed\": 0,\n    \"chunks\": [\n      {\n        \"job_id\": \"chunk-1-uuid\",\n        \"chunk_index\": 1,\n        \"start_page\": 1,\n        \"end_page\": 64,\n        \"page_count\": 64\n      }\n      // ... more chunks\n    ],\n    \"trace_segments\": [\n      // Per-chunk trace details (for debugging)\n    ]\n  }\n}\n</code></pre></p>"},{"location":"extraction/v2-api-guide/#trace-aggregation","title":"Trace Aggregation","text":"<p>Parent-level metrics computed from chunk traces:</p> <ul> <li><code>trace::entry::&lt;stage&gt;</code> - Earliest entry across all chunks (when first chunk started)</li> <li><code>trace::exit::&lt;stage&gt;</code> - Latest exit across all chunks (when last chunk finished)</li> <li><code>trace::resident_time::&lt;stage&gt;</code> - Sum of all chunk durations (total compute time)</li> </ul> <p>Example: <pre><code>Chunk 1: entry=1000, exit=1100 \u2192 duration=100ms\nChunk 2: entry=2000, exit=2150 \u2192 duration=150ms\nChunk 3: entry=2100, exit=2300 \u2192 duration=200ms\n\nParent metrics:\nentry = min(1000, 2000, 2100) = 1000  \u2190 First chunk started\nexit = max(1100, 2150, 2300) = 2300   \u2190 Last chunk finished\nresident_time = 100 + 150 + 200 = 450 \u2190 Total compute\n\nWall-clock time = exit - entry = 1300ms (parallelization benefit!)\nCompute time = resident_time = 450ms (actual work done)\n</code></pre></p>"},{"location":"extraction/v2-api-guide/#migration-from-v1","title":"Migration from V1","text":""},{"location":"extraction/v2-api-guide/#minimal-migration-v2-no-splitting","title":"Minimal Migration (V2, No Splitting)","text":"<p>Smallest possible change - just route to V2 endpoints:</p> <pre><code># Before (V1)\ningestor = Ingestor(\n    message_client_hostname=\"http://localhost\",\n    message_client_port=7670\n)\n\n# After (V2, identical behavior for PDFs \u226432 pages)\ningestor = Ingestor(\n    message_client_hostname=\"http://localhost\",\n    message_client_port=7670,\n    message_client_kwargs={\"api_version\": \"v2\"}  # Only change\n)\n</code></pre> <p>Behavior: No splitting occurs, responses identical to V1.</p>"},{"location":"extraction/v2-api-guide/#full-v2-with-splitting","title":"Full V2 with Splitting","text":"<p>Enable splitting for large PDFs:</p> <pre><code># V2 with PDF splitting\ningestor = Ingestor(\n    message_client_hostname=\"http://localhost\",\n    message_client_port=7670,\n    message_client_kwargs={\"api_version\": \"v2\"}\n).files(pdf_files) \\\n .extract(extract_text=True, extract_tables=True) \\\n .pdf_split_config(pages_per_chunk=64) \\\n .ingest()\n</code></pre>"},{"location":"extraction/v2-api-guide/#test-script-pattern","title":"Test Script Pattern","text":"<p>For test scripts like <code>tools/harness/src/nv_ingest_harness/cases/e2e.py</code>:</p> <pre><code>import os\nfrom nv_ingest_client.client import Ingestor\n\n# Read from environment\napi_version = os.getenv(\"API_VERSION\", \"v1\")\npdf_split_page_count = int(os.getenv(\"PDF_SPLIT_PAGE_COUNT\", \"32\"))\n\n# Build ingestor kwargs\ningestor_kwargs = {\n    \"message_client_hostname\": f\"http://{hostname}\",\n    \"message_client_port\": 7670\n}\n\n# Enable V2 if configured\nif api_version == \"v2\":\n    ingestor_kwargs[\"message_client_kwargs\"] = {\"api_version\": \"v2\"}\n\n# Create ingestor\ningestor = Ingestor(**ingestor_kwargs).files(data_dir)\n\n# Configure splitting for V2\nif api_version == \"v2\" and pdf_split_page_count:\n    ingestor = ingestor.pdf_split_config(pages_per_chunk=pdf_split_page_count)\n\n# Continue with pipeline\ningestor = ingestor.extract(...).ingest()\n</code></pre>"},{"location":"extraction/v2-api-guide/#backward-compatibility","title":"Backward Compatibility","text":"<p>V1 clients continue to work: - Still route to <code>/v1/submit_job</code> and <code>/v1/fetch_job</code> - No changes required - No splitting occurs</p> <p>V2 responses are V1-compatible: - Top-level <code>data</code>, <code>trace</code>, <code>annotations</code> have same structure - Additional metadata in <code>metadata</code> object (ignored by V1 parsers) - Existing response parsing code works unchanged</p> <p>HTTP status codes:</p> Code Meaning Action 200 All chunks complete Parse results 202 Still processing Poll again later 404 Job not found Check job ID 410 Result consumed Already fetched (destructive mode) 500 Server error Check logs 503 Processing failed Check failed_subjobs metadata"},{"location":"extraction/v2-api-guide/#silent-clamping-of-chunk-size","title":"Silent Clamping of Chunk Size","text":"<p>Symptom: Requested chunk size not used</p> <p>Cause: Server clamps to valid range (1-128)</p> <p>Check server logs for: <pre><code>WARNING: Client requested split_page_count=1000; clamped to 128\n</code></pre></p> <p>Solution: Use values within 1-128 range</p>"},{"location":"extraction/v2-api-guide/#response-fields","title":"Response Fields","text":"<p>All PDFs: - <code>data</code> - Array of extracted content - <code>trace</code> - Trace metrics - <code>annotations</code> - Task annotations - <code>metadata.total_pages</code> - Total page count</p> <p>Split PDFs only: - <code>metadata.parent_job_id</code> - Parent job UUID - <code>metadata.pages_per_chunk</code> - Configured chunk size - <code>metadata.chunks[]</code> - Chunk descriptors - <code>metadata.trace_segments[]</code> - Per-chunk traces - <code>metadata.failed_subjobs[]</code> - Failed chunk details</p>"},{"location":"extraction/v2-api-guide/#trace-metrics","title":"Trace Metrics","text":"<p>Parent-level (all jobs): - <code>trace::entry::&lt;stage&gt;</code> - Earliest start time - <code>trace::exit::&lt;stage&gt;</code> - Latest finish time - <code>trace::resident_time::&lt;stage&gt;</code> - Total compute time</p> <p>Chunk-level (split jobs only): - <code>metadata.trace_segments[].trace</code> - Per-chunk traces</p>"},{"location":"extraction/v2-api-guide/#key-files","title":"Key Files","text":"<p>Server Implementation: - <code>src/nv_ingest/api/v2/ingest.py</code> - V2 endpoints - <code>src/nv_ingest/framework/util/service/impl/ingest/redis_ingest_service.py</code> - Redis state management</p> <p>Client Implementation: - <code>client/src/nv_ingest_client/client/interface.py</code> - Ingestor class - <code>client/src/nv_ingest_client/util/util.py</code> - Configuration utilities - <code>client/src/nv_ingest_client/client/ingest_job_handler.py</code> - Job handling</p> <p>Schemas: - <code>api/src/nv_ingest_api/internal/schemas/meta/ingest_job_schema.py</code> - PdfConfigSchema</p>"},{"location":"extraction/v2-api-guide/#faq","title":"FAQ","text":"<p>Q: Do I need to specify chunk size every time? A: No. If you don't call <code>.pdf_split_config()</code>, the server uses either the <code>PDF_SPLIT_PAGE_COUNT</code> env var or the hardcoded default (32 pages).</p> <p>Q: When does splitting actually occur? A: Only when <code>page_count &gt; pages_per_chunk</code>. Smaller PDFs are processed as single jobs (no overhead).</p> <p>Q: Will my V1 response parsing code work with V2? A: Yes! Top-level <code>data</code>, <code>trace</code>, and <code>annotations</code> fields are identical. Additional metadata is added under <code>metadata</code> (which V1 parsers ignore).</p> <p>Q: How do I know if splitting occurred? A: Check <code>len(result[\"metadata\"].get(\"chunks\", [])) &gt; 0</code> or look for server logs: <code>\"Splitting PDF ... into ... chunks\"</code>.</p> <p>Q: What happens if one chunk fails? A: Other chunks still return results. Check <code>metadata.failed_subjobs[]</code> for details. The job returns <code>status: \"failed\"</code> but includes partial results.</p> <p>Q: Does V2 work without splitting? A: Yes! Just enable V2 without calling <code>.pdf_split_config()</code>. PDFs \u2264 default chunk size behave identically to V1.</p>"},{"location":"extraction/vlm-embed/","title":"Use Multimodal Embedding with NeMo Retriever Extraction","text":"<p>This documentation describes how to use NeMo Retriever extraction  with the multimodal embedding model Llama 3.2 NeMo Retriever Multimodal Embedding 1B.</p> <p>The <code>Llama 3.2 NeMo Retriever Multimodal Embedding 1B</code> model is optimized for multimodal question-answering retrieval.  The model can embed documents in the form of an image, text, or a combination of image and text.  Documents can then be retrieved given a user query in text form.  The model supports images that contain text, tables, charts, and infographics.</p> <p>Note</p> <p>NeMo Retriever extraction is also known as NVIDIA Ingest and nv-ingest.</p>"},{"location":"extraction/vlm-embed/#configure-and-run-the-multimodal-nim","title":"Configure and Run the Multimodal NIM","text":"<p>Use the following procedure to configure and run the multimodal embedding NIM locally.</p> <ol> <li> <p>Set the embedding model in your .env file. This tells NeMo Retriever extraction to use the Llama 3.2 Multimodal model instead of the default text-only embedding model.</p> <pre><code>EMBEDDING_IMAGE=nvcr.io/nvidia/nemo-microservices/llama-3.2-nemoretriever-1b-vlm-embed-v1\nEMBEDDING_TAG=1.7.0\nEMBEDDING_NIM_MODEL_NAME=nvidia/llama-3.2-nemoretriever-1b-vlm-embed-v1\n</code></pre> </li> <li> <p>Start the NeMo Retriever extraction services. The multimodal embedding service is included by default.</p> <pre><code>docker compose --profile retrieval up\n</code></pre> </li> </ol> <p>After the services are running, you can interact with the extraction pipeline by using Python. The key to leveraging the multimodal model is  to configure the <code>extract</code> and <code>embed</code> methods to process different content types as either text or images.</p>"},{"location":"extraction/vlm-embed/#example-with-default-text-based-embedding","title":"Example with Default Text-Based Embedding","text":"<p>When you use the multimodal model, by default, all extracted content (text, tables, charts) is treated as plain text.  The following example provides a strong baseline for retrieval.</p> <ul> <li>The <code>extract</code> method is configured to pull out text, tables, and charts.</li> <li>The <code>embed</code> method is called with no arguments.</li> </ul> <pre><code>ingestor = (\n    Ingestor()\n    .files(\"./data/*.pdf\")\n    .extract(\n        extract_text=True,\n        extract_tables=True,\n        extract_charts=True,\n        extract_images=False,\n    )\n    .embed() # Default behavior embeds all content as text\n)\nresults = ingestor.ingest()\n</code></pre>"},{"location":"extraction/vlm-embed/#example-with-embedding-structured-elements-as-images","title":"Example with Embedding Structured Elements as Images","text":"<p>It is common to process PDFs by embedding standard text as text, and embed visual elements like tables and charts as images.  The following example enables the multimodal model to capture the spatial and structural information of the visual content.</p> <ul> <li>The <code>extract</code> method is configured to pull out text, tables, and charts.</li> <li>The <code>embed</code> method is configured with <code>structured_elements_modality=\"image\"</code> to embed the extracted tables and charts as images.</li> </ul> <pre><code>ingestor = (\n    Ingestor()\n    .files(\"./data/*.pdf\")\n    .extract(\n        extract_text=True,\n        extract_tables=True,\n        extract_charts=True,\n        extract_images=False,\n    )\n    .embed(\n        structured_elements_modality=\"image\",\n    )\n)\nresults = ingestor.ingest()\n</code></pre>"},{"location":"extraction/vlm-embed/#example-with-embedding-entire-pdf-pages-as-images","title":"Example with Embedding Entire PDF Pages as Images","text":"<p>For documents where the entire page layout is important (such as infographics, complex diagrams, or forms),  you can configure NeMo Retriever extraction to treat every page as a single image. The following example extracts and embeds each page as an image.</p> <p>Note</p> <p>The <code>extract_page_as_image</code> feature is experimental. Its behavior may change in future releases.</p> <ul> <li>The <code>extract method</code> uses the <code>extract_page_as_image=True</code> parameter. All other extraction types are set to <code>False</code>.</li> <li>The <code>embed method</code> processes the page images.</li> </ul> <pre><code>ingestor = (\n    Ingestor()\n    .files(\"./data/*.pdf\")\n    .extract(\n        extract_text=False,\n        extract_tables=False,\n        extract_charts=False,\n        extract_images=False,\n        extract_page_as_image=True,\n    )\n    .embed(\n        image_elements_modality=\"image\",\n    )\n)\nresults = ingestor.ingest()\n</code></pre>"},{"location":"extraction/vlm-embed/#related-topics","title":"Related Topics","text":"<ul> <li>Support Matrix</li> <li>Troubleshoot Nemo Retriever Extraction</li> <li>Use the NV-Ingest Python API</li> </ul>"}]}