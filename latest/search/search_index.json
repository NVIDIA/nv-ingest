{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"What is NVIDIA NeMo Retriever","text":"<p>NVIDIA NeMo Retriever is a collection of microservices  for building and scaling multimodal data extraction, embedding, and reranking pipelines  with high accuracy and maximum data privacy \u2013 built with NVIDIA NIM.</p> <p>NeMo Retriever provides the following:</p> <ul> <li>Multimodal Data Extraction \u2014 Quickly extract documents at scale that include text, tables, charts, and infographics.</li> <li>Embedding + Indexing \u2014 Embed all extracted text from text chunks and images, and then insert into Milvus - accelerated with NVIDIA cuVS.</li> <li>Retrieval \u2014 Leverage semantic + hybrid search for high accuracy retrieval with the embedding + reranking NIM microservice.</li> </ul> <p></p>"},{"location":"#enterprise-ready-features","title":"Enterprise-Ready Features","text":"<p>NVIDIA NeMo Retriever comes with enterprise-ready features, including the following:</p> <ul> <li>High Accuracy \u2014 NeMo Retriever exhibits a high level of accuracy when retrieving across various modalities through enterprise documents. </li> <li>High Throughput \u2014 NeMo Retriever is capable of extracting, embedding, indexing and retrieving across hundreds of thousands of documents at scale with high throughput. </li> <li>Decomposable/Customizable \u2014 NeMo Retriever consists of modules that can be separately used and deployed in your own environment. </li> <li>Enterprise-Grade Security \u2014 NeMo Retriever NIMs come with security features such as the use of safetensors, continuous patching of CVEs, and more. </li> </ul>"},{"location":"#applications","title":"Applications","text":"<p>The following are some applications that use NVIDIA Nemo Retriever:</p> <ul> <li>Document Research Assistant for Blog Creation (LlamaIndex Jupyter Notebook)</li> <li>Digital Human for Customer Service (NVIDIA AI Blueprint)</li> <li>AI Virtual Assistant for Customer Service (NVIDIA AI Blueprint)</li> <li>Building Code Documentation Agents with CrewAI (CrewAI Demo)</li> <li>Video Search and Summarization (NVIDIA AI Blueprint)</li> </ul>"},{"location":"#related-topics","title":"Related Topics","text":"<ul> <li>NeMo Retriever Text Embedding NIM</li> <li>NeMo Retriever Text Reranking NIM</li> <li>NVIDIA NIM for Object Detection</li> <li>NVIDIA NIM for Image OCR</li> </ul>"},{"location":"extraction/audio/","title":"Use RIVA Audio Processing","text":""},{"location":"extraction/audio/#use-nemo-retriever-extraction-with-riva","title":"Use NeMo Retriever Extraction with Riva","text":"<p>This documentation describes two methods to run NeMo Retriever extraction  with the RIVA ASR NIM microservice for processing audio files.</p> <ul> <li>Run the NIM locally by using Docker Compose</li> <li>Use NVIDIA Cloud Functions (NVCF) endpoints for cloud-based inference</li> </ul> <p>Note</p> <p>NeMo Retriever extraction is also known as NVIDIA Ingest and nv-ingest.</p>"},{"location":"extraction/audio/#overview","title":"Overview","text":"<p>NeMo Retriever extraction now supports the processing and retrieval of audio files for Retrieval Augmented Generation (RAG) applications.  Similar to how the multimodal document extraction pipeline leverages object detection and image OCR microservices,  NeMo Retriever leverages the RIVA ASR NIM microservice  to transcribe audio files to text, which is then embedded by using the NeMo Retriever embedding NIM. </p> <p>Important</p> <p>Due to limitations in available VRAM controls in the current release of audio NIMs, it must run on a dedicated additional GPU. For the full list of requirements to run RIVA NIM, refer to Support Matrix.</p> <p>This Early Access pipeline enables users to now retrieve audio files at the segment level. </p> <p></p>"},{"location":"extraction/audio/#run-the-nim-locally-by-using-docker-compose","title":"Run the NIM Locally by Using Docker Compose","text":"<p>Use the following procedure to run the NIM locally.</p> <p>Important</p> <p>RIVA NIM must run on a dedicated additional GPU. Edit docker-compose.yaml to set the audio service's device_id to a dedicated GPU: device_ids: [\"1\"] or higher.</p> <ol> <li> <p>To access the required container images, log in to the NVIDIA Container Registry (nvcr.io). Use your NGC key as the password. Run the following command in your terminal.</p> <ul> <li>Replace <code>&lt;your-ngc-key&gt;</code> with your actual NGC API key.</li> <li>The username is always <code>$oauthtoken</code>.</li> </ul> <pre><code>$ docker login nvcr.io\nUsername: $oauthtoken\nPassword: &lt;your-ngc-key&gt;\n</code></pre> </li> <li> <p>Store your NGC key in an environment variable file.</p> </li> </ol> <p>For convenience and security, store your NGC key in a .env file. This enables services to access it without needing to enter the key manually each time.</p> <p>Create a .env file in your working directory and add the following line: <pre><code>NGC_API_KEY=&lt;your-ngc-key&gt;\n</code></pre> Again, replace  with your actual NGC key. <ol> <li> <p>Start the nv-ingest services with the <code>audio</code> profile. This profile includes the necessary components for audio processing. Use the following command.</p> <ul> <li>The <code>--profile audio</code> flag ensures that audio-specific services are launched. For more information, refer to Profile Information.</li> <li>The <code>--build</code> flag ensures that any changes to the container images are applied before starting.</li> </ul> <pre><code>docker compose --profile retrieval --profile audio up --build\n</code></pre> </li> <li> <p>After the services are running, you can interact with nv-ingest by using Python.</p> <ul> <li>The <code>Ingestor</code> object initializes the ingestion process.</li> <li>The <code>files</code> method specifies the input files to process.</li> <li>The <code>extract</code> method tells nv-ingest to extract information from WAV audio files.</li> <li>The <code>document_type</code> parameter is optional, because <code>Ingestor</code> should detect the file type automatically.</li> </ul> <pre><code>ingestor = (\n    Ingestor()\n    .files(\"./data/*.wav\")\n    .extract(\n        document_type=\"wav\",  # Ingestor should detect type automatically in most cases\n        extract_method=\"audio\",\n    )\n)\n</code></pre> <p>Tip</p> <p>For more Python examples, refer to NV-Ingest: Python Client Quick Start Guide.</p> </li> </ol>"},{"location":"extraction/audio/#use-nvcf-endpoints-for-cloud-based-inference","title":"Use NVCF Endpoints for Cloud-Based Inference","text":"<p>Instead of running NV-Ingest locally, you can use NVCF to perform inference by using remote endpoints.</p> <ol> <li> <p>NVCF requires an authentication token and a function ID for access. Ensure you have these credentials ready before making API calls.</p> </li> <li> <p>Run inference by using Python. Provide an NVCF endpoint along with authentication details.</p> <ul> <li>The <code>Ingestor</code> object initializes the ingestion process.</li> <li>The <code>files</code> method specifies the input files to process.</li> <li>The <code>extract</code> method tells nv-ingest to extract information from WAV audio files.</li> <li>The <code>document_type</code> parameter is optional, because <code>Ingestor</code> should detect the file type automatically.</li> </ul> <pre><code>ingestor = (\n    Ingestor()\n    .files(\"./data/*.mp3\")\n    .extract(\n        document_type=\"mp3\",\n        extract_method=\"audio\",\n        extract_audio_params={\n            \"grpc_endpoint\": \"grpc.nvcf.nvidia.com:443\",\n            \"auth_token\": \"&lt;API key&gt;\",\n            \"function_id\": \"&lt;function ID&gt;\",\n            \"use_ssl\": True,\n        },\n    )\n)\n</code></pre> <p>Tip</p> <p>For more Python examples, refer to NV-Ingest: Python Client Quick Start Guide.</p> </li> </ol>"},{"location":"extraction/chunking/","title":"Split Documents","text":"<p>Splitting, also known as chunking, breaks large documents or text into smaller, manageable sections to improve retrieval efficiency.  After chunking, only the most relevant pieces of information are retrieved for a given query.  Chunking also prevents text from exceeding the context window of the embedding model.</p> <p>There are two ways that NV Ingest chunks text:</p> <ul> <li>By using the <code>text_depth</code> parameter in the <code>extraction</code> task.</li> <li>Token-based splitting by using the <code>split</code> task.</li> </ul>"},{"location":"extraction/chunking/#extraction-text-depth","title":"Extraction Text Depth","text":"<p>You can use the <code>text_depth</code> parameter to specify how extracted text is chunked together by the extractor.  For example, the following code chunks the document text by page, for document types that have pages.</p> <pre><code>ingestor = ingestor.extract(\n    extract_text=True,\n    text_depth=\"page\"\n)\n</code></pre> <p>The following table contains the <code>text_depth</code> parameter values.</p> Value Description <code>document</code> Doesn't perform any splitting, and returns the full document as one chunk. <code>page</code> Returns a single chunk of text for each page. <p>For most documents, we recommend that you set <code>text_depth</code> to <code>page</code>, because this tends to give the best performance for retrieval.  However, in some cases, such as with <code>.txt</code> documents, there aren't any page breaks to split on. </p> <p>If you want chunks smaller than <code>page</code>, use token-based splitting as described in the following section.</p>"},{"location":"extraction/chunking/#token-based-splitting","title":"Token-Based Splitting","text":"<p>The <code>split</code> task uses a tokenizer to count the number of tokens in the document,  and splits the document based on the desired maximum chunk size and chunk overlap.  We recommend that you use the <code>meta-llama/Llama-3.2-1B</code> tokenizer,  because it's the same tokenizer as the llama-3.2 embedding model that we use for embedding. However, you can use any tokenizer from any HuggingFace model that includes a tokenizer file.</p> <p>Use the <code>split</code> method to chunk large documents as shown in the following code.</p> <p>Note</p> <p>The default tokenizer (<code>meta-llama/Llama-3.2-1B</code>) requires a Hugging Face access token. You must set <code>hf_access_token\": \"hf_***</code> to authenticate.</p> <pre><code>ingestor = ingestor.split(\n    tokenizer=\"meta-llama/Llama-3.2-1B\",\n    chunk_size=1024,\n    chunk_overlap=150,\n    params={\"split_source_types\": [\"text\", \"PDF\"], \"hf_access_token\": \"hf_***\"}\n)\n</code></pre> <p>To use a different tokenizer, such as <code>intfloat/e5-large-unsupervised</code>, you can modify the <code>split</code> call as shown following.</p> <pre><code>ingestor = ingestor.split(\n    tokenizer=\"intfloat/e5-large-unsupervised\",\n    chunk_size=1024,\n    chunk_overlap=150\n)\n</code></pre>"},{"location":"extraction/chunking/#split-parameters","title":"Split Parameters","text":"<p>The following table contains the <code>split</code> parameters.</p> Parameter Description Default <code>tokenizer</code> HuggingFace Tokenizer identifier or path. <code>meta-llama/Llama-3.2-1B</code> <code>chunk_size</code> Maximum number of tokens per chunk. <code>1024</code> <code>chunk_size</code> Number of tokens to overlap between chunks. <code>150</code> <code>params</code> A sub-dictionary that can contain <code>split_source_types</code> and <code>hf_access_token</code> <code>{}</code> <code>hf_access_token</code> Your Hugging Face access token. \u2014 <code>split_source_types</code> The source types to split on (only splits on text by default). \u2014"},{"location":"extraction/chunking/#pre-download-the-tokenizer","title":"Pre-download the Tokenizer","text":"<p>When the NV Ingest container is built, it pre-downloads a default tokenizer, so that it doesn't have to download the tokenizer at runtime. </p> <p>By default, the NV Ingest container downloads the <code>intfloat/e5-large-unsupervised</code> tokenizer, which is not gated, and does not require any special permissions.</p> <p>You can use the <code>meta-llama/Llama-3.2-1B</code> tokenizer instead,  but this is a gated model, and requires special permissions. To pre-download the <code>meta-llama/Llama-3.2-1B</code> tokenizer, you must do the following:</p> <ul> <li>Review the license agreement.</li> <li>Request access.</li> <li>Set the <code>HF_ACCESS_TOKEN</code> environment variable to your HuggingFace access token.</li> <li>Set the <code>DOWNLOAD_LLAMA_TOKENIZER</code> environment variable to <code>true</code>.</li> </ul>"},{"location":"extraction/content-metadata/","title":"Source and Content Metadata Reference for NV-Ingest","text":"<p>This documentation contains the reference for the content metadata.  The definitions used in this documentation are the following:</p> <ul> <li>Source \u2014 The file that is ingested, and from which content and metadata is extracted.</li> <li>Content \u2014 Data extracted from a source, such as text or an image.</li> </ul> <p>Metadata can be extracted from a source or content, or generated by using models, heuristics, or other methods.</p>"},{"location":"extraction/content-metadata/#source-file-metadata","title":"Source File Metadata","text":"<p>The following is the metadata for source files.</p> Field Description Method Source Name The name of the source file. Extracted Source ID The ID of the source file. Extracted Source location The URL, URI, or pointer to the storage location of the source file. \u2014 Source Type The type of the source file, such as pdf, docx, pptx, or txt. Extracted Collection ID The ID of the collection in which the source is contained. \u2014 Date Created The date the source was created. Extracted Last Modified The date the source was last modified. Extracted Partition ID The offset of this data fragment within a larger set of fragments. Generated Access Level The role-based access control for the source. \u2014 Summary A summary of the source. (Not yet implemented.) Generated"},{"location":"extraction/content-metadata/#content-metadata","title":"Content Metadata","text":"<p>The following is the metadata for content.  These fields apply to all content types including text, images, and tables.</p> Field Description Method Type The type of the content. Text, Image, Structured, Table, or Chart. Generated Subtype The type of the content for structured data types, such as table or chart. \u2014 Content Content extracted from the source. Extracted Description A text description of the content object. Generated Page # The page # of the content in the source. Extracted Hierarchy The location or order of the content within the source. Extracted"},{"location":"extraction/content-metadata/#text-metadata","title":"Text Metadata","text":"<p>The following is the metadata for text.</p> Field Description Method Text Type The type of the text, such as header or body. Extracted Keywords Keywords, Named Entities, or other phrases. Extracted Language The language of the content. Generated Summary An abbreviated summary of the content. (Not yet implemented.) Generated"},{"location":"extraction/content-metadata/#image-metadata","title":"Image Metadata","text":"<p>The following is the metadata for images.</p> Field Description Method Image Type The type of the image, such as structured, natural, hybrid, and others. Generated (Classifier) Structured Image Type The type of the content for structured data types, such as bar chart, pie chart, and others. Generated (Classifier) Caption Any caption or subheading associated with Image Extracted Text Extracted text from a structured chart Extracted Image location Location (x,y) of chart within an image Extracted Image location max dimensions Max dimensions (x_max,y_max) of location (x,y) Extracted uploaded_image_uri Mirrors source_metadata.source_location \u2014"},{"location":"extraction/content-metadata/#table-metadata","title":"Table Metadata","text":"<p>The following is the metadata for tables within documents.</p> <p>Warning</p> <p>Tables should not be chunked</p> Field Description Method Table format Structured (dataframe / lists of rows and columns), or serialized as markdown, html, latex, simple (cells separated as spaces). Extracted Table content Extracted text content, formatted according to table_metadata.table_format. Extracted Table location The bounding box of the table. Extracted Table location max dimensions The max dimensions (x_max,y_max) of the bounding box of the table. Extracted Caption The caption for the table or chart. Extracted Title The title of the table. Extracted Subtitle The subtitle of the table. Extracted Axis Axis information for the table. Extracted uploaded_image_uri A mirror of source_metadata.source_location. Generated"},{"location":"extraction/content-metadata/#example-metadata","title":"Example Metadata","text":"<p>The following is an example JSON representation of metadata.  This is an example only, and does not contain the full metadata. For the full file, refer to the data folder.</p> <pre><code>{\n    \"document_type\": \"text\",\n    \"metadata\": \n    {\n        \"content\": \"TestingDocument...\",\n        \"content_url\": \"\",\n        \"source_metadata\": \n        {\n            \"source_name\": \"data/multimodal_test.pdf\",\n            \"source_id\": \"data/multimodal_test.pdf\",\n            \"source_location\": \"\",\n            \"source_type\": \"PDF\",\n            \"collection_id\": \"\",\n            \"date_created\": \"2025-03-13T18:37:14.715892\",\n            \"last_modified\": \"2025-03-13T18:37:14.715534\",\n            \"summary\": \"\",\n            \"partition_id\": -1,\n            \"access_level\": 1\n        },\n        \"content_metadata\": \n        {\n            \"type\": \"structured\",\n            \"description\": \"Structured chart extracted from PDF document.\",\n            \"page_number\": 1,\n            \"hierarchy\": \n            {\n                \"page_count\": 3,\n                \"page\": 1,\n                \"block\": -1,\n                \"line\": -1,\n                \"span\": -1,\n                \"nearby_objects\": \n                {\n                    \"text\": \n                    {\n                        \"content\": [],\n                        \"bbox\": [],\n                        \"type\": []\n                    },\n                    \"images\": \n                    {\n                        \"content\": [],\n                        \"bbox\": [],\n                        \"type\": []\n                    },\n                    \"structured\": \n                    {\n                        \"content\": [],\n                        \"bbox\": [],\n                        \"type\": []\n                    }\n                }\n            },\n            \"subtype\": \"chart\"\n        },\n        \"audio_metadata\": null,\n        \"text_metadata\": null,\n        \"image_metadata\": null,\n        \"table_metadata\": \n        {\n            \"caption\": \"\",\n            \"table_format\": \"image\",\n            \"table_content\": \"Below,is a high-quality picture of some shapes          Picture\",\n            \"table_content_format\": \"\",\n            \"table_location\": \n            [\n                74,\n                614,\n                728,\n                920\n            ],\n            \"table_location_max_dimensions\": \n            [\n                792,\n                1024\n            ],\n            \"uploaded_image_uri\": \"\"\n        },\n        \"chart_metadata\": null,\n        \"error_metadata\": null,\n        \"info_message_metadata\": null,\n        \"debug_metadata\": null,\n        \"raise_on_failure\": false\n    }\n}\n</code></pre>"},{"location":"extraction/contributing/","title":"Contributing to NV-Ingest","text":"<p>External contributions to NV-Ingest will be welcome soon, and they are greatly appreciated!  For more information, refer to Contributing to NV-Ingest.</p>"},{"location":"extraction/custom-metadata/","title":"Use Custom Metadata to Filter Search Results","text":"<p>You can upload custom metadata for documents during ingestion.  By uploading custom metadata you can attach additional information to documents,  and use it for filtering results during retrieval operations.  For example, you can add author metadata to your documents, and filter by author when you retrieve results.  To create filters, you use Milvus Filtering Expressions.</p> <p>Use this documentation to use custom metadata to filter search results when you work with NeMo Retriever extraction.</p>"},{"location":"extraction/custom-metadata/#limitations","title":"Limitations","text":"<p>The following are limitation when you use custom metadata:</p> <ul> <li>Metadata fields must be consistent across documents in the same collection.</li> <li>Complex filter expressions may impact retrieval performance.</li> <li>If you update your custom metadata, you must ingest your documents again to use the new metadata.</li> </ul>"},{"location":"extraction/custom-metadata/#add-custom-metadata-during-ingestion","title":"Add Custom Metadata During Ingestion","text":"<p>You can add custom metadata during the document ingestion process.  You can specify metadata for each file,  and you can specify different metadata for different documents in the same ingestion batch.</p>"},{"location":"extraction/custom-metadata/#metadata-structure","title":"Metadata Structure","text":"<p>You specify custom metadata as a dataframe or a file (json, csv, or parquet). </p> <p>The following example contains metadata fields for category, department, and timestamp.  You can create whatever metadata is helpful for your scenario.</p> <pre><code>import pandas as pd\n\nmeta_df = pd.DataFrame\n(\n    {\n        \"source\": [\"data/woods_frost.pdf\", \"data/multimodal_test.pdf\"],\n        \"category\": [\"Alpha\", \"Bravo\"],\n        \"department\": [\"Language\", \"Engineering\"],\n        \"timestamp\": [\"2025-05-01T00:00:00\", \"2025-05-02T00:00:00\"]\n    }\n)\n\n# Convert the dataframe to a csv file, \n# to demonstrate how to ingest a metadata file in a later step.\n\nfile_path = \"./meta_file.csv\"\nmeta_df.to_csv(file_path)\n</code></pre>"},{"location":"extraction/custom-metadata/#example-add-custom-metadata-during-ingestion","title":"Example: Add Custom Metadata During Ingestion","text":"<p>The following example adds custom metadata during ingestion.  For more information about the <code>Ingestor</code> class, see Use the NV-Ingest Python API. For more information about the <code>vdb_upload</code> method, see Upload Data.</p> <pre><code>from nv_ingest_client.client import Ingestor\n\nhostname=\"localhost\"\ncollection_name = \"nv_ingest_collection\"\nsparse = True\n\ningestor = ( \n    Ingestor(message_client_hostname=hostname)\n        .files([\"data/woods_frost.pdf\", \"data/multimodal_test.pdf\"])\n            .extract(\n                extract_text=True,\n                extract_tables=True,\n                extract_charts=True,\n                extract_images=True,\n                text_depth=\"page\"\n            )\n                .embed(text=True, tables=True)\n                    .vdb_upload(\n                        collection_name=collection_name, \n                        milvus_uri=f\"http://{hostname}:19530\", \n                        sparse=sparse, \n                        minio_endpoint=f\"{hostname}:9000\", \n                        dense_dim=2048,\n                        meta_dataframe=file_path, \n                        meta_source_field=\"source\", \n                        meta_fields=[\"category, department, timestamp\"]\n                    )\n)\nresults = ingestor.ingest_async().result()\n</code></pre>"},{"location":"extraction/custom-metadata/#best-practices","title":"Best Practices","text":"<p>The following are the best practices when you work with custom metadata:</p> <ul> <li>Plan metadata structure before ingestion.</li> <li>Test filter expressions with small datasets first.</li> <li>Consider performance implications of complex filters.</li> <li>Validate metadata during ingestion.</li> <li>Handle missing metadata fields gracefully.</li> <li>Log invalid filter expressions.</li> </ul>"},{"location":"extraction/custom-metadata/#use-custom-metadata-to-filter-results-during-retrieval","title":"Use Custom Metadata to Filter Results During Retrieval","text":"<p>You can use custom metadata to filter documents during retrieval operations.  Use filter expressions that follow the Milvus boolean expression syntax.  For more information, refer to Filtering Explained.</p>"},{"location":"extraction/custom-metadata/#example-filter-expressions","title":"Example Filter Expressions","text":"<p>The following example filters results by category.</p> <pre><code>filter_expr = 'content_metadata[\"category\"] == \"technical\"'\n</code></pre> <p>The following example filters results by time range.</p> <pre><code>filter_expr = 'content_metadata[\"timestamp\"] &gt;= \"2024-03-01T00:00:00\" and content_metadata[\"timestamp\"] &lt;= \"2025-12-31T00:00:00\"'\n</code></pre> <p>The following example filters by category and uses multiple logical operators.</p> <pre><code>filter_expr = '(content_metadata[\"department\"] == \"engineering\" and content_metadata[\"priority\"] == \"high\") or content_metadata[\"category\"] == \"critical\"'\n</code></pre>"},{"location":"extraction/custom-metadata/#example-use-a-filter-expression-in-search","title":"Example: Use a Filter Expression in Search","text":"<p>After ingestion is complete, and documents are uploaded to the database with metadata,  you can use the <code>content_metadata</code> field to filter search results.</p> <p>The following example uses a filter expression to narrow results by department.</p> <pre><code>from nv_ingest_client.util.milvus import nvingest_retrieval\n\nhostname=\"localhost\"\ncollection_name = \"nv_ingest_collection\"\nsparse = True\ntop_k = 5\nmodel_name=\"nvidia/llama-3.2-nv-embedqa-1b-v2\"\n\nfilter_expr = 'content_metadata[\"department\"] == \"Engineering\"'\n\nqueries = [\"this is expensive\"]\nq_results = []\nfor que in queries:\n    q_results\n        .append(\n            nvingest_retrieval(\n                [que], \n                collection_name, \n                f\"http://{hostname}:19530\", \n                embedding_endpoint=f\"http://{hostname}:8012/v1\",  \n                hybrid=sparse, \n                top_k=top_k, \n                model_name=model_name, \n                gpu_search=False, \n                _filter=filter_expr\n            )\n        )\n\nprint(f\"{q_results}\")\n</code></pre>"},{"location":"extraction/custom-metadata/#related-content","title":"Related Content","text":"<ul> <li>For a notebook that uses the CLI to add custom metadata and filter query results, see metadata_and_filtered_search.ipynb .</li> </ul>"},{"location":"extraction/data-store/","title":"Upload Data","text":""},{"location":"extraction/data-store/#data-upload-for-nemo-retriever-extraction","title":"Data Upload for NeMo Retriever Extraction","text":"<p>Use this documentation to learn how NeMo Retriever extraction handles and uploads data.</p> <p>Note</p> <p>NeMo Retriever extraction is also known as NVIDIA Ingest and nv-ingest.</p>"},{"location":"extraction/data-store/#overview","title":"Overview","text":"<p>NeMo Retriever extraction supports extracting text representations of various forms of content,  and ingesting to the Milvus vector database.  NeMo Retriever extraction does not store data on disk directly, except through Milvus.  The data upload task pulls extraction results to the Python client,  and then pushes them to Milvus by using its underlying MinIO object store service.</p> <p>The vector database stores only the extracted text representations of ingested data.  It does not store the embeddings for images.</p> <p>NeMo Retriever extraction supports uploading data by using the Ingestor.vdb_upload API.  Currently, data upload is not supported through the NV Ingest CLI.</p>"},{"location":"extraction/data-store/#upload-to-milvus","title":"Upload to Milvus","text":"<p>The <code>vdb_upload</code> method uses GPU Cagra accelerated bulk indexing support to load chunks into Milvus.  To enable hybrid retrieval, nv-ingest supports both dense (llama-embedder embeddings) and sparse (bm25) embeddings. </p> <p>Bulk indexing is high throughput, but has a built-in overhead of around one minute.  If the number of ingested documents is 10 or fewer, nv-ingest uses faster streaming inserts instead.  You can control this by setting <code>stream=True</code>. </p> <p>If you set <code>recreate=True</code>, nv-ingest drops and recreates the collection given as collection_name.  The Milvus service persists data to disk by using a Docker volume defined in docker-compose.yaml.  You can delete all collections by deleting that volume, and then restarting the nv-ingest service.</p> <p>Warning</p> <p>When you use the <code>vdb_upload</code> task with Milvus, you must expose the ports for the Milvus and MinIO containers to the nv-ingest client. This ensures that the nv-ingest client can connect to both services and perform the <code>vdb_upload</code> action.</p> <p>To upload to Milvus, use code similar to the following.</p> <pre><code>Ingestor(client=client)\n    .files(\"data/multimodal_test.pdf\")\n    .extract()\n    .embed()\n    .caption()\n    .vdb_upload(\n        collection_name=collection_name,\n        milvus_uri=milvus_uri,\n        sparse=sparse,\n        # for llama-3.2 embedder, use 1024 for e5-v5\n        dense_dim=2048,\n        stream=False,\n        recreate=False\n    )\n</code></pre>"},{"location":"extraction/data-store/#upload-to-a-custom-data-store","title":"Upload to a Custom Data Store","text":"<p>You can ingest to other data stores by using the <code>Ingestor.vdb_upload</code> method;  however, you must configure other data stores and connections yourself.  NeMo Retriever extraction does not provide connections to other data sources. </p>"},{"location":"extraction/environment-config/","title":"Environment Configuration Variables for NV-Ingest","text":"<p>The following are the environment configuration variables that you can specify in your .env file.</p>"},{"location":"extraction/environment-config/#general-environment-variables","title":"General Environment Variables","text":"Name Example Description <code>INGEST_LOG_LEVEL</code> - <code>DEBUG</code>  - <code>INFO</code>  - <code>WARNING</code>  - <code>ERROR</code>  - <code>CRITICAL</code> The log level for the ingest service, which controls the verbosity of the logging output. <code>MESSAGE_CLIENT_HOST</code> - <code>redis</code>  - <code>localhost</code>  - <code>192.168.1.10</code> Specifies the hostname or IP address of the message broker used for communication between services. <code>MESSAGE_CLIENT_PORT</code> - <code>7670</code>  - <code>6379</code> Specifies the port number on which the message broker is listening. <code>MINIO_BUCKET</code> <code>nv-ingest</code> Name of MinIO bucket, used to store image, table, and chart extractions. <code>NGC_API_KEY</code> <code>nvapi-*************</code> An authorized NGC API key, used to interact with hosted NIMs. To create an NGC key, go to https://org.ngc.nvidia.com/setup/api-keys. <code>NIM_NGC_API_KEY</code> \u2014 The key that NIM microservices inside docker containers use to access NGC resources. This is necessary only in some cases when it is different from <code>NGC_API_KEY</code>. If this is not specified, <code>NGC_API_KEY</code> is used to access NGC resources. <code>NVIDIA_BUILD_API_KEY</code> \u2014 The key to access NIMs that are hosted on build.nvidia.com instead of a self-hosted NIM. This is necessary only in some cases when it is different from <code>NGC_API_KEY</code>. If this is not specified, <code>NGC_API_KEY</code> is used for build.nvidia.com. <code>OTEL_EXPORTER_OTLP_ENDPOINT</code> <code>http://otel-collector:4317</code> The endpoint for the OpenTelemetry exporter, used for sending telemetry data. <code>REDIS_INGEST_TASK_QUEUE</code> <code>ingest_task_queue</code> The name of the task queue in Redis where tasks are stored and processed. <code>DOWNLOAD_LLAMA_TOKENIZER</code> <code>True</code> If <code>True</code>, the llama-3.2 tokenizer will be pre-dowloaded at build time. If not set to <code>True</code>, the (e5-large-unsupervised)[https://huggingface.co/intfloat/e5-large-unsupervised] tokenizer will be pre-downloaded. Note: setting this to <code>True</code> requires a HuggingFace access token with access to the gated Llama-3.2 models. See below for more info. <code>HF_ACCESS_TOKEN</code> - The HuggingFace access token used to pre-downlaod the Llama-3.2 tokenizer from HuggingFace (see above for more info). Llama 3.2 is a gated model, so you must request access to the Llama-3.2 models and then set this variable to a token that can access gated repositories on your behalf in order to use <code>DOWNLOAD_LLAMA_TOKENIZER=True</code>."},{"location":"extraction/environment-config/#library-mode-environment-variables","title":"Library Mode Environment Variables","text":"<p>These environment variables apply specifically when running NV-Ingest in library mode.</p> Name Example Description <code>NVIDIA_BUILD_API_KEY</code> <code>nvapi-*************</code> API key for NVIDIA-hosted NIM services. <code>NVIDIA_API_KEY</code> <code>nvapi-*************</code> copy of <code>NVIDIA_BUILD_API_KEY</code>, llama-index connectors use this key"},{"location":"extraction/faq/","title":"Frequently Asked Questions for NeMo Retriever Extraction","text":"<p>This documentation contains the Frequently Asked Questions (FAQ) for NeMo Retriever extraction.</p> <p>Note</p> <p>NeMo Retriever extraction is also known as NVIDIA Ingest and nv-ingest.</p>"},{"location":"extraction/faq/#what-if-i-already-have-a-retrieval-pipeline-can-i-just-use-nemo-retriever-extraction","title":"What if I already have a retrieval pipeline? Can I just use NeMo Retriever extraction?","text":"<p>You can use the nv-ingest-cli or Python APIs to perform extraction only, and then consume the results. Using the Python API, <code>results</code> is a list object with one entry. For code examples, see the Jupyter notebooks Multimodal RAG with LlamaIndex  and Multimodal RAG with LangChain.</p>"},{"location":"extraction/faq/#where-does-nemo-retriever-extraction-nv-ingest-ingest-to","title":"Where does NeMo Retriever extraction (nv-ingest) ingest to?","text":"<p>NeMo Retriever extraction supports extracting text representations of various forms of content,  and ingesting to the Milvus vector database.  NeMo Retriever extraction does not store data on disk except through Milvus and its underlying Minio object store.  You can ingest to other data stores; however, you must configure other data stores yourself.  For more information, refer to Data Upload.</p>"},{"location":"extraction/faq/#how-would-i-process-unstructured-images","title":"How would I process unstructured images?","text":"<p>For images that <code>nemoretriever-page-elements-v2</code> does not classify as tables, charts, or infographics,  you can use our VLM caption task to create a dense caption of the detected image.  That caption is then be embedded along with the rest of your content.  For more information, refer to Extract Captions from Images.</p>"},{"location":"extraction/faq/#when-should-i-consider-using-nemoretriever-parse","title":"When should I consider using nemoretriever-parse?","text":"<p>For scanned documents, or documents with complex layouts,  we recommend that you use nemoretriever-parse.  Nemo Retriever parse provides higher-accuracy text extraction.  For more information, refer to Use Nemo Retriever Extraction with nemoretriever-parse.</p>"},{"location":"extraction/faq/#why-are-the-environment-variables-different-between-library-mode-and-self-hosted-mode","title":"Why are the environment variables different between library mode and self-hosted mode?","text":""},{"location":"extraction/faq/#self-hosted-deployments","title":"Self-Hosted Deployments","text":"<p>For self-hosted deployments, you should set the environment variables <code>NGC_API_KEY</code> and <code>NIM_NGC_API_KEY</code>. For more information, refer to Generate Your NGC Keys.</p> <p>For advanced scenarios, you might want to set <code>docker-compose</code> environment variables for NIM container paths, tags, and batch sizes.  You can set those directly in <code>docker-compose.yaml</code>, or in an environment variable file that docker compose uses.</p>"},{"location":"extraction/faq/#library-mode","title":"Library Mode","text":"<p>For library mode, you should set the environment variables <code>NVIDIA_BUILD_API_KEY</code> and <code>NVIDIA_API_KEY</code>.  For more information, refer to Generate Your NGC Keys.</p> <p>For advanced scenarios, you might want to use library mode with self-hosted NIM instances.  You can set custom endpoints for each NIM.  For examples of <code>*_ENDPOINT</code> variables, refer to nv-ingest/docker-compose.yaml.</p>"},{"location":"extraction/faq/#what-parameters-or-settings-can-i-adjust-to-optimize-extraction-from-my-documents-or-data","title":"What parameters or settings can I adjust to optimize extraction from my documents or data?","text":"<p>See the Profile Information section  for information about the optional NIM components of the pipeline.</p> <p>You can configure the <code>extract</code>, <code>caption</code>, and other tasks by using the Ingestor API.</p> <p>To choose what types of content to extract, use code similar to the following.  For more information, refer to Extract Specific Elements from PDFs.</p> <pre><code>Ingestor(client=client)\n    .files(\"data/multimodal_test.pdf\")\n    .extract(              \n        extract_text=True,\n        extract_tables=True,\n        extract_charts=True,\n        extract_images=True,\n        paddle_output_format=\"markdown\",\n        extract_infographics=True,\n        text_depth=\"page\"\n    )\n</code></pre> <p>To generate captions for images, use code similar to the following. For more information, refer to Extract Captions from Images.</p> <pre><code>Ingestor(client=client)\n    .files(\"data/multimodal_test.pdf\")\n    .extract()\n    .embed()\n    .caption()\n)\n</code></pre>"},{"location":"extraction/helm/","title":"Deploy With Helm for NeMo Retriever Extraction","text":"<p>To deploy NeMo Retriever extraction by using Helm,  refer to NV-Ingest Helm Charts.</p>"},{"location":"extraction/nemoretriever-parse/","title":"Use NeMo Retriever Parse","text":""},{"location":"extraction/nemoretriever-parse/#use-nemo-retriever-extraction-with-nemoretriever-parse","title":"Use Nemo Retriever Extraction with nemoretriever-parse","text":"<p>This documentation describes two methods to run NeMo Retriever extraction  with nemoretriever-parse.</p> <ul> <li>Run the NIM locally by using Docker Compose</li> <li>Use NVIDIA Cloud Functions (NVCF) endpoints for cloud-based inference</li> </ul> <p>Note</p> <p>NeMo Retriever extraction is also known as NVIDIA Ingest and nv-ingest.</p> <p>Currently, extraction with nemoretriever-parse only supports PDFs, not image files.  For more information, see Troubleshoot Nemo Retriever Extraction.</p>"},{"location":"extraction/nemoretriever-parse/#run-the-nim-locally-by-using-docker-compose","title":"Run the NIM Locally by Using Docker Compose","text":"<p>Use the following procedure to run the NIM locally.</p> <p>Important</p> <p>Due to limitations in available VRAM controls in the current release of nemoretriever_parse, it must run on a dedicated additional GPU. Edit docker-compose.yaml to set nemoretriever_parse's device_id to a dedicated GPU: device_ids: [\"1\"] or higher.</p> <ol> <li> <p>Start the nv-ingest services with the <code>nemoretriever-parse</code> profile. This profile includes the necessary components for extracting text and metadata from images. Use the following command.</p> <ul> <li>The --profile nemoretriever-parse flag ensures that vision-language retrieval services are launched.  For more information, refer to Profile Information.</li> <li>The --build flag ensures that any changes to the container images are applied before starting.</li> </ul> <pre><code>docker compose --profile nemoretriever-parse up --build\n</code></pre> </li> <li> <p>After the services are running, you can interact with nv-ingest by using Python.</p> <ul> <li>The <code>Ingestor</code> object initializes the ingestion process.</li> <li>The <code>files</code> method specifies the input files to process.</li> <li>The <code>extract</code> method tells nv-ingest to use <code>nemoretriever-parse</code> for extracting text and metadata from images.</li> <li>The <code>document_type</code> parameter is optional, because <code>Ingestor</code> should detect the file type automatically.</li> </ul> <pre><code>ingestor = (\n    Ingestor()\n    .files(\"./data/*.pdf\")\n    .extract(\n        document_type=\"pdf\",  # Ingestor should detect type automatically in most cases\n        extract_method=\"nemoretriever_parse\"\n    )\n)\n</code></pre> <p>Tip</p> <p>For more Python examples, refer to NV-Ingest: Python Client Quick Start Guide.</p> </li> </ol>"},{"location":"extraction/nemoretriever-parse/#using-nvcf-endpoints-for-cloud-based-inference","title":"Using NVCF Endpoints for Cloud-Based Inference","text":"<p>Instead of running NV-Ingest locally, you can use NVCF to perform inference by using remote endpoints.</p> <ol> <li> <p>Set the authentication token in the <code>.env</code> file.</p> <pre><code>NVIDIA_BUILD_API_KEY=nvapi-...\n</code></pre> </li> <li> <p>Modify <code>docker-compose.yaml</code> to use the hosted <code>nemoretriever-parse</code> service.</p> <pre><code># build.nvidia.com hosted nemoretriever-parse\n- NEMORETRIEVER_PARSE_HTTP_ENDPOINT=https://integrate.api.nvidia.com/v1/chat/completions\n#- NEMORETRIEVER_PARSE_HTTP_ENDPOINT=http://nemoretriever-parse:8000/v1/chat/completions\n</code></pre> </li> <li> <p>Run inference by using Python.</p> <ul> <li>The <code>Ingestor</code> object initializes the ingestion process.</li> <li>The <code>files</code> method specifies the input files to process.</li> <li>The <code>extract</code> method tells nv-ingest to use <code>nemoretriever-parse</code> for extracting text and metadata from images.</li> <li>The <code>document_type</code> parameter is optional, because <code>Ingestor</code> should detect the file type automatically.</li> </ul> <pre><code>ingestor = (\n    Ingestor()\n    .files(\"./data/*.pdf\")\n    .extract(\n        document_type=\"pdf\",  # Ingestor should detect type automatically in most cases\n        extract_method=\"nemoretriever_parse\"\n    )\n)\n</code></pre> <p>Tip</p> <p>For more Python examples, refer to NV-Ingest: Python Client Quick Start Guide.</p> </li> </ol>"},{"location":"extraction/ngc-api-key/","title":"Generate Your NGC Keys","text":"<p>NGC contains many public images, models, and datasets that can be pulled immediately without authentication.  To push and pull custom images, you must generate a key and authenticate with NGC.</p> <p>To create a key, go to https://org.ngc.nvidia.com/setup/api-keys.</p> <p>When you create an NGC key, select the following for Services Included.</p> <ul> <li>NGC Catalog</li> <li>Public API Endpoints</li> </ul> <p>Important</p> <p>Early Access participants must also select Private Registry.</p> <p></p>"},{"location":"extraction/ngc-api-key/#docker-login-to-ngc","title":"Docker Login to NGC","text":"<p>To pull the NIM container image from NGC, use your key to log in to the NGC registry by entering the following command and then following the prompts.  For the username, enter <code>$oauthtoken</code> exactly as shown.  It is a special authentication key for all users.</p> <pre><code>$ docker login nvcr.io\nUsername: $oauthtoken\nPassword: &lt;Your Key&gt;\n</code></pre>"},{"location":"extraction/notebooks/","title":"Notebooks for NeMo Retriever Extraction","text":"<p>To get started using NeMo Retriever extraction, you can try one of the ready-made notebooks that are available.</p> <p>Note</p> <p>NeMo Retriever extraction is also known as NVIDIA Ingest and nv-ingest.</p> <p>To get started with the basics, try one of the following notebooks:</p> <ul> <li>NV-Ingest: CLI Client Quick Start Guide</li> <li>NV-Ingest: Python Client Quick Start Guide</li> <li>How to add metadata to your documents and filter searches</li> <li>How to reindex a collection</li> </ul> <p>For more advanced scenarios, try one of the following notebooks:</p> <ul> <li>Try out the NVIDIA Multimodal PDF Data Extraction Blueprint</li> <li>Evaluate bo767 retrieval recall accuracy with NV-Ingest and Milvus</li> <li>Multimodal RAG with LangChain</li> <li>Multimodal RAG with LlamaIndex</li> </ul>"},{"location":"extraction/notebooks/#related-topics","title":"Related Topics","text":"<ul> <li>Prerequisites</li> <li>Support Matrix</li> <li>Deploy Without Containers (Library Mode)</li> <li>Deploy With Docker Compose (Self-Hosted)</li> <li>Deploy With Helm</li> </ul>"},{"location":"extraction/nv-ingest-python-api/","title":"Use the NV-Ingest Python API","text":"<p>The NV-Ingest Python API provides a simple and flexible interface for processing and extracting information from various document types, including PDFs.</p> <p>Tip</p> <p>There is a Jupyter notebook available to help you get started with the Python API. For more information, refer to Python Client Quick Start Guide.</p>"},{"location":"extraction/nv-ingest-python-api/#summary-of-key-methods","title":"Summary of Key Methods","text":"<p>The main class in the nv-ingest API is <code>Ingestor</code>.  The <code>Ingestor</code> class provides an interface for building, managing, and running data ingestion jobs, enabling for chainable task additions and job state tracking.  The following table describes methods of the <code>Ingestor</code> class.</p> Method Description <code>caption</code> Extract captions from images within the document. <code>embed</code> Generate embeddings from extracted content. <code>extract</code> Add an extraction task (text, tables, charts). <code>files</code> Add document paths for processing. <code>ingest</code> Submit jobs and retrieve results synchronously. <code>load</code> Ensure files are locally accessible (downloads if needed). <code>split</code> Split documents into smaller sections for processing. For more information, refer to Split Documents. <code>vdb_upload</code> Pushes extraction results to Milvus vector database. For more information, refer to Data Upload."},{"location":"extraction/nv-ingest-python-api/#quick-start-extracting-pdfs","title":"Quick Start: Extracting PDFs","text":"<p>The following example demonstrates how to initialize <code>Ingestor</code>, load a PDF file, and extract its contents. The <code>extract</code> method enables different types of data to be extracted.</p>"},{"location":"extraction/nv-ingest-python-api/#extract-a-single-pdf","title":"Extract a Single PDF","text":"<p>Use the following code to extract a single PDF file.</p> <pre><code>from nv_ingest_client.client.interface import Ingestor\n\n# Initialize Ingestor with a local PDF file\ningestor = Ingestor().files(\"path/to/document.pdf\")\n\n# Extract text, tables, and images\nresult = ingestor.extract().ingest()\n\nprint(result)\n</code></pre>"},{"location":"extraction/nv-ingest-python-api/#extract-multiple-pdfs","title":"Extract Multiple PDFs","text":"<p>Use the following code to process multiple PDFs at one time.</p> <pre><code>ingestor = Ingestor().files([\"path/to/doc1.pdf\", \"path/to/doc2.pdf\"])\n\n# Extract content from all PDFs\nresult = ingestor.extract().ingest()\n\nfor doc in result:\n    print(doc)\n</code></pre>"},{"location":"extraction/nv-ingest-python-api/#extract-specific-elements-from-pdfs","title":"Extract Specific Elements from PDFs","text":"<p>By default, the <code>extract</code> method extracts all supported content types.  You can customize the extraction behavior by using the following code.</p> <pre><code>ingestor = ingestor.extract(\n    extract_text=True,  # Extract text\n    text_depth=\"page\",\n    extract_tables=False,  # Skip table extraction\n    extract_charts=True,  # Extract charts\n    extract_infographics=True,  # Extract infographic images\n    extract_images=False  # Skip image extraction\n)\n</code></pre>"},{"location":"extraction/nv-ingest-python-api/#extract-non-standard-document-types","title":"Extract Non-standard Document Types","text":"<p>NV-Ingest also supports extracting text from <code>.md</code>, <code>.sh</code>, and <code>.html</code> files</p> <pre><code>ingestor = Ingestor().files([\"path/to/doc1.md\", \"path/to/doc2.html\"])\n\ningestor = ingestor.extract(\n    extract_text=True,  # Only extract text\n    extract_tables=False,\n    extract_charts=False,\n    extract_infographics=False,\n    extract_images=False\n)\n\nresult = ingestor.ingest()\n</code></pre>"},{"location":"extraction/nv-ingest-python-api/#extract-with-custom-document-type","title":"Extract with Custom Document Type","text":"<p>Use the following code to specify a custom document type for extraction.</p> <pre><code>ingestor = ingestor.extract(document_type=\"pdf\")\n</code></pre>"},{"location":"extraction/nv-ingest-python-api/#track-job-progress","title":"Track Job Progress","text":"<p>For large document batches, you can enable a progress bar by setting <code>show_progress</code> to true.  Use the following code.</p> <pre><code>result = ingestor.extract().ingest(show_progress=True)\n</code></pre>"},{"location":"extraction/nv-ingest-python-api/#extract-captions-from-images","title":"Extract Captions from Images","text":"<p>The <code>caption</code> method generates image captions by using a vision-language model.  This can be used to describe images extracted from documents.</p> <p>Note</p> <p>The default model used by <code>caption</code> is <code>meta/llama-3.2-11b-vision-instruct</code>.</p> <pre><code>ingestor = ingestor.caption()\n</code></pre> <p>To specify a different API endpoint, pass additional parameters to <code>caption</code>.</p> <pre><code>ingestor = ingestor.caption(\n    endpoint_url=\"https://ai.api.nvidia.com/v1/gr/meta/llama-3.2-11b-vision-instruct/chat/completions\",\n    model_name=\"meta/llama-3.2-11b-vision-instruct\",\n    api_key=\"nvapi-\"\n)\n</code></pre>"},{"location":"extraction/nv-ingest-python-api/#extract-embeddings","title":"Extract Embeddings","text":"<p>The <code>embed</code> method in NV-Ingest generates text embeddings for document content.</p> <pre><code>ingestor = ingestor.embed()\n</code></pre> <p>Note</p> <p>By default, <code>embed</code> uses the llama-3.2-nv-embedqa-1b-v2 model.</p> <p>To use a different embedding model, such as nv-embedqa-e5-v5, specify a different <code>model_name</code> and <code>endpoint_url</code>.</p> <pre><code>ingestor = ingestor.embed(\n    endpoint_url=\"https://integrate.api.nvidia.com/v1\",\n    model_name=\"nvidia/nv-embedqa-e5-v5\",\n    api_key=\"nvapi-\"\n)\n</code></pre>"},{"location":"extraction/nv-ingest_cli/","title":"Use the NV-Ingest Command Line Interface","text":"<p>After you install the Python dependencies, you can use the NV-Ingest command line interface (CLI).  To use the CLI, use the <code>nv-ingest-cli</code> command.</p> <p>To check the version of the CLI that you have installed, run the following command.</p> <pre><code>nv-ingest-cli --version\n</code></pre> <p>To get a list of the current CLI commands and their options, run the following command.</p> <pre><code>nv-ingest-cli --help\n</code></pre> <p>Tip</p> <p>There is a Jupyter notebook available to help you get started with the CLI. For more information, refer to CLI Client Quick Start Guide.</p>"},{"location":"extraction/nv-ingest_cli/#examples","title":"Examples","text":"<p>Use the following code examples to submit a document to the <code>nv-ingest-ms-runtime</code> service.</p> <p>Each of the following commands can be run from the host machine, or from within the <code>nv-ingest-ms-runtime</code> container.</p> <ul> <li>Host: <code>nv-ingest-cli ...</code></li> <li>Container: <code>nv-ingest-cli ...</code></li> </ul>"},{"location":"extraction/nv-ingest_cli/#example-text-file-with-no-splitting","title":"Example: Text File With No Splitting","text":"<p>To submit a text file with no splitting, run the following code.</p> <p>Note</p> <p>You receive a response that contains a single document, which is the entire text file. The data that is returned is wrapped in the appropriate metadata structure.</p> <pre><code>nv-ingest-cli \\\n  --doc ./data/test.pdf \\\n  --client_host=localhost \\\n  --client_port=7670\n</code></pre>"},{"location":"extraction/nv-ingest_cli/#example-pdf-file-with-splitting-only","title":"Example: PDF File With Splitting Only","text":"<p>To submit a .pdf file with only a splitting task, run the following code.</p> <pre><code>nv-ingest-cli \\\n  --doc ./data/test.pdf \\\n  --output_directory ./processed_docs \\\n  --task='split' \\\n  --client_host=localhost \\\n  --client_port=7670\n</code></pre>"},{"location":"extraction/nv-ingest_cli/#example-pdf-file-with-splitting-and-extraction","title":"Example: PDF File With Splitting and Extraction","text":"<p>To submit a .pdf file with both a splitting task and an extraction task, run the following code.</p> <p>Note</p> <p>This currently only works for pdfium, nemoretriever_parse, and Unstructured.io. Haystack, Adobe, and LlamaParse have existing workflows, but have not been fully converted to use our unified metadata schema.</p> <pre><code>nv-ingest-cli \\\n  --doc ./data/test.pdf \\\n  --output_directory ./processed_docs \\\n  --task='extract:{\"document_type\": \"pdf\", \"extract_method\": \"pdfium\"}' \\\n  --task='extract:{\"document_type\": \"docx\", \"extract_method\": \"python_docx\"}' \\\n  --task='split' \\\n  --client_host=localhost \\\n  --client_port=7670\n</code></pre>"},{"location":"extraction/nv-ingest_cli/#example-process-a-dataset","title":"Example: Process a Dataset","text":"<p>To submit a dataset for processing, run the following code.  To create a dataset, refer to Command Line Dataset Creation with Enumeration and Sampling.</p> <pre><code>nv-ingest-cli \\\n  --dataset dataset.json \\\n  --output_directory ./processed_docs \\\n  --task='extract:{\"document_type\": \"pdf\", \"extract_method\": \"pdfium\"}' \\\n  --client_host=localhost \\\n  --client_port=7670\n</code></pre> <p>Submit a PDF file with extraction tasks and upload extracted images to MinIO.</p> <pre><code>nv-ingest-cli \\\n  --doc ./data/test.pdf \\\n  --output_directory ./processed_docs \\\n  --task='extract:{\"document_type\": \"pdf\", \"extract_method\": \"pdfium\"}' \\\n  --client_host=localhost \\\n  --client_port=7670\n</code></pre>"},{"location":"extraction/nv-ingest_cli/#command-line-dataset-creation-with-enumeration-and-sampling","title":"Command Line Dataset Creation with Enumeration and Sampling","text":"<p>The <code>gen_dataset.py</code> script samples files from a specified source directory according to defined proportions and a total size target.  It offers options for caching the file list, outputting a sampled file list, and validating the output.</p> <pre><code>python ./src/util/gen_dataset.py --source_directory=./data --size=1GB --sample pdf=60 --sample txt=40 --output_file \\\n  dataset.json --validate-output\n</code></pre>"},{"location":"extraction/overview/","title":"What is NeMo Retriever Extraction?","text":"<p>NeMo Retriever extraction is a scalable, performance-oriented document content and metadata extraction microservice.  NeMo Retriever extraction uses specialized NVIDIA NIM microservices  to find, contextualize, and extract text, tables, charts and images that you can use in downstream generative applications.</p> <p>Note</p> <p>NeMo Retriever extraction is also known as NVIDIA Ingest and nv-ingest.</p> <p>NeMo Retriever extraction enables parallelization of splitting documents into pages where artifacts are classified (such as text, tables, charts, and images), extracted, and further contextualized through optical character recognition (OCR) into a well defined JSON schema.  From there, NeMo Retriever extraction can optionally manage computation of embeddings for the extracted content,  and optionally manage storing into a vector database Milvus.</p> <p>Note</p> <p>Cached and Deplot are deprecated. Instead, NeMo Retriever extraction now uses the yolox-graphic-elements NIM. With this change, you should now be able to run NeMo Retriever extraction on a single 24GB A10G or better GPU. If you want to use the old pipeline, with Cached and Deplot, use the NeMo Retriever extraction 24.12.1 release.</p>"},{"location":"extraction/overview/#what-nemo-retriever-extraction-is","title":"What NeMo Retriever Extraction Is \u2714\ufe0f","text":"<p>The following diagram shows the Nemo Retriever extraction pipeline.</p> <p></p> <p>NeMo Retriever extraction is a microservice service that does the following:</p> <ul> <li>Accept a JSON job description, containing a document payload, and a set of ingestion tasks to perform on that payload.</li> <li>Allow the results of a job to be retrieved. The result is a JSON dictionary that contains a list of metadata describing objects extracted from the base document, and processing annotations and timing/trace data.</li> <li>Support multiple methods of extraction for each document type to balance trade-offs between throughput and accuracy. For example, for .pdf documents, extraction is performed by using pdfium, nemoretriever-parse, Unstructured.io, and Adobe Content Extraction Services.</li> <li>Support various types of pre- and post- processing operations, including text splitting and chunking, transform and filtering, embedding generation, and image offloading to storage.</li> </ul> <p>NeMo Retriever extraction supports the following file types:</p> <ul> <li><code>bmp</code></li> <li><code>docx</code></li> <li><code>html</code> (treated as text)</li> <li><code>jpeg</code></li> <li><code>json</code> (treated as text)</li> <li><code>md</code> (treated as text)</li> <li><code>pdf</code></li> <li><code>png</code></li> <li><code>pptx</code></li> <li><code>sh</code> (treated as text)</li> <li><code>tiff</code></li> <li><code>txt</code></li> </ul>"},{"location":"extraction/overview/#what-nemo-retriever-extraction-isnt","title":"What NeMo Retriever Extraction Isn't \u2716\ufe0f","text":"<p>NeMo Retriever extraction does not do the following:</p> <ul> <li>Run a static pipeline or fixed set of operations on every submitted document.</li> <li>Act as a wrapper for any specific document parsing library.</li> </ul>"},{"location":"extraction/overview/#related-topics","title":"Related Topics","text":"<ul> <li>Prerequisites</li> <li>Support Matrix</li> <li>Deploy Without Containers (Library Mode)</li> <li>Deploy With Docker Compose (Self-Hosted)</li> <li>Deploy With Helm</li> <li>Notebooks</li> <li>Multimodal PDF Data Extraction</li> </ul>"},{"location":"extraction/prerequisites/","title":"Prerequisites for NeMo Retriever Extraction","text":"<p>Before you begin using NeMo Retriever extraction, ensure the following software prerequisites are met.</p>"},{"location":"extraction/prerequisites/#software","title":"Software","text":"<ul> <li>Linux operating systems (Ubuntu 22.04 or later recommended)</li> <li>Docker</li> <li>Docker Compose</li> <li>CUDA Toolkit (NVIDIA Driver &gt;= <code>535</code>, CUDA &gt;= <code>12.2</code>)</li> <li>NVIDIA Container Toolkit</li> <li>Conda Python environment and package manager</li> </ul> <p>Note</p> <p>You install Python later. NV-Ingest only supports Python version 3.10.</p>"},{"location":"extraction/prerequisites/#related-topics","title":"Related Topics","text":"<ul> <li>Support Matrix</li> <li>Deploy Without Containers (Library Mode)</li> <li>Deploy With Docker Compose (Self-Hosted)</li> <li>Deploy With Helm</li> </ul>"},{"location":"extraction/quickstart-guide/","title":"Deploy With Docker Compose (Self-Hosted) for NeMo Retriever Extraction","text":"<p>Use this documentation to get started using NeMo Retriever extraction in self-hosted mode.</p>"},{"location":"extraction/quickstart-guide/#step-1-starting-containers","title":"Step 1: Starting Containers","text":"<p>This example demonstrates how to use the provided docker-compose.yaml to start all needed services with a few commands.</p> <p>Warning</p> <p>NIM containers on their first startup can take 10-15 minutes to pull and fully load models.</p> <p>If you prefer, you can run on Kubernetes by using our Helm chart. Also, there are additional environment variables you might want to configure.</p> <ol> <li> <p>Git clone the repo:</p> <p><code>git clone https://github.com/nvidia/nv-ingest</code></p> </li> <li> <p>Change the directory to the cloned repo</p> <p><code>cd nv-ingest</code>.</p> </li> <li> <p>Generate API keys and authenticate with NGC with the <code>docker login</code> command:</p> <pre><code># This is required to access pre-built containers and NIM microservices\n$ docker login nvcr.io\nUsername: $oauthtoken\nPassword: &lt;Your Key&gt;\n</code></pre> </li> <li> <p>Create a .env file that contains your NVIDIA Build API key.</p> <p>Note</p> <p>If you use an NGC personal key, then you should provide the same value for all keys, but you must specify each environment variable individually. In the past, you could create an API key. If you have an API key, you can still use that. For more information, refer to Generate Your NGC Keys and Environment Configuration Variables.</p> <pre><code># Container images must access resources from NGC.\n\nNGC_API_KEY=&lt;key to download containers from NGC&gt;\nNIM_NGC_API_KEY=&lt;key to download model files after containers start&gt;\n</code></pre> </li> <li> <p>Make sure NVIDIA is set as your default container runtime before running the docker compose command with the command:</p> <p><code>sudo nvidia-ctk runtime configure --runtime=docker --set-as-default</code></p> </li> <li> <p>Start core services. This example uses the table-structure profile.  For more information about other profiles, see Profile Information.</p> <p><code>docker compose --profile retrieval --profile table-structure up</code></p> <p>Tip</p> <p>By default, we have configured log levels to be verbose. It's possible to observe service startup proceeding. You will notice a lot of log messages. Disable verbose logging by configuring <code>NIM_TRITON_LOG_VERBOSE=0</code> for each NIM in docker-compose.yaml.</p> </li> <li> <p>When core services have fully started, <code>nvidia-smi</code> should show processes like the following:</p> <pre><code># If it's taking &gt; 1m for `nvidia-smi` to return, the bus will likely be busy setting up the models.\n+---------------------------------------------------------------------------------------+\n| Processes:                                                                            |\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n|        ID   ID                                                             Usage      |\n|=======================================================================================|\n|    0   N/A  N/A     80461      C   milvus                                     1438MiB |\n|    0   N/A  N/A     83791      C   tritonserver                               2492MiB |\n|    0   N/A  N/A     85605      C   tritonserver                               1896MiB |\n|    0   N/A  N/A     85889      C   tritonserver                               2824MiB |\n|    0   N/A  N/A     88253      C   tritonserver                               2824MiB |\n|    0   N/A  N/A     91194      C   tritonserver                               4546MiB |\n+---------------------------------------------------------------------------------------+\n</code></pre> </li> <li> <p>Observe the started containers with <code>docker ps</code>:</p> <pre><code>CONTAINER ID   IMAGE                                                                                                  COMMAND                  CREATED          STATUS                   PORTS                                                                                                                                                                                                                                                                                                       NAMES\n1b885f37c991   nvcr.io/nvidia/nemo-microservices/nv-ingest:25.03                                                      \"/opt/conda/envs/nv_\u2026\"   3 minutes ago    Up 3 minutes (healthy)   0.0.0.0:7670-7671-&gt;7670-7671/tcp, :::7670-7671-&gt;7670-7671/tcp                                                                                                                                                                                                                                               nv-ingest-nv-ingest-ms-runtime-1\n62c6b999c413   zilliz/attu:v2.3.5                                                                                     \"docker-entrypoint.s\u2026\"   13 minutes ago   Up 3 minutes             0.0.0.0:3001-&gt;3000/tcp, :::3001-&gt;3000/tcp                                                                                                                                                                                                                                                                   milvus-attu\n14ef31ed7f49   milvusdb/milvus:v2.5.3-gpu                                                                             \"/tini -- milvus run\u2026\"   13 minutes ago   Up 3 minutes (healthy)   0.0.0.0:9091-&gt;9091/tcp, :::9091-&gt;9091/tcp, 0.0.0.0:19530-&gt;19530/tcp, :::19530-&gt;19530/tcp                                                                                                                                                                                                                    milvus-standalone\ndceaf36cc5df   otel/opentelemetry-collector-contrib:0.91.0                                                            \"/otelcol-contrib --\u2026\"   13 minutes ago   Up 3 minutes             0.0.0.0:4317-4318-&gt;4317-4318/tcp, :::4317-4318-&gt;4317-4318/tcp, 0.0.0.0:8889-&gt;8889/tcp, :::8889-&gt;8889/tcp, 0.0.0.0:9988-&gt;9988/tcp, :::9988-&gt;9988/tcp, 0.0.0.0:13133-&gt;13133/tcp, :::13133-&gt;13133/tcp, 55678/tcp, 0.0.0.0:33249-&gt;9411/tcp, :::33247-&gt;9411/tcp, 0.0.0.0:55680-&gt;55679/tcp, :::55680-&gt;55679/tcp   nv-ingest-otel-collector-1\nfb252020e4d2   nvcr.io/nvidia/nim/nemoretriever-graphic-elements-v1:1.2.0-rc1-latest-datacenter-release-24734263   \"/opt/nim/start_serv\u2026\"   13 minutes ago   Up 3 minutes             0.0.0.0:8003-&gt;8000/tcp, :::8003-&gt;8000/tcp, 0.0.0.0:8004-&gt;8001/tcp, :::8004-&gt;8001/tcp, 0.0.0.0:8005-&gt;8002/tcp, :::8005-&gt;8002/tcp                                                                                                                                                                             nv-ingest-graphic-elements-1\nc944a9d76831   nvcr.io/nvidia/nim/paddleocr:1.2.0-latest-datacenter-release-24685083                               \"/opt/nim/start_serv\u2026\"   13 minutes ago   Up 3 minutes             0.0.0.0:8009-&gt;8000/tcp, :::8009-&gt;8000/tcp, 0.0.0.0:8010-&gt;8001/tcp, :::8010-&gt;8001/tcp, 0.0.0.0:8011-&gt;8002/tcp, :::8011-&gt;8002/tcp                                                                                                                                                                             nv-ingest-paddle-1\n5bea344526a2   nvcr.io/nvidia/nim/nemoretriever-page-elements-v2:1.2.0-rc0-latest-datacenter-release-24730057      \"/opt/nim/start_serv\u2026\"   13 minutes ago   Up 3 minutes             0.0.0.0:8000-8002-&gt;8000-8002/tcp, :::8000-8002-&gt;8000-8002/tcp                                                                                                                                                                                                                                               nv-ingest-page-elements-1\n16dc2311a6cc   nvcr.io/nvidia/nim/llama-3.2-nv-embedqa-1b-v2:1.5.0-rc0-latest-datacenter-release-24738403          \"/opt/nim/start_serv\u2026\"   13 minutes ago   Up 3 minutes             0.0.0.0:8012-&gt;8000/tcp, :::8012-&gt;8000/tcp, 0.0.0.0:8013-&gt;8001/tcp, :::8013-&gt;8001/tcp, 0.0.0.0:8014-&gt;8002/tcp, :::8014-&gt;8002/tcp                                                                                                                                                                             nv-ingest-embedding-1\ncea3ce001888   nvcr.io/nvidia/nim/nemoretriever-table-structure-v1:1.2.0-rc1-latest-datacenter-release-24826492    \"/opt/nim/start_serv\u2026\"   13 minutes ago   Up 3 minutes             0.0.0.0:8006-&gt;8000/tcp, :::8006-&gt;8000/tcp, 0.0.0.0:8007-&gt;8001/tcp, :::8007-&gt;8001/tcp, 0.0.0.0:8008-&gt;8002/tcp, :::8008-&gt;8002/tcp                                                                                                                                                                             nv-ingest-table-structure-1\n7ddbf7690036   openzipkin/zipkin                                                                                      \"start-zipkin\"           13 minutes ago   Up 3 minutes (healthy)   9410/tcp, 0.0.0.0:9411-&gt;9411/tcp, :::9411-&gt;9411/tcp                                                                                                                                                                                                                                                         nv-ingest-zipkin-1\nb73bbe0c202d   minio/minio:RELEASE.2023-03-20T20-16-18Z                                                               \"/usr/bin/docker-ent\u2026\"   13 minutes ago   Up 3 minutes (healthy)   0.0.0.0:9000-9001-&gt;9000-9001/tcp, :::9000-9001-&gt;9000-9001/tcp                                                                                                                                                                                                                                               minio\n97fa798dbe4f   prom/prometheus:latest                                                                                 \"/bin/prometheus --w\u2026\"   13 minutes ago   Up 3 minutes             0.0.0.0:9090-&gt;9090/tcp, :::9090-&gt;9090/tcp                                                                                                                                                                                                                                                                   nv-ingest-prometheus-1\nf17cb556b086   grafana/grafana                                                                                        \"/run.sh\"                13 minutes ago   Up 3 minutes             0.0.0.0:3000-&gt;3000/tcp, :::3000-&gt;3000/tcp                                                                                                                                                                                                                                                                   grafana-service\n3403c5a0e7be   redis/redis-stack                                                                                      \"/entrypoint.sh\"         13 minutes ago   Up 3 minutes             0.0.0.0:6379-&gt;6379/tcp, :::6379-&gt;6379/tcp, 8001/tcp                                                                                                                                                                                                                                                         nv-ingest-redis-1\n</code></pre> </li> </ol>"},{"location":"extraction/quickstart-guide/#step-2-install-python-dependencies","title":"Step 2: Install Python Dependencies","text":"<p>You can interact with the NV-Ingest service from the host, or by using <code>docker exec</code> to run commands in the NV-Ingest container.</p> <p>To interact from the host, you'll need a Python environment and install the client dependencies:</p> <pre><code># conda not required but makes it easy to create a fresh Python environment\nconda create --name nv-ingest-dev python=3.12.11\nconda activate nv-ingest-dev\npip install nv-ingest-client==2025.3.10.dev20250310\n</code></pre> <p>Tip</p> <p>To confirm that you have activated your Conda environment, run <code>which pip</code> and <code>which python</code>, and confirm that you see <code>nvingest</code> in the result. You can do this before any pip or python command that you run.</p> <p>Note</p> <p>Interacting from the host depends on the appropriate port being exposed from the nv-ingest container to the host as defined in docker-compose.yaml. If you prefer, you can disable exposing that port and interact with the NV-Ingest service directly from within its container. To interact within the container run <code>docker exec -it nv-ingest-nv-ingest-ms-runtime-1 bash</code>. You'll be in the <code>/workspace</code> directory with <code>DATASET_ROOT</code> from the .env file mounted at <code>./data</code>. The pre-activated <code>nv_ingest_runtime</code> conda environment has all the Python client libraries pre-installed and you should see <code>(nv_ingest_runtime) root@aba77e2a4bde:/workspace#</code>. From the bash prompt above, you can run the nv-ingest-cli and Python examples described following.</p>"},{"location":"extraction/quickstart-guide/#step-3-ingesting-documents","title":"Step 3: Ingesting Documents","text":"<p>You can submit jobs programmatically in Python or using the NV-Ingest CLI.</p> <p>In the below examples, we are doing text, chart, table, and image extraction:</p> <ul> <li>extract_text \u2014 Uses PDFium to find and extract text from pages.</li> <li>extract_images \u2014 Uses PDFium to extract images.</li> <li>extract_tables \u2014 Uses object detection family of NIMs to find tables and charts, and PaddleOCR NIM for table extraction.</li> <li>extract_charts \u2014 Enables or disables chart extraction, also based on the object detection NIM family.</li> </ul>"},{"location":"extraction/quickstart-guide/#in-python","title":"In Python","text":"<p>Tip</p> <p>For more Python examples, refer to NV-Ingest: Python Client Quick Start Guide.</p> <pre><code>import logging, os, time\nfrom nv_ingest_client.client import Ingestor, NvIngestClient\nfrom nv_ingest_client.util.process_json_files import ingest_json_results_to_blob\nclient = NvIngestClient(                                                                         \n    message_client_port=7670,                                                               \n    message_client_hostname=\"localhost\"        \n)                                                                 \n# do content extraction from files                               \ningestor = (\n    Ingestor(client=client)\n    .files(\"data/multimodal_test.pdf\")\n    .extract(             \n        extract_text=True,\n        extract_tables=True,\n        extract_charts=True,\n        extract_images=True,\n        paddle_output_format=\"markdown\",\n        extract_infographics=True,\n        # extract_method=\"nemoretriever_parse\", # Slower, but maximally accurate, especially for PDFs with pages that are scanned images\n        text_depth=\"page\"\n    ).embed()\n    .vdb_upload(\n        collection_name=\"test\",\n        sparse=False,\n        # for llama-3.2 embedder, use 1024 for e5-v5\n        dense_dim=2048\n    )\n)\nprint(\"Starting ingestion..\")\nt0 = time.time()\nresults = ingestor.ingest()\nt1 = time.time()\nprint(f\"Time taken: {t1-t0} seconds\")\n# results blob is directly inspectable\nprint(ingest_json_results_to_blob(results[0]))\n</code></pre> <p>Note</p> <p>To use library mode with nemoretriever_parse, uncomment <code>extract_method=\"nemoretriever_parse\"</code> in the previous code. For more information, refer to Use Nemo Retriever Extraction with nemoretriever-parse.</p> <pre><code>Starting ingestion..\n1 records to insert to milvus\nlogged 8 records\nTime taken: 5.479151725769043 seconds\nThis chart shows some gadgets, and some very fictitious costs. Gadgets and their cost   Chart 1 - Hammer - Powerdrill - Bluetooth speaker - Minifridge - Premium desk fan Dollars $- - $20.00 - $40.00 - $60.00 - $80.00 - $100.00 - $120.00 - $140.00 - $160.00 Cost\nTable 1\n| This table describes some animals, and some activities they might be doing in specific locations. | This table describes some animals, and some activities they might be doing in specific locations. | This table describes some animals, and some activities they might be doing in specific locations. |\n| Animal | Activity | Place |\n| Giraffe | Driving a car | At the beach |\n| Lion | Putting on sunscreen | At the park |\n| Cat | Jumping onto a laptop | In a home office |\n| Dog | Chasing a squirrel | In the front yard |\nTestingDocument\nA sample document with headings and placeholder text\nIntroduction\nThis is a placeholder document that can be used for any purpose. It contains some \nheadings and some placeholder text to fill the space. The text is not important and contains \nno real value, but it is useful for testing. Below, we will have some simple tables and charts \nthat we can use to confirm Ingest is working as expected.\nTable 1\nThis table describes some animals, and some activities they might be doing in specific \nlocations.\nAnimal Activity Place\nGira@e Driving a car At the beach\nLion Putting on sunscreen At the park\nCat Jumping onto a laptop In a home o@ice\nDog Chasing a squirrel In the front yard\nChart 1\nThis chart shows some gadgets, and some very fictitious costs.\nimage_caption:[]\nimage_caption:[]\nBelow,is a high-quality picture of some shapes          Picture\nTable 2\n| This table shows some popular colors that cars might come in | This table shows some popular colors that cars might come in | This table shows some popular colors that cars might come in | This table shows some popular colors that cars might come in |\n| Car | Color1 | Color2 | Color3 |\n| Coupe | White | Silver | Flat Gray |\n| Sedan | White | Metallic Gray | Matte Gray |\n| Minivan | Gray | Beige | Black |\n| Truck | Dark Gray | Titanium Gray | Charcoal |\n| Convertible | Light Gray | Graphite | Slate Gray |\nSection One\nThis is the first section of the document. It has some more placeholder text to show how \nthe document looks like. The text is not meant to be meaningful or informative, but rather to \ndemonstrate the layout and formatting of the document.\n\u2022 This is the first bullet point\n\u2022 This is the second bullet point\n\u2022 This is the third bullet point\nSection Two\nThis is the second section of the document. It is more of the same as we\u2019ve seen in the rest \nof the document. The content is meaningless, but the intent is to create a very simple \nsmoke test to ensure extraction is working as intended. This will be used in CI as time goes \non to ensure that changes we make to the library do not negatively impact our accuracy.\nTable 2\nThis table shows some popular colors that cars might come in.\nCar Color1 Color2 Color3\nCoupe White Silver Flat Gray\nSedan White Metallic Gray Matte Gray\nMinivan Gray Beige Black\nTruck Dark Gray Titanium Gray Charcoal\nConvertible Light Gray Graphite Slate Gray\nPicture\nBelow, is a high-quality picture of some shapes.\nimage_caption:[]\nimage_caption:[]\nThis chart shows some average frequency ranges for speaker drivers. Frequency Ranges ofSpeaker Drivers   Tweeter - Midrange - Midwoofer - Subwoofer Chart2 Hertz (log scale) 1 - 10 - 100 - 1000 - 10000 - 100000 FrequencyRange Start (Hz) - Frequency Range End (Hz)\nChart 2\nThis chart shows some average frequency ranges for speaker drivers.\nConclusion\nThis is the conclusion of the document. It has some more placeholder text, but the most \nimportant thing is that this is the conclusion. As we end this document, we should have \nbeen able to extract 2 tables, 2 charts, and some text including 3 bullet points.\nimage_caption:[]\n</code></pre>"},{"location":"extraction/quickstart-guide/#using-the-nv-ingest-cli","title":"Using the <code>nv-ingest-cli</code>","text":"<p>Tip</p> <p>There is a Jupyter notebook available to help you get started with the CLI. For more information, refer to CLI Client Quick Start Guide.</p> <pre><code>nv-ingest-cli \\\n  --doc ./data/multimodal_test.pdf \\\n  --output_directory ./processed_docs \\\n  --task='extract:{\"document_type\": \"pdf\", \"extract_method\": \"pdfium\", \"extract_tables\": \"true\", \"extract_images\": \"true\", \"extract_charts\": \"true\"}' \\\n  --client_host=localhost \\\n  --client_port=7670\n</code></pre> <p>You should notice output indicating document processing status followed by a breakdown of time spent during job execution: <pre><code>None of PyTorch, TensorFlow &gt;= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n[nltk_data] Downloading package punkt_tab to\n[nltk_data]     /raid/jdyer/miniforge3/envs/nv-ingest-\n[nltk_data]     dev/lib/python3.10/site-\n[nltk_data]     packages/llama_index/core/_static/nltk_cache...\n[nltk_data]   Package punkt_tab is already up-to-date!\nINFO:nv_ingest_client.nv_ingest_cli:Processing 1 documents.\nINFO:nv_ingest_client.nv_ingest_cli:Output will be written to: ./processed_docs\nProcessing files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:02&lt;00:00,  2.34s/file, pages_per_sec=1.28]\nINFO:nv_ingest_client.cli.util.processing:message_broker_task_source: Avg: 2.39 ms, Median: 2.39 ms, Total Time: 2.39 ms, Total % of Trace Computation: 0.06%\nINFO:nv_ingest_client.cli.util.processing:broker_source_network_in: Avg: 9.51 ms, Median: 9.51 ms, Total Time: 9.51 ms, Total % of Trace Computation: 0.25%\nINFO:nv_ingest_client.cli.util.processing:job_counter: Avg: 1.47 ms, Median: 1.47 ms, Total Time: 1.47 ms, Total % of Trace Computation: 0.04%\nINFO:nv_ingest_client.cli.util.processing:job_counter_channel_in: Avg: 0.46 ms, Median: 0.46 ms, Total Time: 0.46 ms, Total % of Trace Computation: 0.01%\nINFO:nv_ingest_client.cli.util.processing:metadata_injection: Avg: 3.52 ms, Median: 3.52 ms, Total Time: 3.52 ms, Total % of Trace Computation: 0.09%\nINFO:nv_ingest_client.cli.util.processing:metadata_injection_channel_in: Avg: 0.16 ms, Median: 0.16 ms, Total Time: 0.16 ms, Total % of Trace Computation: 0.00%\nINFO:nv_ingest_client.cli.util.processing:pdf_content_extractor: Avg: 475.64 ms, Median: 163.77 ms, Total Time: 2378.21 ms, Total % of Trace Computation: 62.73%\nINFO:nv_ingest_client.cli.util.processing:pdf_content_extractor_channel_in: Avg: 0.31 ms, Median: 0.31 ms, Total Time: 0.31 ms, Total % of Trace Computation: 0.01%\nINFO:nv_ingest_client.cli.util.processing:image_content_extractor: Avg: 0.67 ms, Median: 0.67 ms, Total Time: 0.67 ms, Total % of Trace Computation: 0.02%\nINFO:nv_ingest_client.cli.util.processing:image_content_extractor_channel_in: Avg: 0.21 ms, Median: 0.21 ms, Total Time: 0.21 ms, Total % of Trace Computation: 0.01%\nINFO:nv_ingest_client.cli.util.processing:docx_content_extractor: Avg: 0.46 ms, Median: 0.46 ms, Total Time: 0.46 ms, Total % of Trace Computation: 0.01%\nINFO:nv_ingest_client.cli.util.processing:docx_content_extractor_channel_in: Avg: 0.20 ms, Median: 0.20 ms, Total Time: 0.20 ms, Total % of Trace Computation: 0.01%\nINFO:nv_ingest_client.cli.util.processing:pptx_content_extractor: Avg: 0.68 ms, Median: 0.68 ms, Total Time: 0.68 ms, Total % of Trace Computation: 0.02%\nINFO:nv_ingest_client.cli.util.processing:pptx_content_extractor_channel_in: Avg: 0.46 ms, Median: 0.46 ms, Total Time: 0.46 ms, Total % of Trace Computation: 0.01%\nINFO:nv_ingest_client.cli.util.processing:audio_data_extraction: Avg: 1.08 ms, Median: 1.08 ms, Total Time: 1.08 ms, Total % of Trace Computation: 0.03%\nINFO:nv_ingest_client.cli.util.processing:audio_data_extraction_channel_in: Avg: 0.20 ms, Median: 0.20 ms, Total Time: 0.20 ms, Total % of Trace Computation: 0.01%\nINFO:nv_ingest_client.cli.util.processing:dedup_images: Avg: 0.42 ms, Median: 0.42 ms, Total Time: 0.42 ms, Total % of Trace Computation: 0.01%\nINFO:nv_ingest_client.cli.util.processing:dedup_images_channel_in: Avg: 0.42 ms, Median: 0.42 ms, Total Time: 0.42 ms, Total % of Trace Computation: 0.01%\nINFO:nv_ingest_client.cli.util.processing:filter_images: Avg: 0.59 ms, Median: 0.59 ms, Total Time: 0.59 ms, Total % of Trace Computation: 0.02%\nINFO:nv_ingest_client.cli.util.processing:filter_images_channel_in: Avg: 0.57 ms, Median: 0.57 ms, Total Time: 0.57 ms, Total % of Trace Computation: 0.02%\nINFO:nv_ingest_client.cli.util.processing:table_data_extraction: Avg: 240.75 ms, Median: 240.75 ms, Total Time: 481.49 ms, Total % of Trace Computation: 12.70%\nINFO:nv_ingest_client.cli.util.processing:table_data_extraction_channel_in: Avg: 0.38 ms, Median: 0.38 ms, Total Time: 0.38 ms, Total % of Trace Computation: 0.01%\nINFO:nv_ingest_client.cli.util.processing:chart_data_extraction: Avg: 300.54 ms, Median: 299.94 ms, Total Time: 901.62 ms, Total % of Trace Computation: 23.78%\nINFO:nv_ingest_client.cli.util.processing:chart_data_extraction_channel_in: Avg: 0.23 ms, Median: 0.23 ms, Total Time: 0.23 ms, Total % of Trace Computation: 0.01%\nINFO:nv_ingest_client.cli.util.processing:infographic_data_extraction: Avg: 0.77 ms, Median: 0.77 ms, Total Time: 0.77 ms, Total % of Trace Computation: 0.02%\nINFO:nv_ingest_client.cli.util.processing:infographic_data_extraction_channel_in: Avg: 0.25 ms, Median: 0.25 ms, Total Time: 0.25 ms, Total % of Trace Computation: 0.01%\nINFO:nv_ingest_client.cli.util.processing:caption_ext: Avg: 0.55 ms, Median: 0.55 ms, Total Time: 0.55 ms, Total % of Trace Computation: 0.01%\nINFO:nv_ingest_client.cli.util.processing:caption_ext_channel_in: Avg: 0.51 ms, Median: 0.51 ms, Total Time: 0.51 ms, Total % of Trace Computation: 0.01%\nINFO:nv_ingest_client.cli.util.processing:embed_text: Avg: 1.21 ms, Median: 1.21 ms, Total Time: 1.21 ms, Total % of Trace Computation: 0.03%\nINFO:nv_ingest_client.cli.util.processing:embed_text_channel_in: Avg: 0.21 ms, Median: 0.21 ms, Total Time: 0.21 ms, Total % of Trace Computation: 0.01%\nINFO:nv_ingest_client.cli.util.processing:store_embedding_minio: Avg: 0.32 ms, Median: 0.32 ms, Total Time: 0.32 ms, Total % of Trace Computation: 0.01%\nINFO:nv_ingest_client.cli.util.processing:store_embedding_minio_channel_in: Avg: 1.18 ms, Median: 1.18 ms, Total Time: 1.18 ms, Total % of Trace Computation: 0.03%\nINFO:nv_ingest_client.cli.util.processing:message_broker_task_sink_channel_in: Avg: 0.42 ms, Median: 0.42 ms, Total Time: 0.42 ms, Total % of Trace Computation: 0.01%\nINFO:nv_ingest_client.cli.util.processing:No unresolved time detected. Trace times account for the entire elapsed duration.\nINFO:nv_ingest_client.cli.util.processing:Processed 1 files in 2.34 seconds.\nINFO:nv_ingest_client.cli.util.processing:Total pages processed: 3\nINFO:nv_ingest_client.cli.util.processing:Throughput (Pages/sec): 1.28\nINFO:nv_ingest_client.cli.util.processing:Throughput (Files/sec): 0.43\n</code></pre></p>"},{"location":"extraction/quickstart-guide/#step-4-inspecting-and-consuming-results","title":"Step 4: Inspecting and Consuming Results","text":"<p>After the ingestion steps above have been completed, you should be able to find the <code>text</code> and <code>image</code> subfolders inside your processed docs folder. Each will contain JSON-formatted extracted content and metadata.</p> <p>When processing has completed, you'll have separate result files for text and image data: <pre><code>ls -R processed_docs/\n</code></pre> <pre><code>processed_docs/:\nimage  structured  text\n\nprocessed_docs/image:\nmultimodal_test.pdf.metadata.json\n\nprocessed_docs/structured:\nmultimodal_test.pdf.metadata.json\n\nprocessed_docs/text:\nmultimodal_test.pdf.metadata.json\n</code></pre></p> <p>For the full metadata definitions, refer to Content Metadata. </p> <p>We also provide a script for inspecting extracted images.</p> <p>First, install <code>tkinter</code> by running the following code. Choose the code for your OS.</p> <ul> <li> <p>For Ubuntu/Debian Linux:</p> <pre><code>sudo apt-get update\nsudo apt-get install python3-tk\n</code></pre> </li> <li> <p>For Fedora/RHEL Linux:</p> <pre><code>sudo dnf install python3-tkinter\n</code></pre> </li> <li> <p>For macOS using Homebrew:</p> <pre><code>brew install python-tk\n</code></pre> </li> </ul> <p>Then, run the following command to execute the script for inspecting the extracted image:</p> <pre><code>python src/util/image_viewer.py --file_path ./processed_docs/image/multimodal_test.pdf.metadata.json\n</code></pre> <p>Tip</p> <p>Beyond inspecting the results, you can read them into things like llama-index or langchain retrieval pipelines. Also, checkout our demo using a retrieval pipeline on build.nvidia.com to query over document content pre-extracted with NV-Ingest.</p>"},{"location":"extraction/quickstart-guide/#profile-information","title":"Profile Information","text":"<p>The values that you specify in the <code>--profile</code> option of your <code>docker compose up</code> command are explained in the following table.  You can specify multiple <code>--profile</code> options.</p> Profile Type Description <code>retrieval</code> Core Enables the embedding NIM and (GPU accelerated) Milvus. <code>table-structure</code> Core Enables the yolox table structure NIM which enhances markdown formatting of extracted table content. This benefits answer generation by downstream LLMs. <code>audio</code> Advanced Use Riva for processing audio files. For more information, refer to Audio Processing. <code>nemoretriever-parse</code> Advanced Use nemoretriever-parse, which adds state-of-the-art text and table extraction. For more information, refer to Use Nemo Retriever Extraction with nemoretriever-parse. <code>vlm</code> Advanced Use llama 3.2 11B Vision for experimental image captioning of unstructured images. <p>Important</p> <p>Advanced features require additional GPU support and disk space. For more information, refer to Support Matrix.</p>"},{"location":"extraction/quickstart-guide/#related-topics","title":"Related Topics","text":"<ul> <li>Troubleshoot</li> <li>Prerequisites</li> <li>Support Matrix</li> <li>Deploy Without Containers (Library Mode)</li> <li>Deploy With Helm</li> <li>Notebooks</li> <li>Multimodal PDF Data Extraction</li> </ul>"},{"location":"extraction/quickstart-library-mode/","title":"Deploy Without Containers (Library Mode) for NeMo Retriever Extraction","text":"<p>For small-scale workloads, such as workloads of fewer than 100 documents, you can use library mode setup.  Library mode depends on NIMs that are already self-hosted, or, by default, NIMs that are hosted on build.nvidia.com.</p> <p>To get started using NeMo Retriever extraction in library mode, you need the following:</p> <ul> <li>Linux operating systems (Ubuntu 22.04 or later recommended)</li> <li>Python 3.12</li> <li>We strongly advise using an isolated Python virtual env, such as provided by uv or conda</li> </ul>"},{"location":"extraction/quickstart-library-mode/#step-1-prepare-your-environment","title":"Step 1: Prepare Your Environment","text":"<p>Use the following procedure to prepare your environment.</p> <ol> <li> <p>Run the following code to create your NV Ingest Conda environment.</p> <pre><code>   uv venv --python 3.12 nvingest &amp;&amp; \\\n     source nvingest/bin/activate &amp;&amp; \\\n     uv pip install nv-ingest==25.6.2 nv-ingest-api==25.6.2 nv-ingest-client==25.6.3\n</code></pre> <p>Tip</p> <p>To confirm that you have activated your Conda environment, run <code>which python</code> and confirm that you see <code>nvingest</code> in the result. You can do this before any python command that you run.</p> </li> <li> <p>Set or create a .env file that contains your NVIDIA Build API key and other environment variables.</p> <p>Note</p> <p>If you use an NGC personal key, then you should provide the same value for all keys, but you must specify each environment variable individually. In the past, you could create an API key. If you have an API key, you can still use that. For more information, refer to Generate Your NGC Keys and Environment Configuration Variables.</p> <ul> <li> <p>To set your variables, use the following code.</p> <p><pre><code>export NVIDIA_BUILD_API_KEY=nvapi-&lt;your key&gt;\nexport NVIDIA_API_KEY=nvapi-&lt;your key&gt;\n</code></pre>     - To add your variables to an .env file, include the following.</p> <pre><code>NVIDIA_BUILD_API_KEY=nvapi-&lt;your key&gt;\nNVIDIA_API_KEY=nvapi-&lt;your key&gt;    \n</code></pre> </li> </ul> </li> </ol>"},{"location":"extraction/quickstart-library-mode/#step-2-ingest-documents","title":"Step 2: Ingest Documents","text":"<p>You can submit jobs programmatically by using Python.</p> <p>Tip</p> <p>For more Python examples, refer to NV-Ingest: Python Client Quick Start Guide.</p> <p>If you have a very high number of CPUs, and see the process hang without progress,  we recommend that you use <code>taskset</code> to limit the number of CPUs visible to the process.  Use the following code.</p> <pre><code>taskset -c 0-3 python your_ingestion_script.py\n</code></pre> <p>On a 4 CPU core low end laptop, the following code should take about 10 seconds.</p> <pre><code>import logging, os, time, sys\n\nfrom nv_ingest.framework.orchestration.ray.util.pipeline.pipeline_runners import run_pipeline\nfrom nv_ingest.framework.orchestration.ray.util.pipeline.pipeline_runners import PipelineCreationSchema\nfrom nv_ingest_api.util.logging.configuration import configure_logging as configure_local_logging\nfrom nv_ingest_client.client import Ingestor, NvIngestClient\nfrom nv_ingest_api.util.message_brokers.simple_message_broker import SimpleClient\nfrom nv_ingest_client.util.process_json_files import ingest_json_results_to_blob\n\n# Start the pipeline subprocess for library mode\nconfig = PipelineCreationSchema()\n\nrun_pipeline(config, block=False, disable_dynamic_scaling=True, run_in_subprocess=True)\n\nclient = NvIngestClient(\n    message_client_allocator=SimpleClient,\n    message_client_port=7671,\n    message_client_hostname=\"localhost\"\n)\n\n# gpu_cagra accelerated indexing is not available in milvus-lite\n# Provide a filename for milvus_uri to use milvus-lite\nmilvus_uri = \"milvus.db\"\ncollection_name = \"test\"\nsparse = False\n\n# do content extraction from files                                \ningestor = (\n    Ingestor(client=client)\n    .files(\"data/multimodal_test.pdf\")\n    .extract(\n        extract_text=True,\n        extract_tables=True,\n        extract_charts=True,\n        extract_images=True,\n        paddle_output_format=\"markdown\",\n        extract_infographics=True,\n        # Slower, but maximally accurate, especially for PDFs with pages that are scanned images\n        # extract_method=\"nemoretriever_parse\",\n        text_depth=\"page\"\n    ).embed()\n    .vdb_upload(\n        collection_name=collection_name,\n        milvus_uri=milvus_uri,\n        sparse=sparse,\n        # for llama-3.2 embedder, use 1024 for e5-v5\n        dense_dim=2048\n    )\n)\n\nprint(\"Starting ingestion..\")\nt0 = time.time()\nresults = ingestor.ingest(show_progress=True)\nt1 = time.time()\nprint(f\"Time taken: {t1 - t0} seconds\")\n\n# results blob is directly inspectable\nprint(ingest_json_results_to_blob(results[0]))\n</code></pre> <p>Note</p> <p>To use library mode with nemoretriever_parse, uncomment <code>extract_method=\"nemoretriever_parse\"</code> in the previous code. For more information, refer to Use Nemo Retriever Extraction with nemoretriever-parse.</p> <p>You can see the extracted text that represents the content of the ingested test document.</p> <pre><code>Starting ingestion..\nTime taken: 9.243880033493042 seconds\n\nTestingDocument\nA sample document with headings and placeholder text\nIntroduction\nThis is a placeholder document that can be used for any purpose. It contains some \nheadings and some placeholder text to fill the space. The text is not important and contains \nno real value, but it is useful for testing. Below, we will have some simple tables and charts \nthat we can use to confirm Ingest is working as expected.\nTable 1\nThis table describes some animals, and some activities they might be doing in specific \nlocations.\nAnimal Activity Place\nGira@e Driving a car At the beach\nLion Putting on sunscreen At the park\nCat Jumping onto a laptop In a home o@ice\nDog Chasing a squirrel In the front yard\nChart 1\nThis chart shows some gadgets, and some very fictitious costs.\n\n... document extract continues ...\n</code></pre>"},{"location":"extraction/quickstart-library-mode/#step-3-query-ingested-content","title":"Step 3: Query Ingested Content","text":"<p>To query for relevant snippets of the ingested content, and use them with an LLM to generate answers, use the following code.</p> <pre><code>from openai import OpenAI\nfrom nv_ingest_client.util.milvus import nvingest_retrieval\nimport os\n\nmilvus_uri = \"milvus.db\"\ncollection_name = \"test\"\nsparse=False\n\nqueries = [\"Which animal is responsible for the typos?\"]\n\nretrieved_docs = nvingest_retrieval(\n    queries,\n    collection_name,\n    milvus_uri=milvus_uri,\n    hybrid=sparse,\n    top_k=1,\n)\n\n# simple generation example\nextract = retrieved_docs[0][0][\"entity\"][\"text\"]\nclient = OpenAI(\n  base_url = \"https://integrate.api.nvidia.com/v1\",\n  api_key = os.environ[\"NVIDIA_BUILD_API_KEY\"]\n)\n\nprompt = f\"Using the following content: {extract}\\n\\n Answer the user query: {queries[0]}\"\nprint(f\"Prompt: {prompt}\")\ncompletion = client.chat.completions.create(\n  model=\"nvidia/llama-3.1-nemotron-70b-instruct\",\n  messages=[{\"role\":\"user\",\"content\": prompt}],\n)\nresponse = completion.choices[0].message.content\n\nprint(f\"Answer: {response}\")\n</code></pre> <pre><code>Prompt: Using the following content: TestingDocument\nA sample document with headings and placeholder text\nIntroduction\nThis is a placeholder document that can be used for any purpose. It contains some \nheadings and some placeholder text to fill the space. The text is not important and contains \nno real value, but it is useful for testing. Below, we will have some simple tables and charts \nthat we can use to confirm Ingest is working as expected.\nTable 1\nThis table describes some animals, and some activities they might be doing in specific \nlocations.\nAnimal Activity Place\nGira@e Driving a car At the beach\nLion Putting on sunscreen At the park\nCat Jumping onto a laptop In a home o@ice\nDog Chasing a squirrel In the front yard\nChart 1\nThis chart shows some gadgets, and some very fictitious costs.\n\n Answer the user query: Which animal is responsible for the typos?\nAnswer: A clever query!\n\nAfter carefully examining the provided content, I'd like to point out the potential \"typos\" (assuming you're referring to the unusual or intentionally incorrect text) and attempt to playfully \"assign blame\" to an animal based on the context:\n\n1. **Gira@e** (instead of Giraffe) - **Animal blamed: Giraffe** (Table 1, first row)\n    * The \"@\" symbol in \"Gira@e\" suggests a possible typo or placeholder character, which we'll humorously attribute to the Giraffe's alleged carelessness.\n2. **o@ice** (instead of Office) - **Animal blamed: Cat**\n    * The same \"@\" symbol appears in \"o@ice\", which is related to the Cat's activity in the same table. Perhaps the Cat was in a hurry while typing and introduced the error?\n\nSo, according to this whimsical analysis, both the **Giraffe** and the **Cat** are \"responsible\" for the typos, with the Giraffe possibly being the more egregious offender given the more blatant character substitution in its name.\n</code></pre>"},{"location":"extraction/quickstart-library-mode/#related-topics","title":"Related Topics","text":"<ul> <li>Prerequisites</li> <li>Support Matrix</li> <li>Deploy With Docker Compose (Self-Hosted)</li> <li>Deploy With Helm</li> <li>Notebooks</li> <li>Multimodal PDF Data Extraction</li> </ul>"},{"location":"extraction/releasenotes-nv-ingest/","title":"Release Notes for NeMo Retriever Extraction","text":"<p>This documentation contains the release notes for NeMo Retriever extraction.</p> <p>Note</p> <p>NeMo Retriever extraction is also known as NVIDIA Ingest and nv-ingest.</p>"},{"location":"extraction/releasenotes-nv-ingest/#release-2563","title":"Release 25.6.3","text":"<p>The NeMo Retriever extraction 25.6.3 release is a patch release  that updates the client's dependency on pypdfium2 to the latest stable version.</p> <p>Only the release branch and the <code>nv-ingest-client</code> package have been updated in 25.6.3.  The previously released 25.6.2 container on NGC remains unchanged.</p>"},{"location":"extraction/releasenotes-nv-ingest/#release-2562","title":"Release 25.6.2","text":"<p>The NeMo Retriever extraction 25.06 release focuses on accuracy improvements and feature expansions, including the following:</p> <ul> <li>Improve reranker accuracy.</li> <li>Upgrade Python version from 3.10 to 3.12</li> <li>Helm deployment now has similar throughput performance to docker deployment.</li> <li>Add support for the latest version of the OpenAI API.</li> <li>Add MIG support. For details, see Enable NVIDIA GPU MIG.</li> <li>Add time slicing support. For details, see Enable GPU time-slicing.</li> <li>Add support for RIVA NIM for optional audio extraction. For details, see helm/values.yaml.</li> <li>New notebook for How to add metadata to your documents and filter searches.</li> </ul>"},{"location":"extraction/releasenotes-nv-ingest/#breaking-changes","title":"Breaking Changes","text":"<p>There are no breaking changes in this version.</p>"},{"location":"extraction/releasenotes-nv-ingest/#upgrade","title":"Upgrade","text":"<p>To upgrade the Helm Charts for this version, refer to NV-Ingest Helm Charts.</p>"},{"location":"extraction/releasenotes-nv-ingest/#release-2542","title":"Release 25.4.2","text":"<p>The NeMo Retriever extraction 25.04 release focuses on small bug fixes and improvements, including the following:</p> <ul> <li>Fixed a known issue where large text file ingestion failed.</li> <li>The REST service is now more resilient, and recovers from worker failures and connection errors.</li> <li>Various improvements on the client side to reduce retry rates, and improve overall quality of life.</li> <li>New notebook for How to reindex a collection.</li> <li>Expanded chunking documentation. For more information, refer to Split Documents.</li> </ul>"},{"location":"extraction/releasenotes-nv-ingest/#breaking-changes_1","title":"Breaking Changes","text":"<p>There are no breaking changes in this version.</p>"},{"location":"extraction/releasenotes-nv-ingest/#upgrade_1","title":"Upgrade","text":"<p>To upgrade the Helm Charts for this version, refer to NV-Ingest Helm Charts.</p>"},{"location":"extraction/releasenotes-nv-ingest/#release-2530","title":"Release 25.3.0","text":"<p>The NeMo Retriever extraction 25.03 release includes accuracy improvements, feature expansions, and throughput improvements.</p>"},{"location":"extraction/releasenotes-nv-ingest/#new-features","title":"New Features","text":"<ul> <li>Consolidated NeMo Retriever extraction to run on a single GPU (H100, A100, L40S, or A10G). For details, refer to Support Matrix.</li> <li>Added Library Mode for a lightweight no-GPU deployment that uses NIM endpoints hosted on build.nvidia.com. For details, refer to Deploy Without Containers (Library Mode).</li> <li>Added support for infographics extraction.</li> <li>Added support for RIVA NIM for Audio extraction (Early Access). For details, refer to Audio Processing.</li> <li>Added support for Llama-3.2 VLM for Image Captioning capability.</li> <li>docX, pptx, jpg, png support for image detection &amp; extraction.</li> <li>Deprecated DePlot and CACHED NIMs.</li> </ul>"},{"location":"extraction/releasenotes-nv-ingest/#release-24121","title":"Release 24.12.1","text":"<p>Cases where .split() tasks fail during ingestion are now fixed.</p>"},{"location":"extraction/releasenotes-nv-ingest/#release-2412","title":"Release 24.12","text":"<p>We currently do not support OCR-based text extraction. This was discovered in an unsupported use case and is not a product functionality issue.</p>"},{"location":"extraction/support-matrix/","title":"Support Matrix for NeMo Retriever Extraction","text":"<p>Before you begin using NeMo Retriever extraction, ensure that you have the hardware for your use case.</p>"},{"location":"extraction/support-matrix/#core-and-advanced-pipeline-features","title":"Core and Advanced Pipeline Features","text":"<p>The Nemo Retriever extraction core pipeline features run on a single A10G or better GPU.  The core pipeline features include the following:</p> <ul> <li>llama3.2-nv-embedqa-1b-v2 \u2014 Embedding model for converting text chunks into vectors.</li> <li>nemoretriever-page-elements-v2 \u2014 Detects and classifies images on a page as a table, chart or infographic. </li> <li>nemoretriever-table-structure-v1 \u2014 Detects rows, columns, and cells within a table to preserve table structure and convert to Markdown format. </li> <li>nemoretriever-graphic-elements-v1 \u2014 Detects graphic elements within chart images such as titles, legends, axes, and numerical values. </li> <li>paddleocr \u2014 Image OCR model to detect and extract text from images.</li> <li>retrieval \u2014 Enables embedding and indexing into Milvus.</li> </ul> <p>Advanced features require additional GPU support and disk space.  This includes the following:</p> <ul> <li>Audio extraction \u2014 Use Riva for processing audio files. For more information, refer to Audio Processing.</li> <li><code>nemoretriever-parse</code> \u2014 Use nemoretriever-parse, which adds state-of-the-art text and table extraction. For more information, refer to Use Nemo Retriever Extraction with nemoretriever-parse.</li> <li>VLM image captioning \u2014 Use llama 3.2 11B Vision for experimental image captioning of unstructured images.</li> </ul>"},{"location":"extraction/support-matrix/#hardware-requirements","title":"Hardware Requirements","text":"<p>The following are the hardware requirements to run NeMo Retriever Extraction.</p> GPU Option H100 A100 A10G L40S Family SXM or PCIe SXM or PCIe \u2014 \u2014 Memory 80GB 80GB 24GB 48GB Core Features Total GPUs 1 1 1 1 Core Features Total Disk Space ~150GB ~150GB ~150GB ~150GB Audio Additional Dedicated GPUs 1 1 1 1 Audio Additional Disk Space ~37GB ~37GB ~37GB ~37GB nemoretriever-parse Additional Dedicated GPUs 1 1 1 1 nemoretriever-parse Additional Disk Space ~16GB ~16GB ~16GB ~16GB VLM Additional Dedicated GPUs 1 1 1 1 VLM Additional Disk Space ~16GB ~16GB ~16GB ~16GB"},{"location":"extraction/support-matrix/#related-topics","title":"Related Topics","text":"<ul> <li>Prerequisites</li> <li>Deploy Without Containers (Library Mode)</li> <li>Deploy With Docker Compose (Self-Hosted)</li> <li>Deploy With Helm</li> </ul>"},{"location":"extraction/telemetry/","title":"Telemetry with NeMo Retriever Extraction","text":"<p>You can view telemetry data for NeMo Retriever extraction.</p> <p>Note</p> <p>NeMo Retriever extraction is also known as NVIDIA Ingest and nv-ingest.</p>"},{"location":"extraction/telemetry/#opentelemetry","title":"OpenTelemetry","text":"<p>After OpenTelemetry and Zipkin are running, you can open your browser to explore traces: </p> <ul> <li>Docker \u2014 Use http://$YOUR_DOCKER_HOST:9411/zipkin/ </li> <li>Kubernetes \u2014 Use http://$YOUR_K8S_OTEL_POD:9411/zipkin/</li> </ul> <p></p>"},{"location":"extraction/telemetry/#prometheus","title":"Prometheus","text":"<p>After Prometheus is running, you can open your browser to explore metrics: </p> <ul> <li>Docker \u2014 Use http://$YOUR_DOCKER_HOST:9090/zipkin/</li> <li>Kubernetes \u2014 Use http://$YOUR_K8S_OTEL_POD:9090/zipkin/</li> </ul> <p></p>"},{"location":"extraction/troubleshoot/","title":"Troubleshoot NeMo Retriever Extraction","text":"<p>Use this documentation to troubleshoot issues that arise when you use NeMo Retriever extraction.</p>"},{"location":"extraction/troubleshoot/#cant-process-malformed-input-files","title":"Can't process malformed input files","text":"<p>When you run a job you might see errors similar to the following:</p> <ul> <li>Failed to process the message</li> <li>Failed to extract image</li> <li>File may be malformed</li> <li>Failed to format paragraph</li> </ul> <p>These errors can occur when your input file is malformed.  Verify or fix the format of your input file, and try resubmitting your job.</p>"},{"location":"extraction/troubleshoot/#cant-start-new-thread-error","title":"Can't start new thread error","text":"<p>In rare cases, when you run a job you might an see an error similar to <code>can't start new thread</code>.  This error occurs when the maximum number of processes available to a single user is too low. To resolve the issue, set or raise the maximum number of processes (<code>-u</code>) by using the ulimit command. Before you change the <code>-u</code> setting, consider the following:</p> <ul> <li>Apply the <code>-u</code> setting directly to the user (or the Docker container environment) that runs your ingest service.</li> <li>For <code>-u</code> we recommend 10,000 as a baseline, but you might need to raise or lower it based on your actual usage and system configuration.</li> </ul> <pre><code>ulimit -u 10,000\n</code></pre>"},{"location":"extraction/troubleshoot/#extract-method-nemoretriever-parse-doesnt-support-image-files","title":"Extract method nemoretriever-parse doesn't support image files","text":"<p>Currently, extraction with nemoretriever-parse doesn't support image files, only scanned PDFs.  To work around this issue, convert image files to PDFs before you use <code>extract_method=\"nemoretriever_parse\"</code>.</p>"},{"location":"extraction/troubleshoot/#too-many-open-files-error","title":"Too many open files error","text":"<p>In rare cases, when you run a job you might an see an error similar to <code>too many open files</code> or <code>max open file descriptor</code>.  This error occurs when the open file descriptor limit for your service user account is too low. To resolve the issue, set or raise the maximum number of open file descriptors (<code>-n</code>) by using the ulimit command. Before you change the <code>-n</code> setting, consider the following:</p> <ul> <li>Apply the <code>-n</code> setting directly to the user (or the Docker container environment) that runs your ingest service.</li> <li>For <code>-n</code> we recommend 10,000 as a baseline, but you might need to raise or lower it based on your actual usage and system configuration.</li> </ul> <pre><code>ulimit -n 10,000\n</code></pre>"},{"location":"extraction/troubleshoot/#triton-server-info-messages-incorrectly-logged-as-errors","title":"Triton server INFO messages incorrectly logged as errors","text":"<p>Sometimes messages are incorrectly logged as errors, when they are information.  When this happens, you can ignore the errors, and treat the messages as information.  For example, you might see log messages that look similar to the following.</p> <pre><code>ERROR 2025-04-24 22:49:44.266 nimutils.py:68] tritonserver: /usr/local/lib/libcurl.so.4: ...\nERROR 2025-04-24 22:49:44.268 nimutils.py:68] I0424 22:49:44.265292 98 cache_manager.cc:480] \"Create CacheManager with cache_dir: '/opt/tritonserver/caches'\"\nERROR 2025-04-24 22:49:44.431 nimutils.py:68] I0424 22:49:44.431796 98 pinned_memory_manager.cc:277] \"Pinned memory pool is created at '0x7f8e4a000000' with size 268435456\"\nERROR 2025-04-24 22:49:44.432 nimutils.py:68] I0424 22:49:44.432036 98 cuda_memory_manager.cc:107] \"CUDA memory pool is created on device 0 with size 67108864\"\nERROR 2025-04-24 22:49:44.433 nimutils.py:68] I0424 22:49:44.433448 98 model_config_utils.cc:753] \"Server side auto-completed config: \"\nERROR 2025-04-24 22:49:44.433 nimutils.py:68] name: \"yolox\"\nERROR 2025-04-24 22:49:44.433 nimutils.py:68] platform: \"tensorrt_plan\"\nERROR 2025-04-24 22:49:44.433 nimutils.py:68] max_batch_size: 32\nERROR 2025-04-24 22:49:44.433 nimutils.py:68] input {\nERROR 2025-04-24 22:49:44.433 nimutils.py:68] name: \"input\"\nERROR 2025-04-24 22:49:44.433 nimutils.py:68] data_type: TYPE_FP32\nERROR 2025-04-24 22:49:44.433 nimutils.py:68] dims: 3\nERROR 2025-04-24 22:49:44.433 nimutils.py:68] dims: 1024\nERROR 2025-04-24 22:49:44.433 nimutils.py:68] dims: 1024\nERROR 2025-04-24 22:49:44.433 nimutils.py:68] }\nERROR 2025-04-24 22:49:44.433 nimutils.py:68] output {\nERROR 2025-04-24 22:49:44.434 nimutils.py:68] name: \"output\"\nERROR 2025-04-24 22:49:44.434 nimutils.py:68] data_type: TYPE_FP32\nERROR 2025-04-24 22:49:44.434 nimutils.py:68] dims: 21504\nERROR 2025-04-24 22:49:44.434 nimutils.py:68] dims: 9\nERROR 2025-04-24 22:49:44.434 nimutils.py:68] }\n...\n</code></pre>"},{"location":"extraction/troubleshoot/#related-topics","title":"Related Topics","text":"<ul> <li>Support Matrix</li> <li>Prerequisites</li> <li>Deploy Without Containers (Library Mode)</li> <li>Deploy With Docker Compose (Self-Hosted)</li> <li>Deploy With Helm</li> </ul>"}]}