# SPDX-FileCopyrightText: Copyright (c) 2024, NVIDIA CORPORATION & AFFILIATES.
# All rights reserved.
# SPDX-License-Identifier: Apache-2.0

services:
  redis:
    image: "redis/redis-stack"
    ports:
      - "6379:6379"

  yolox:
    image: nvcr.io/ohlfw0olaadg/ea-participants/nv-yolox-structured-images-v1:0.1.0
    ports:
      - "8000:8000"
      - "8001:8001"
      - "8002:8002"
    volumes:
      - ${HOME}/.cache:/home/nvs/.cache
    user: root
    environment:
      - NIM_HTTP_API_PORT=8000
      - NIM_TRITON_LOG_VERBOSE=1
      - NGC_API_KEY=${NIM_NGC_API_KEY:-${NGC_API_KEY:-ngcapikey}}
      - CUDA_VISIBLE_DEVICES=1
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    runtime: nvidia

  deplot:
    image: nvcr.io/ohlfw0olaadg/ea-participants/deplot:1.0.0
    ports:
      - "8003:8000"
      - "8004:8001"
      - "8005:8002"
    volumes:
      - ${HOME}/.cache:/opt/nim/.cache
    user: root
    environment:
      - NIM_HTTP_API_PORT=8000
      - NIM_TRITON_LOG_VERBOSE=1
      - NGC_API_KEY=${NIM_NGC_API_KEY:-${NGC_API_KEY:-ngcapikey}}
      - CUDA_VISIBLE_DEVICES=0
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    runtime: nvidia

  cached:
    image: nvcr.io/ohlfw0olaadg/ea-participants/cached:0.1.0
    shm_size: 2gb
    ports:
      - "8006:8000"
      - "8007:8001"
      - "8008:8002"
    volumes:
      - ${HOME}/.cache:/home/nvs/.cache
    user: root
    environment:
      - NIM_HTTP_API_PORT=8000
      - NIM_TRITON_LOG_VERBOSE=1
      - NGC_API_KEY=${NIM_NGC_API_KEY:-${NGC_API_KEY:-ngcapikey}}
      - CUDA_VISIBLE_DEVICES=1
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    runtime: nvidia

  paddle:
    image: nvcr.io/ohlfw0olaadg/ea-participants/paddleocr:0.1.0
    shm_size: 2gb
    ports:
      - "8009:8000"
      - "8010:8001"
      - "8011:8002"
    volumes:
      - ${HOME}/.cache:/home/nvs/.cache
    user: root
    environment:
      - NIM_HTTP_API_PORT=8000
      - NIM_TRITON_LOG_VERBOSE=1
      - NGC_API_KEY=${NIM_NGC_API_KEY:-${NGC_API_KEY:-ngcapikey}}
      - CUDA_VISIBLE_DEVICES=1
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    runtime: nvidia

  embedding:
    # NIM ON
    image: nvcr.io/nim/nvidia/nv-embedqa-e5-v5:1.0.1
    shm_size: 16gb
    ports:
      - "8012:8000"
      - "8013:8001"
      - "8014:8002"
    environment:
      - NIM_HTTP_API_PORT=8000
      - NIM_TRITON_LOG_VERBOSE=1
      - NGC_API_KEY=${NIM_NGC_API_KEY:-${NGC_API_KEY:-ngcapikey}}
      - CUDA_VISIBLE_DEVICES=1
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    runtime: nvidia

  nv-ingest-ms-runtime:
    image: nvcr.io/ohlfw0olaadg/ea-participants/nv-ingest:24.08
    build:
      context: ${NV_INGEST_ROOT}
      dockerfile: "./Dockerfile"
      target: runtime
    volumes:
      - ${DATASET_ROOT}:/workspace/data
    ports:
      - "7670:7670"
    cap_add:
      - sys_nice
    environment:
      - CACHED_GRPC_ENDPOINT=cached:8001
      - CACHED_HEALTH_ENDPOINT=cached:8000
      - CACHED_HTTP_ENDPOINT=""
      - CUDA_VISIBLE_DEVICES=1
      - DEPLOT_GRPC_ENDPOINT=""
      # self hosted deplot
      - DEPLOT_HEALTH_ENDPOINT=deplot:8000
      - DEPLOT_HTTP_ENDPOINT=http://deplot:8000/v1/chat/completions
      # build.nvidia.com hosted deplot
      #- DEPLOT_HTTP_ENDPOINT=https://ai.api.nvidia.com/v1/vlm/google/deplot
      - DOUGHNUT_GRPC_TRITON=triton-doughnut:8001
      - INGEST_LOG_LEVEL=DEFAULT
      - MESSAGE_CLIENT_HOST=redis
      - MESSAGE_CLIENT_PORT=6379
      - MINIO_BUCKET=${MINIO_BUCKET:-nv-ingest}
      - MRC_IGNORE_NUMA_CHECK=1
      - NGC_API_KEY=${NGC_API_KEY:-ngcapikey}
      - NVIDIA_BUILD_API_KEY=${NVIDIA_BUILD_API_KEY:-${NGC_API_KEY:-ngcapikey}}
      - OTEL_EXPORTER_OTLP_ENDPOINT=otel-collector:4317
      - PADDLE_GRPC_ENDPOINT=paddle:8001
      - PADDLE_HEALTH_ENDPOINT=paddle:8000
      - PADDLE_HTTP_ENDPOINT=""
      - READY_CHECK_ALL_COMPONENTS=True
      - REDIS_MORPHEUS_TASK_QUEUE=morpheus_task_queue
      - TABLE_DETECTION_GRPC_TRITON=yolox:8001
      - TABLE_DETECTION_HTTP_TRITON=""
      - YOLOX_GRPC_ENDPOINT=yolox:8001
      - YOLOX_HEALTH_ENDPOINT=yolox:8000
      - YOLOX_HTTP_ENDPOINT=""
    healthcheck:
      test: curl --fail http://nv-ingest-ms-runtime:7670/v1/health/ready || exit 1
      interval: 10s
      timeout: 5s
      retries: 5
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]


  otel-collector:
    image: otel/opentelemetry-collector-contrib:0.91.0
    hostname: otel-collector
    command: ["--config=/etc/otel-collector-config.yaml"]
    volumes:
      - ./config/otel-collector-config.yaml:/etc/otel-collector-config.yaml
    ports:
      - "8888:8888" # Prometheus metrics exposed by the collector
      - "8889:8889" # Prometheus exporter metrics
      - "13133:13133" # health_check extension
      - "9411" # Zipkin receiver
      - "4317:4317" # OTLP gRPC receiver
      - "4318:4318" # OTLP/HTTP receiver
      - "55680:55679" # zpages extension
    depends_on:
      - zipkin

  zipkin:
    image: openzipkin/zipkin
    environment:
      JAVA_OPTS: "-Xms2g -Xmx2g -XX:+ExitOnOutOfMemoryError"
    ports:
      - "9411:9411" # Zipkin UI and API

  prometheus:
    image: prom/prometheus:latest
    command:
      - --web.console.templates=/etc/prometheus/consoles
      - --web.console.libraries=/etc/prometheus/console_libraries
      - --storage.tsdb.retention.time=1h
      - --config.file=/etc/prometheus/prometheus-config.yaml
      - --storage.tsdb.path=/prometheus
      - --web.enable-lifecycle
      - --web.route-prefix=/
      - --enable-feature=exemplar-storage
      - --enable-feature=otlp-write-receiver
    volumes:
      - ./config/prometheus.yaml:/etc/prometheus/prometheus-config.yaml
    ports:
      - "9090:9090"

  grafana:
    container_name: grafana-service
    image: grafana/grafana
    ports:
      - "3000:3000"

#  etcd:
#    # Turn on to leverage the `vdb_upload` task
#    restart: always
#    container_name: milvus-etcd
#    image: quay.io/coreos/etcd:v3.5.5
#    environment:
#      - ETCD_AUTO_COMPACTION_MODE=revision
#      - ETCD_AUTO_COMPACTION_RETENTION=1000
#      - ETCD_QUOTA_BACKEND_BYTES=4294967296
#      - ETCD_SNAPSHOT_COUNT=50000
#    volumes:
#      - ./.volumes/etcd:/etcd
#    command: etcd -advertise-client-urls=http://127.0.0.1:2379 -listen-client-urls http://0.0.0.0:2379 --data-dir /etcd
#    healthcheck:
#      test: ["CMD", "etcdctl", "endpoint", "health"]
#      interval: 30s
#      timeout: 20s
#      retries: 3

#  minio:
#    # Turn on to leverage the `store` and `vdb_upload` task
#    restart: always
#    container_name: minio
#    hostname: minio
#    image: minio/minio:RELEASE.2023-03-20T20-16-18Z
#    environment:
#      MINIO_ACCESS_KEY: ${MINIO_ACCESS_KEY:-minioadmin}
#      MINIO_SECRET_KEY: ${MINIO_SECRET_KEY:-minioadmin}
#    ports:
#      - "9001:9001"
#      - "9000:9000"
#    volumes:
#      - ./.volumes/minio:/minio_data
#    command: minio server /minio_data --console-address ":9001"
#    healthcheck:
#      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
#      interval: 30s
#      timeout: 20s
#      retries: 3

#  milvus:
#    # Turn on to leverage the `vdb_upload` task
#    restart: always
#    container_name: milvus-standalone
#    image: milvusdb/milvus:v2.3.5
#    command: ["milvus", "run", "standalone"]
#    hostname: milvus
#    security_opt:
#      - seccomp:unconfined
#    environment:
#      ETCD_ENDPOINTS: etcd:2379
#      MINIO_ADDRESS: minio:9000
#    volumes:
#      - ./.volumes/milvus:/var/lib/milvus
#    healthcheck:
#      test: ["CMD", "curl", "-f", "http://localhost:9091/healthz"]
#      interval: 30s
#      start_period: 90s
#      timeout: 20s
#      retries: 3
#    ports:
#      - "19530:19530"
#      - "9091:9091"
#    depends_on:
#      - "etcd"
#      - "minio"

#  attu:
#    # Turn on to leverage the `vdb_upload` task
#    restart: always
#    container_name: milvus-attu
#    image: zilliz/attu:v2.3.5
#    hostname: attu
#    environment:
#      MILVUS_URL: http://milvus:19530
#    ports:
#      - "3001:3000"
#    depends_on:
#      - "milvus"

