{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"What is NVIDIA NeMo Retriever?","text":"<p>NVIDIA NeMo Retriever is a collection of microservices  for building and scaling multimodal data extraction, embedding, and reranking pipelines  with high accuracy and maximum data privacy \u2013 built with NVIDIA NIM.  NeMo Retriever, part of the NVIDIA NeMo software suite for managing the AI agent lifecycle,  ensures data privacy and seamlessly connects to proprietary data wherever it resides,  empowering secure, enterprise-grade retrieval.</p> <p>NeMo Retriever provides the following:</p> <ul> <li>Multimodal Data Extraction \u2014 Quickly extract documents at scale that include text, tables, charts, and infographics.</li> <li>Embedding + Indexing \u2014 Embed all extracted text from text chunks and images, and then insert into Milvus - accelerated with NVIDIA cuVS.</li> <li>Retrieval \u2014 Leverage semantic + hybrid search for high accuracy retrieval with the embedding + reranking NIM microservice.</li> </ul> <p></p>"},{"location":"#enterprise-ready-features","title":"Enterprise-Ready Features","text":"<p>NVIDIA NeMo Retriever comes with enterprise-ready features, including the following:</p> <ul> <li>High Accuracy \u2014 NeMo Retriever exhibits a high level of accuracy when retrieving across various modalities through enterprise documents. </li> <li>High Throughput \u2014 NeMo Retriever is capable of extracting, embedding, indexing and retrieving across hundreds of thousands of documents at scale with high throughput. </li> <li>Decomposable/Customizable \u2014 NeMo Retriever consists of modules that can be separately used and deployed in your own environment. </li> <li>Enterprise-Grade Security \u2014 NeMo Retriever NIMs come with security features such as the use of safetensors, continuous patching of CVEs, and more. </li> </ul>"},{"location":"#applications","title":"Applications","text":"<p>The following are some applications that use NVIDIA Nemo Retriever:</p> <ul> <li>AI Virtual Assistant for Customer Service (NVIDIA AI Blueprint)</li> <li>Build an Enterprise RAG pipeline (NVIDIA AI Blueprint)</li> <li>Building Code Documentation Agents with CrewAI (CrewAI Demo)</li> <li>Digital Human for Customer Service (NVIDIA AI Blueprint)</li> <li>Document Research Assistant for Blog Creation (LlamaIndex Jupyter Notebook)</li> <li>Video Search and Summarization (NVIDIA AI Blueprint)</li> </ul>"},{"location":"#related-topics","title":"Related Topics","text":"<ul> <li>NeMo Retriever Text Embedding NIM</li> <li>NeMo Retriever Text Reranking NIM</li> <li>NVIDIA NIM for Object Detection</li> <li>NVIDIA NIM for Image OCR</li> </ul>"},{"location":"extraction/audio/","title":"Use NeMo Retriever Extraction with Riva for Audio Processing","text":"<p>This documentation describes two methods to run NeMo Retriever extraction  with the RIVA ASR NIM microservice for processing audio files.</p> <ul> <li>Run the NIM locally by using Docker Compose</li> <li>Use NVIDIA Cloud Functions (NVCF) endpoints for cloud-based inference</li> </ul> <p>Note</p> <p>NeMo Retriever extraction is also known as NVIDIA Ingest and nv-ingest.</p>"},{"location":"extraction/audio/#overview","title":"Overview","text":"<p>NeMo Retriever extraction now supports the processing and retrieval of audio files for Retrieval Augmented Generation (RAG) applications.  Similar to how the multimodal document extraction pipeline leverages object detection and image OCR microservices,  NeMo Retriever leverages the RIVA ASR NIM microservice  to transcribe audio files to text, which is then embedded by using the NeMo Retriever embedding NIM. </p> <p>Important</p> <p>Due to limitations in available VRAM controls in the current release of audio NIMs, it must run on a dedicated additional GPU. For the full list of requirements to run RIVA NIM, refer to Support Matrix.</p> <p>Important</p> <p>To perform audio ingestion, librosa must be installed in the nv-ingest container. Edit docker-compose.yaml and set <code>INSTALL_AUDIO_EXTRACTION_DEPS=true</code>  to install librosa during container startup.</p> <p>This Early Access pipeline enables users to now retrieve audio files at the segment level. </p> <p></p>"},{"location":"extraction/audio/#run-the-nim-locally-by-using-docker-compose","title":"Run the NIM Locally by Using Docker Compose","text":"<p>Use the following procedure to run the NIM locally.</p> <p>Important</p> <p>RIVA NIM must run on a dedicated additional GPU. Edit docker-compose.yaml to set the audio service's device_id to a dedicated GPU: device_ids: [\"1\"] or higher.</p> <ol> <li> <p>To access the required container images, log in to the NVIDIA Container Registry (nvcr.io). Use your NGC key as the password. Run the following command in your terminal.</p> <ul> <li>Replace <code>&lt;your-ngc-key&gt;</code> with your actual NGC API key.</li> <li>The username is always <code>$oauthtoken</code>.</li> </ul> <pre><code>$ docker login nvcr.io\nUsername: $oauthtoken\nPassword: &lt;your-ngc-key&gt;\n</code></pre> </li> <li> <p>Store your NGC key in an environment variable file.</p> </li> </ol> <p>For convenience and security, store your NGC key in a .env file. This enables services to access it without needing to enter the key manually each time.</p> <p>Create a .env file in your working directory and add the following line: <pre><code>NGC_API_KEY=&lt;your-ngc-key&gt;\n</code></pre> Again, replace  with your actual NGC key. <ol> <li> <p>Start the nv-ingest services with the <code>audio</code> profile. This profile includes the necessary components for audio processing. Use the following command.</p> <ul> <li>The <code>--profile audio</code> flag ensures that audio-specific services are launched. For more information, refer to Profile Information.</li> </ul> <pre><code>docker compose --profile retrieval --profile audio up\n</code></pre> </li> <li> <p>After the services are running, you can interact with nv-ingest by using Python.</p> <ul> <li>The <code>Ingestor</code> object initializes the ingestion process.</li> <li>The <code>files</code> method specifies the input files to process.</li> <li>The <code>extract</code> method tells nv-ingest to extract information from WAV audio files.</li> <li>The <code>document_type</code> parameter is optional, because <code>Ingestor</code> should detect the file type automatically.</li> </ul> <pre><code>ingestor = (\n    Ingestor()\n    .files(\"./data/*.wav\")\n    .extract(\n        document_type=\"wav\",  # Ingestor should detect type automatically in most cases\n        extract_method=\"audio\",\n    )\n)\n</code></pre> <p>Tip</p> <p>For more Python examples, refer to NV-Ingest: Python Client Quick Start Guide.</p> </li> </ol>"},{"location":"extraction/audio/#use-nvcf-endpoints-for-cloud-based-inference","title":"Use NVCF Endpoints for Cloud-Based Inference","text":"<p>Instead of running NV-Ingest locally, you can use NVCF to perform inference by using remote endpoints.</p> <ol> <li> <p>NVCF requires an authentication token and a function ID for access. Ensure you have these credentials ready before making API calls.</p> </li> <li> <p>Run inference by using Python. Provide an NVCF endpoint along with authentication details.</p> <ul> <li>The <code>Ingestor</code> object initializes the ingestion process.</li> <li>The <code>files</code> method specifies the input files to process.</li> <li>The <code>extract</code> method tells nv-ingest to extract information from WAV audio files.</li> <li>The <code>document_type</code> parameter is optional, because <code>Ingestor</code> should detect the file type automatically.</li> </ul> <pre><code>ingestor = (\n    Ingestor()\n    .files(\"./data/*.mp3\")\n    .extract(\n        document_type=\"mp3\",\n        extract_method=\"audio\",\n        extract_audio_params={\n            \"grpc_endpoint\": \"grpc.nvcf.nvidia.com:443\",\n            \"auth_token\": \"&lt;API key&gt;\",\n            \"function_id\": \"&lt;function ID&gt;\",\n            \"use_ssl\": True,\n        },\n    )\n)\n</code></pre> <p>Tip</p> <p>For more Python examples, refer to NV-Ingest: Python Client Quick Start Guide.</p> </li> </ol>"},{"location":"extraction/audio/#related-topics","title":"Related Topics","text":"<ul> <li>Support Matrix</li> <li>Troubleshoot Nemo Retriever Extraction</li> <li>Use the NV-Ingest Python API</li> </ul>"},{"location":"extraction/chunking/","title":"Split Documents","text":"<p>Splitting, also known as chunking, breaks large documents or text into smaller, manageable sections to improve retrieval efficiency.  After chunking, only the most relevant pieces of information are retrieved for a given query.  Chunking also prevents text from exceeding the context window of the embedding model.</p> <p>There are two ways that NV Ingest chunks text:</p> <ul> <li>By using the <code>text_depth</code> parameter in the <code>extraction</code> task.</li> <li>Token-based splitting by using the <code>split</code> task.</li> </ul> <p>Warning</p> <p>NeMo Retriever extraction is designed to process language and language-length strings. If you submit a document that contains extremely long, or non-language text strings, such as a DNA sequence, errors or unexpected results occur.</p>"},{"location":"extraction/chunking/#extraction-text-depth","title":"Extraction Text Depth","text":"<p>You can use the <code>text_depth</code> parameter to specify how extracted text is chunked together by the extractor.  For example, the following code chunks the document text by page, for document types that have pages.</p> <pre><code>ingestor = ingestor.extract(\n    extract_text=True,\n    text_depth=\"page\"\n)\n</code></pre> <p>The following table contains the <code>text_depth</code> parameter values.</p> Value Description <code>document</code> Doesn't perform any splitting, and returns the full document as one chunk. <code>page</code> Returns a single chunk of text for each page. <p>For most documents, we recommend that you set <code>text_depth</code> to <code>page</code>, because this tends to give the best performance for retrieval.  However, in some cases, such as with <code>.txt</code> documents, there aren't any page breaks to split on. </p> <p>If you want chunks smaller than <code>page</code>, use token-based splitting as described in the following section.</p>"},{"location":"extraction/chunking/#token-based-splitting","title":"Token-Based Splitting","text":"<p>The <code>split</code> task uses a tokenizer to count the number of tokens in the document,  and splits the document based on the desired maximum chunk size and chunk overlap.  We recommend that you use the <code>meta-llama/Llama-3.2-1B</code> tokenizer,  because it's the same tokenizer as the llama-3.2 embedding model that we use for embedding. However, you can use any tokenizer from any HuggingFace model that includes a tokenizer file.</p> <p>Use the <code>split</code> method to chunk large documents as shown in the following code.</p> <p>Note</p> <p>The default tokenizer (<code>meta-llama/Llama-3.2-1B</code>) requires a Hugging Face access token. You must set <code>hf_access_token\": \"hf_***</code> to authenticate.</p> <pre><code>ingestor = ingestor.split(\n    tokenizer=\"meta-llama/Llama-3.2-1B\",\n    chunk_size=1024,\n    chunk_overlap=150,\n    params={\"split_source_types\": [\"text\", \"PDF\"], \"hf_access_token\": \"hf_***\"}\n)\n</code></pre> <p>To use a different tokenizer, such as <code>intfloat/e5-large-unsupervised</code>, you can modify the <code>split</code> call as shown following.</p> <pre><code>ingestor = ingestor.split(\n    tokenizer=\"intfloat/e5-large-unsupervised\",\n    chunk_size=1024,\n    chunk_overlap=150,\n    params={\"split_source_types\": [\"text\", \"PDF\"], \"hf_access_token\": \"hf_***\"}\n)\n</code></pre>"},{"location":"extraction/chunking/#split-parameters","title":"Split Parameters","text":"<p>The following table contains the <code>split</code> parameters.</p> Parameter Description Default <code>tokenizer</code> HuggingFace Tokenizer identifier or path. <code>meta-llama/Llama-3.2-1B</code> <code>chunk_size</code> Maximum number of tokens per chunk. <code>1024</code> <code>chunk_overlap</code> Number of tokens to overlap between chunks. <code>150</code> <code>params</code> A sub-dictionary that can contain <code>split_source_types</code> and <code>hf_access_token</code> <code>{}</code> <code>hf_access_token</code> Your Hugging Face access token. \u2014 <code>split_source_types</code> The source types to split on (only splits on text by default). \u2014"},{"location":"extraction/chunking/#pre-download-the-tokenizer","title":"Pre-download the Tokenizer","text":"<p>When the NV Ingest container is built, it pre-downloads a default tokenizer, so that it doesn't have to download the tokenizer at runtime. </p> <p>By default, the NV Ingest container downloads the <code>intfloat/e5-large-unsupervised</code> tokenizer, which is not gated, and does not require any special permissions.</p> <p>You can use the <code>meta-llama/Llama-3.2-1B</code> tokenizer instead,  but this is a gated model, and requires special permissions. To pre-download the <code>meta-llama/Llama-3.2-1B</code> tokenizer, you must do the following:</p> <ul> <li>Review the license agreement.</li> <li>Request access.</li> <li>Set the <code>HF_ACCESS_TOKEN</code> environment variable to your HuggingFace access token.</li> <li>Set the <code>DOWNLOAD_LLAMA_TOKENIZER</code> environment variable to <code>true</code>.</li> </ul>"},{"location":"extraction/content-metadata/","title":"Source and Content Metadata Reference for NV-Ingest","text":"<p>This documentation contains the reference for the content metadata.  The definitions used in this documentation are the following:</p> <ul> <li>Source \u2014 The file that is ingested, and from which content and metadata is extracted.</li> <li>Content \u2014 Data extracted from a source, such as text or an image.</li> </ul> <p>Metadata can be extracted from a source or content, or generated by using models, heuristics, or other methods.</p>"},{"location":"extraction/content-metadata/#source-file-metadata","title":"Source File Metadata","text":"<p>The following is the metadata for source files.</p> Field Description Method Source Name The name of the source file. Extracted Source ID The ID of the source file. Extracted Source location The URL, URI, or pointer to the storage location of the source file. \u2014 Source Type The type of the source file, such as pdf, docx, pptx, or txt. Extracted Collection ID The ID of the collection in which the source is contained. \u2014 Date Created The date the source was created. Extracted Last Modified The date the source was last modified. Extracted Partition ID The offset of this data fragment within a larger set of fragments. Generated Access Level The role-based access control for the source. \u2014 Summary A summary of the source. (Not yet implemented.) Generated"},{"location":"extraction/content-metadata/#content-metadata","title":"Content Metadata","text":"<p>The following is the metadata for content.  These fields apply to all content types including text, images, and tables.</p> Field Description Method Type The type of the content. Text, Image, Structured, Table, or Chart. Generated Subtype The type of the content for structured data types, such as table or chart. \u2014 Content Content extracted from the source. Extracted Description A text description of the content object. Generated Page # The page # of the content in the source. Extracted Hierarchy The location or order of the content within the source. Extracted"},{"location":"extraction/content-metadata/#text-metadata","title":"Text Metadata","text":"<p>The following is the metadata for text.</p> Field Description Method Text Type The type of the text, such as header or body. Extracted Keywords Keywords, Named Entities, or other phrases. Extracted Language The language of the content. Generated Summary An abbreviated summary of the content. (Not yet implemented.) Generated"},{"location":"extraction/content-metadata/#image-metadata","title":"Image Metadata","text":"<p>The following is the metadata for images.</p> Field Description Method Image Type The type of the image, such as structured, natural, hybrid, and others. Generated (Classifier) Structured Image Type The type of the content for structured data types, such as bar chart, pie chart, and others. Generated (Classifier) Caption Any caption or subheading associated with Image Extracted Text Extracted text from a structured chart Extracted Image location Location (x,y) of chart within an image Extracted Image location max dimensions Max dimensions (x_max,y_max) of location (x,y) Extracted uploaded_image_uri Mirrors source_metadata.source_location \u2014"},{"location":"extraction/content-metadata/#table-metadata","title":"Table Metadata","text":"<p>The following is the metadata for tables within documents.</p> <p>Warning</p> <p>Tables should not be chunked</p> Field Description Method Table format Structured (dataframe / lists of rows and columns), or serialized as markdown, html, latex, simple (cells separated as spaces). Extracted Table content Extracted text content, formatted according to table_metadata.table_format. Extracted Table location The bounding box of the table. Extracted Table location max dimensions The max dimensions (x_max,y_max) of the bounding box of the table. Extracted Caption The caption for the table or chart. Extracted Title The title of the table. Extracted Subtitle The subtitle of the table. Extracted Axis Axis information for the table. Extracted uploaded_image_uri A mirror of source_metadata.source_location. Generated"},{"location":"extraction/content-metadata/#example-metadata","title":"Example Metadata","text":"<p>The following is an example JSON representation of metadata.  This is an example only, and does not contain the full metadata. For the full file, refer to the data folder.</p> <pre><code>{\n    \"document_type\": \"text\",\n    \"metadata\": \n    {\n        \"content\": \"TestingDocument...\",\n        \"content_url\": \"\",\n        \"source_metadata\": \n        {\n            \"source_name\": \"data/multimodal_test.pdf\",\n            \"source_id\": \"data/multimodal_test.pdf\",\n            \"source_location\": \"\",\n            \"source_type\": \"PDF\",\n            \"collection_id\": \"\",\n            \"date_created\": \"2025-03-13T18:37:14.715892\",\n            \"last_modified\": \"2025-03-13T18:37:14.715534\",\n            \"summary\": \"\",\n            \"partition_id\": -1,\n            \"access_level\": 1\n        },\n        \"content_metadata\": \n        {\n            \"type\": \"structured\",\n            \"description\": \"Structured chart extracted from PDF document.\",\n            \"page_number\": 1,\n            \"hierarchy\": \n            {\n                \"page_count\": 3,\n                \"page\": 1,\n                \"block\": -1,\n                \"line\": -1,\n                \"span\": -1,\n                \"nearby_objects\": \n                {\n                    \"text\": \n                    {\n                        \"content\": [],\n                        \"bbox\": [],\n                        \"type\": []\n                    },\n                    \"images\": \n                    {\n                        \"content\": [],\n                        \"bbox\": [],\n                        \"type\": []\n                    },\n                    \"structured\": \n                    {\n                        \"content\": [],\n                        \"bbox\": [],\n                        \"type\": []\n                    }\n                }\n            },\n            \"subtype\": \"chart\"\n        },\n        \"audio_metadata\": null,\n        \"text_metadata\": null,\n        \"image_metadata\": null,\n        \"table_metadata\": \n        {\n            \"caption\": \"\",\n            \"table_format\": \"image\",\n            \"table_content\": \"Below,is a high-quality picture of some shapes          Picture\",\n            \"table_content_format\": \"\",\n            \"table_location\": \n            [\n                74,\n                614,\n                728,\n                920\n            ],\n            \"table_location_max_dimensions\": \n            [\n                792,\n                1024\n            ],\n            \"uploaded_image_uri\": \"\"\n        },\n        \"chart_metadata\": null,\n        \"error_metadata\": null,\n        \"info_message_metadata\": null,\n        \"debug_metadata\": null,\n        \"raise_on_failure\": false\n    }\n}\n</code></pre>"},{"location":"extraction/contributing/","title":"Contributing to NV-Ingest","text":"<p>External contributions to NV-Ingest will be welcome soon, and they are greatly appreciated!  For more information, refer to Contributing to NV-Ingest.</p>"},{"location":"extraction/custom-metadata/","title":"Use Custom Metadata to Filter Search Results","text":"<p>You can upload custom metadata for documents during ingestion.  By uploading custom metadata you can attach additional information to documents,  and use it for filtering results during retrieval operations.  For example, you can add author metadata to your documents, and filter by author when you retrieve results.  To create filters, you use Milvus Filtering Expressions.</p> <p>Use this documentation to use custom metadata to filter search results when you work with NeMo Retriever extraction.</p>"},{"location":"extraction/custom-metadata/#limitations","title":"Limitations","text":"<p>The following are limitation when you use custom metadata:</p> <ul> <li>Metadata fields must be consistent across documents in the same collection.</li> <li>Complex filter expressions may impact retrieval performance.</li> <li>If you update your custom metadata, you must ingest your documents again to use the new metadata.</li> </ul>"},{"location":"extraction/custom-metadata/#add-custom-metadata-during-ingestion","title":"Add Custom Metadata During Ingestion","text":"<p>You can add custom metadata during the document ingestion process.  You can specify metadata for each file,  and you can specify different metadata for different documents in the same ingestion batch.</p>"},{"location":"extraction/custom-metadata/#metadata-structure","title":"Metadata Structure","text":"<p>You specify custom metadata as a dataframe or a file (json, csv, or parquet). </p> <p>The following example contains metadata fields for category, department, and timestamp.  You can create whatever metadata is helpful for your scenario.</p> <pre><code>import pandas as pd\n\nmeta_df = pd.DataFrame(\n    {\n        \"source\": [\"data/woods_frost.pdf\", \"data/multimodal_test.pdf\"],\n        \"category\": [\"Alpha\", \"Bravo\"],\n        \"department\": [\"Language\", \"Engineering\"],\n        \"timestamp\": [\"2025-05-01T00:00:00\", \"2025-05-02T00:00:00\"]\n    }\n)\n\n# Convert the dataframe to a csv file, \n# to demonstrate how to ingest a metadata file in a later step.\n\nfile_path = \"./meta_file.csv\"\nmeta_df.to_csv(file_path)\n</code></pre>"},{"location":"extraction/custom-metadata/#example-add-custom-metadata-during-ingestion","title":"Example: Add Custom Metadata During Ingestion","text":"<p>The following example adds custom metadata during ingestion.  For more information about the <code>Ingestor</code> class, see Use the NV-Ingest Python API. For more information about the <code>vdb_upload</code> method, see Upload Data.</p> <pre><code>from nv_ingest_client.client import Ingestor\n\nhostname=\"localhost\"\ncollection_name = \"nv_ingest_collection\"\nsparse = True\n\ningestor = ( \n    Ingestor(message_client_hostname=hostname)\n        .files([\"data/woods_frost.pdf\", \"data/multimodal_test.pdf\"])\n        .extract(\n            extract_text=True,\n            extract_tables=True,\n            extract_charts=True,\n            extract_images=True,\n            text_depth=\"page\"\n        )\n        .embed()\n        .vdb_upload(\n            collection_name=collection_name, \n            milvus_uri=f\"http://{hostname}:19530\", \n            sparse=sparse, \n            minio_endpoint=f\"{hostname}:9000\", \n            dense_dim=2048,\n            meta_dataframe=file_path, \n            meta_source_field=\"source\", \n            meta_fields=[\"category\", \"department\", \"timestamp\"]\n        )\n)\nresults = ingestor.ingest_async().result()\n</code></pre>"},{"location":"extraction/custom-metadata/#best-practices","title":"Best Practices","text":"<p>The following are the best practices when you work with custom metadata:</p> <ul> <li>Plan metadata structure before ingestion.</li> <li>Test filter expressions with small datasets first.</li> <li>Consider performance implications of complex filters.</li> <li>Validate metadata during ingestion.</li> <li>Handle missing metadata fields gracefully.</li> <li>Log invalid filter expressions.</li> </ul>"},{"location":"extraction/custom-metadata/#use-custom-metadata-to-filter-results-during-retrieval","title":"Use Custom Metadata to Filter Results During Retrieval","text":"<p>You can use custom metadata to filter documents during retrieval operations.  Use filter expressions that follow the Milvus boolean expression syntax.  For more information, refer to Filtering Explained.</p>"},{"location":"extraction/custom-metadata/#example-filter-expressions","title":"Example Filter Expressions","text":"<p>The following example filters results by category.</p> <pre><code>filter_expr = 'content_metadata[\"category\"] == \"technical\"'\n</code></pre> <p>The following example filters results by time range.</p> <pre><code>filter_expr = 'content_metadata[\"timestamp\"] &gt;= \"2024-03-01T00:00:00\" and content_metadata[\"timestamp\"] &lt;= \"2025-12-31T00:00:00\"'\n</code></pre> <p>The following example filters by category and uses multiple logical operators.</p> <pre><code>filter_expr = '(content_metadata[\"department\"] == \"engineering\" and content_metadata[\"priority\"] == \"high\") or content_metadata[\"category\"] == \"critical\"'\n</code></pre>"},{"location":"extraction/custom-metadata/#example-use-a-filter-expression-in-search","title":"Example: Use a Filter Expression in Search","text":"<p>After ingestion is complete, and documents are uploaded to the database with metadata,  you can use the <code>content_metadata</code> field to filter search results.</p> <p>The following example uses a filter expression to narrow results by department.</p> <pre><code>from nv_ingest_client.util.milvus import nvingest_retrieval\n\nhostname=\"localhost\"\ncollection_name = \"nv_ingest_collection\"\nsparse = True\ntop_k = 5\nmodel_name=\"nvidia/llama-3.2-nv-embedqa-1b-v2\"\n\nfilter_expr = 'content_metadata[\"department\"] == \"Engineering\"'\n\nqueries = [\"this is expensive\"]\nq_results = []\nfor que in queries:\n    q_results.append(\n        nvingest_retrieval(\n            [que], \n            collection_name, \n            milvus_uri=f\"http://{hostname}:19530\", \n            embedding_endpoint=f\"http://{hostname}:8012/v1\",  \n            hybrid=sparse, \n            top_k=top_k, \n            model_name=model_name, \n            gpu_search=False, \n            _filter=filter_expr\n        )\n    )\n\nprint(f\"{q_results}\")\n</code></pre>"},{"location":"extraction/custom-metadata/#related-content","title":"Related Content","text":"<ul> <li>For a notebook that uses the CLI to add custom metadata and filter query results, see metadata_and_filtered_search.ipynb .</li> </ul>"},{"location":"extraction/data-store/","title":"Data Upload for NeMo Retriever Extraction","text":"<p>Use this documentation to learn how NeMo Retriever extraction handles and uploads data.</p> <p>Note</p> <p>NeMo Retriever extraction is also known as NVIDIA Ingest and nv-ingest.</p>"},{"location":"extraction/data-store/#overview","title":"Overview","text":"<p>NeMo Retriever extraction supports extracting text representations of various forms of content,  and ingesting to the Milvus vector database.  NeMo Retriever extraction does not store data on disk directly, except through Milvus.  The data upload task pulls extraction results to the Python client,  and then pushes them to Milvus by using its underlying MinIO object store service.</p> <p>The vector database stores only the extracted text representations of ingested data.  It does not store the embeddings for images.</p> <p>NeMo Retriever extraction supports uploading data by using the Ingestor.vdb_upload API.  Currently, data upload is not supported through the NV Ingest CLI.</p>"},{"location":"extraction/data-store/#upload-to-milvus","title":"Upload to Milvus","text":"<p>The <code>vdb_upload</code> method uses GPU Cagra accelerated bulk indexing support to load chunks into Milvus.  To enable hybrid retrieval, nv-ingest supports both dense (llama-embedder embeddings) and sparse (bm25) embeddings. </p> <p>Bulk indexing is high throughput, but has a built-in overhead of around one minute.  If the number of ingested documents is 10 or fewer, nv-ingest uses faster streaming inserts instead.  You can control this by setting <code>stream=True</code>. </p> <p>If you set <code>recreate=True</code>, nv-ingest drops and recreates the collection given as collection_name.  The Milvus service persists data to disk by using a Docker volume defined in docker-compose.yaml.  You can delete all collections by deleting that volume, and then restarting the nv-ingest service.</p> <p>Warning</p> <p>When you use the <code>vdb_upload</code> task with Milvus, you must expose the ports for the Milvus and MinIO containers to the nv-ingest client. This ensures that the nv-ingest client can connect to both services and perform the <code>vdb_upload</code> action.</p> <p>Tip</p> <p>When you use the <code>vdb_upload</code> method, the behavior of the upload depends on the <code>return_failures</code> parameter of the <code>ingest</code> method. For details, refer to Capture Job Failures.</p> <p>To upload to Milvus, use code similar to the following to define your <code>Ingestor</code>.</p> <pre><code>Ingestor(client=client)\n    .files(\"data/multimodal_test.pdf\")\n    .extract()\n    .embed()\n    .caption()\n    .vdb_upload(\n        collection_name=collection_name,\n        milvus_uri=milvus_uri,\n        sparse=sparse,\n        # for llama-3.2 embedder, use 1024 for e5-v5\n        dense_dim=2048,\n        stream=False,\n        recreate=False\n    )\n</code></pre>"},{"location":"extraction/data-store/#upload-to-a-custom-data-store","title":"Upload to a Custom Data Store","text":"<p>You can ingest to other data stores by using the <code>Ingestor.vdb_upload</code> method;  however, you must configure other data stores and connections yourself.  NeMo Retriever extraction does not provide connections to other data sources. </p> <p>Important</p> <p>NVIDIA makes no claim about accuracy, performance, or functionality of any vector database except Milvus. If you use a different vector database, it's your responsibility to test and maintain it.</p> <p>For more information, refer to Build a Custom Vector Database Operator.</p>"},{"location":"extraction/data-store/#related-topics","title":"Related Topics","text":"<ul> <li>Use the NeMo Retriever Extraction Python API</li> <li>Troubleshoot Nemo Retriever Extraction</li> </ul>"},{"location":"extraction/environment-config/","title":"Environment Variables for NeMo Retriever Extraction","text":"<p>The following are the environment variables that you can use to configure NeMo Retriever extraction. You can specify these in your .env file or directly in your environment.</p> <p>Note</p> <p>NeMo Retriever extraction is also known as NVIDIA Ingest and nv-ingest.</p>"},{"location":"extraction/environment-config/#general-environment-variables","title":"General Environment Variables","text":"Name Example Description <code>DOWNLOAD_LLAMA_TOKENIZER</code> <code>True</code> If <code>True</code>, the llama-3.2 tokenizer will be pre-dowloaded at build time. If not set to <code>True</code>, the (e5-large-unsupervised)[https://huggingface.co/intfloat/e5-large-unsupervised] tokenizer will be pre-downloaded. Note: setting this to <code>True</code> requires a HuggingFace access token with access to the gated Llama-3.2 models. See below for more info. <code>HF_ACCESS_TOKEN</code> - The HuggingFace access token used to pre-downlaod the Llama-3.2 tokenizer from HuggingFace (see above for more info). Llama 3.2 is a gated model, so you must request access to the Llama-3.2 models and then set this variable to a token that can access gated repositories on your behalf in order to use <code>DOWNLOAD_LLAMA_TOKENIZER=True</code>. <code>INGEST_LOG_LEVEL</code> - <code>DEBUG</code>  - <code>INFO</code>  - <code>WARNING</code>  - <code>ERROR</code>  - <code>CRITICAL</code> The log level for the ingest service, which controls the verbosity of the logging output. <code>MESSAGE_CLIENT_HOST</code> - <code>redis</code>  - <code>localhost</code>  - <code>192.168.1.10</code> Specifies the hostname or IP address of the message broker used for communication between services. <code>MESSAGE_CLIENT_PORT</code> - <code>7670</code>  - <code>6379</code> Specifies the port number on which the message broker is listening. <code>MINIO_BUCKET</code> <code>nv-ingest</code> Name of MinIO bucket, used to store image, table, and chart extractions. <code>NGC_API_KEY</code> <code>nvapi-*************</code> An authorized NGC API key, used to interact with hosted NIMs. To create an NGC key, go to https://org.ngc.nvidia.com/setup/api-keys. <code>NIM_NGC_API_KEY</code> \u2014 The key that NIM microservices inside docker containers use to access NGC resources. This is necessary only in some cases when it is different from <code>NGC_API_KEY</code>. If this is not specified, <code>NGC_API_KEY</code> is used to access NGC resources. <code>OTEL_EXPORTER_OTLP_ENDPOINT</code> <code>http://otel-collector:4317</code> The endpoint for the OpenTelemetry exporter, used for sending telemetry data. <code>REDIS_INGEST_TASK_QUEUE</code> <code>ingest_task_queue</code> The name of the task queue in Redis where tasks are stored and processed."},{"location":"extraction/environment-config/#library-mode-environment-variables","title":"Library Mode Environment Variables","text":"<p>These environment variables apply specifically when running NV-Ingest in library mode.</p> Name Example Description <code>NVIDIA_API_KEY</code> <code>nvapi-*************</code> API key for NVIDIA-hosted NIM services."},{"location":"extraction/environment-config/#related-topics","title":"Related Topics","text":"<ul> <li>Configure Ray Logging</li> </ul>"},{"location":"extraction/faq/","title":"Frequently Asked Questions for NeMo Retriever Extraction","text":"<p>This documentation contains the Frequently Asked Questions (FAQ) for NeMo Retriever extraction.</p> <p>Note</p> <p>NeMo Retriever extraction is also known as NVIDIA Ingest and nv-ingest.</p>"},{"location":"extraction/faq/#what-if-i-already-have-a-retrieval-pipeline-can-i-just-use-nemo-retriever-extraction","title":"What if I already have a retrieval pipeline? Can I just use NeMo Retriever extraction?","text":"<p>You can use the nv-ingest-cli or Python APIs to perform extraction only, and then consume the results. Using the Python API, <code>results</code> is a list object with one entry. For code examples, see the Jupyter notebooks Multimodal RAG with LlamaIndex  and Multimodal RAG with LangChain.</p>"},{"location":"extraction/faq/#where-does-nemo-retriever-extraction-nv-ingest-ingest-to","title":"Where does NeMo Retriever extraction (nv-ingest) ingest to?","text":"<p>NeMo Retriever extraction supports extracting text representations of various forms of content,  and ingesting to the Milvus vector database.  NeMo Retriever extraction does not store data on disk except through Milvus and its underlying Minio object store.  You can ingest to other data stores; however, you must configure other data stores yourself.  For more information, refer to Data Upload.</p>"},{"location":"extraction/faq/#how-would-i-process-unstructured-images","title":"How would I process unstructured images?","text":"<p>For images that <code>nemoretriever-page-elements-v2</code> does not classify as tables, charts, or infographics,  you can use our VLM caption task to create a dense caption of the detected image.  That caption is then be embedded along with the rest of your content.  For more information, refer to Extract Captions from Images.</p>"},{"location":"extraction/faq/#when-should-i-consider-using-nemoretriever-parse","title":"When should I consider using nemoretriever-parse?","text":"<p>For scanned documents, or documents with complex layouts,  we recommend that you use nemoretriever-parse.  Nemo Retriever parse provides higher-accuracy text extraction.  For more information, refer to Use Nemo Retriever Extraction with nemoretriever-parse.</p>"},{"location":"extraction/faq/#why-are-the-environment-variables-different-between-library-mode-and-self-hosted-mode","title":"Why are the environment variables different between library mode and self-hosted mode?","text":""},{"location":"extraction/faq/#self-hosted-deployments","title":"Self-Hosted Deployments","text":"<p>For self-hosted deployments, you should set the environment variables <code>NGC_API_KEY</code> and <code>NIM_NGC_API_KEY</code>. For more information, refer to Generate Your NGC Keys.</p> <p>For advanced scenarios, you might want to set <code>docker-compose</code> environment variables for NIM container paths, tags, and batch sizes.  You can set those directly in <code>docker-compose.yaml</code>, or in an environment variable file that docker compose uses.</p>"},{"location":"extraction/faq/#library-mode","title":"Library Mode","text":"<p>For production environments, you should use the provided Helm charts. For library mode, you should set the environment variable <code>NVIDIA_API_KEY</code>. This is because the NeMo Retriever containers and the NeMo Retriever services running inside them do not have access to the environment variables on the host machine where you run the <code>docker compose</code> command. Setting the variables in the <code>.env</code> file ensures that they are passed into the containers and available to the services that need them.</p> <p>For advanced scenarios, you might want to use library mode with self-hosted NIM instances.  You can set custom endpoints for each NIM.  For examples of <code>*_ENDPOINT</code> variables, refer to nv-ingest/docker-compose.yaml.</p>"},{"location":"extraction/faq/#what-parameters-or-settings-can-i-adjust-to-optimize-extraction-from-my-documents-or-data","title":"What parameters or settings can I adjust to optimize extraction from my documents or data?","text":"<p>See the Profile Information section  for information about the optional NIM components of the pipeline.</p> <p>You can configure the <code>extract</code>, <code>caption</code>, and other tasks by using the Ingestor API.</p> <p>To choose what types of content to extract, use code similar to the following.  For more information, refer to Extract Specific Elements from PDFs.</p> <pre><code>Ingestor(client=client)\n    .files(\"data/multimodal_test.pdf\")\n    .extract(              \n        extract_text=True,\n        extract_tables=True,\n        extract_charts=True,\n        extract_images=True,\n        paddle_output_format=\"markdown\",\n        extract_infographics=True,\n        text_depth=\"page\"\n    )\n</code></pre> <p>To generate captions for images, use code similar to the following. For more information, refer to Extract Captions from Images.</p> <pre><code>Ingestor(client=client)\n    .files(\"data/multimodal_test.pdf\")\n    .extract()\n    .embed()\n    .caption()\n)\n</code></pre>"},{"location":"extraction/helm/","title":"Deploy With Helm for NeMo Retriever Extraction","text":"<p>To deploy NeMo Retriever extraction by using Helm,  refer to NV-Ingest Helm Charts.</p>"},{"location":"extraction/metadata_documentation/","title":"Metadata Schema Documentation","text":"<p>This document provides a detailed explanation of the <code>MetadataSchema</code> and its constituent sub-schemas used within the NVIDIA Ingest Framework. This schema defines the structure for metadata associated with ingested content.</p>"},{"location":"extraction/metadata_documentation/#main-schema-metadataschema","title":"Main Schema: <code>MetadataSchema</code>","text":"<p>The <code>MetadataSchema</code> is the primary container for all metadata. It includes the core content, its URL, embedding, and various specialized metadata blocks.</p> Field Type Default Value/Behavior Description <code>content</code> <code>str</code> <code>\"\"</code> The actual textual content extracted from the source. <code>content_url</code> <code>str</code> <code>\"\"</code> URL pointing to the location of the content, if applicable. <code>embedding</code> <code>Optional[List[float]]</code> <code>None</code> Optional numerical vector representation (embedding) of the content. <code>source_metadata</code> <code>Optional[SourceMetadataSchema]</code> <code>None</code> Metadata about the original source of the content. See SourceMetadataSchema. <code>content_metadata</code> <code>Optional[ContentMetadataSchema]</code> <code>None</code> General metadata about the extracted content itself. See ContentMetadataSchema. <code>audio_metadata</code> <code>Optional[AudioMetadataSchema]</code> <code>None</code> Specific metadata for audio content. Automatically set to <code>None</code> if <code>content_metadata.type</code> is not <code>AUDIO</code>. See AudioMetadataSchema. <code>text_metadata</code> <code>Optional[TextMetadataSchema]</code> <code>None</code> Specific metadata for text content. Automatically set to <code>None</code> if <code>content_metadata.type</code> is not <code>TEXT</code>. See TextMetadataSchema. <code>image_metadata</code> <code>Optional[ImageMetadataSchema]</code> <code>None</code> Specific metadata for image content. Automatically set to <code>None</code> if <code>content_metadata.type</code> is not <code>IMAGE</code>. See ImageMetadataSchema. <code>table_metadata</code> <code>Optional[TableMetadataSchema]</code> <code>None</code> Specific metadata for tabular content. Automatically set to <code>None</code> if <code>content_metadata.type</code> is not <code>STRUCTURED</code>. See TableMetadataSchema. <code>chart_metadata</code> <code>Optional[ChartMetadataSchema]</code> <code>None</code> Specific metadata for chart content. See ChartMetadataSchema. <code>error_metadata</code> <code>Optional[ErrorMetadataSchema]</code> <code>None</code> Metadata describing any errors encountered during processing. See ErrorMetadataSchema. <code>info_message_metadata</code> <code>Optional[InfoMessageMetadataSchema]</code> <code>None</code> Informational messages related to the processing. See InfoMessageMetadataSchema. <code>debug_metadata</code> <code>Optional[Dict[str, Any]]</code> <code>None</code> A dictionary for storing any arbitrary debug information. <code>raise_on_failure</code> <code>bool</code> <code>False</code> If <code>True</code>, indicates that processing should halt on failure. <p>Note: A <code>model_validator</code> ensures that type-specific metadata fields (<code>audio_metadata</code>, <code>image_metadata</code>, <code>text_metadata</code>, <code>table_metadata</code>) are set to <code>None</code> if the <code>content_metadata.type</code> does not match the respective content type.</p>"},{"location":"extraction/metadata_documentation/#sub-schemas","title":"Sub-Schemas","text":""},{"location":"extraction/metadata_documentation/#sourcemetadataschema","title":"<code>SourceMetadataSchema</code>","text":"<p>Describes the origin of the ingested content.</p> Field Type Default Value Description <code>source_name</code> <code>str</code> Required Name of the source (e.g., filename, URL). <code>source_id</code> <code>str</code> Required Unique identifier for the source. <code>source_location</code> <code>str</code> <code>\"\"</code> Physical or logical location of the source (e.g., path, database table). <code>source_type</code> <code>Union[DocumentTypeEnum, str]</code> Required Type of the source document (e.g., <code>pdf</code>, <code>docx</code>, <code>url</code>). Uses <code>DocumentTypeEnum</code>. <code>collection_id</code> <code>str</code> <code>\"\"</code> Identifier for any collection this source belongs to. <code>date_created</code> <code>str</code> <code>datetime.now().isoformat()</code> ISO 8601 timestamp of when the source was created. Validated to be in ISO 8601 format. <code>last_modified</code> <code>str</code> <code>datetime.now().isoformat()</code> ISO 8601 timestamp of when the source was last modified. Validated to be in ISO 8601 format. <code>summary</code> <code>str</code> <code>\"\"</code> A brief summary of the source content. <code>partition_id</code> <code>int</code> <code>-1</code> Identifier for a partition if the source is part of a larger, partitioned dataset. <code>access_level</code> <code>Union[AccessLevelEnum, int]</code> <code>AccessLevelEnum.UNKNOWN</code> Access level associated with the source. Uses <code>AccessLevelEnum</code>."},{"location":"extraction/metadata_documentation/#contentmetadataschema","title":"<code>ContentMetadataSchema</code>","text":"<p>General metadata about the extracted content.</p> Field Type Default Value Description <code>type</code> <code>ContentTypeEnum</code> Required The type of the extracted content (e.g., <code>TEXT</code>, <code>IMAGE</code>, <code>AUDIO</code>). Uses <code>ContentTypeEnum</code>. <code>description</code> <code>str</code> <code>\"\"</code> A description of the extracted content. <code>page_number</code> <code>int</code> <code>-1</code> Page number from which the content was extracted, if applicable (e.g., for PDFs). <code>hierarchy</code> <code>ContentHierarchySchema</code> <code>ContentHierarchySchema()</code> Hierarchical information about the content's location within the source. See ContentHierarchySchema. <code>subtype</code> <code>Union[ContentTypeEnum, str]</code> <code>\"\"</code> A more specific subtype for the content (e.g., if <code>type</code> is <code>IMAGE</code>, <code>subtype</code> could be <code>diagram</code>). <code>start_time</code> <code>int</code> <code>-1</code> Start time in milliseconds for time-based media (e.g., audio, video). <code>end_time</code> <code>int</code> <code>-1</code> End time in milliseconds for time-based media."},{"location":"extraction/metadata_documentation/#contenthierarchyschema","title":"<code>ContentHierarchySchema</code>","text":"<p>Describes the structural location of content within a document.</p> Field Type Default Value Description <code>page_count</code> <code>int</code> <code>-1</code> Total number of pages in the document, if applicable. <code>page</code> <code>int</code> <code>-1</code> The specific page number where the content resides. <code>block</code> <code>int</code> <code>-1</code> Identifier for a block of content (e.g., paragraph, section). <code>line</code> <code>int</code> <code>-1</code> Line number within a block, if applicable. <code>span</code> <code>int</code> <code>-1</code> Span identifier within a line, for finer granularity. <code>nearby_objects</code> <code>NearbyObjectsSchema</code> <code>NearbyObjectsSchema()</code> Information about objects (text, images, structured data) near the current content. See NearbyObjectsSchema."},{"location":"extraction/metadata_documentation/#nearbyobjectsschema-currently-unused","title":"<code>NearbyObjectsSchema</code> (Currently Unused)","text":"<p>Container for different types of nearby objects.</p> Field Type Default Value Description <code>text</code> <code>NearbyObjectsSubSchema</code> <code>NearbyObjectsSubSchema()</code> Nearby textual objects. See NearbyObjectsSubSchema. <code>images</code> <code>NearbyObjectsSubSchema</code> <code>NearbyObjectsSubSchema()</code> Nearby image objects. <code>structured</code> <code>NearbyObjectsSubSchema</code> <code>NearbyObjectsSubSchema()</code> Nearby structured data objects (e.g., tables)."},{"location":"extraction/metadata_documentation/#nearbyobjectssubschema","title":"<code>NearbyObjectsSubSchema</code>","text":"<p>Describes a list of nearby objects of a specific type.</p> Field Type Default Value Description <code>content</code> <code>List[str]</code> <code>default_factory=list</code> List of content strings for the nearby objects. <code>bbox</code> <code>List[tuple]</code> <code>default_factory=list</code> List of bounding boxes (e.g., coordinates) for the nearby objects. <code>type</code> <code>List[str]</code> <code>default_factory=list</code> List of types for the nearby objects."},{"location":"extraction/metadata_documentation/#textmetadataschema","title":"<code>TextMetadataSchema</code>","text":"<p>Specific metadata for textual content.</p> Field Type Default Value Description <code>text_type</code> <code>TextTypeEnum</code> Required Type of text (e.g., <code>document</code>, <code>title</code>, <code>ocr</code>). Uses <code>TextTypeEnum</code>. <code>summary</code> <code>str</code> <code>\"\"</code> A summary of this specific text segment. <code>keywords</code> <code>Union[str, List[str], Dict]</code> <code>\"\"</code> Keywords extracted from or associated with the text. Can be a single string, list of strings, or a dictionary. <code>language</code> <code>LanguageEnum</code> <code>\"en\"</code> Detected or specified language of the text. Uses <code>LanguageEnum</code>. Defaults to English. <code>text_location</code> <code>tuple</code> <code>(0, 0, 0, 0)</code> Bounding box or coordinates of the text within its source (e.g., on a page). <code>text_location_max_dimensions</code> <code>tuple</code> <code>(0, 0, 0, 0)</code> Maximum dimensions of the space where <code>text_location</code> is defined (e.g., page width/height)."},{"location":"extraction/metadata_documentation/#imagemetadataschema","title":"<code>ImageMetadataSchema</code>","text":"<p>Specific metadata for image content.</p> Field Type Default Value Description <code>image_type</code> <code>Union[DocumentTypeEnum, str]</code> Required Type of the image document (e.g., <code>png</code>, <code>jpeg</code>). Uses <code>DocumentTypeEnum</code> or a string. <code>structured_image_type</code> <code>ContentTypeEnum</code> <code>ContentTypeEnum.NONE</code> If the image represents structured data (e.g., a table or chart), its <code>ContentTypeEnum</code>. <code>caption</code> <code>str</code> <code>\"\"</code> Caption associated with the image. <code>text</code> <code>str</code> <code>\"\"</code> Text extracted from the image (e.g., via OCR). <code>image_location</code> <code>tuple</code> <code>(0, 0, 0, 0)</code> Bounding box or coordinates of the image within its source. <code>image_location_max_dimensions</code> <code>tuple</code> <code>(0, 0)</code> Maximum dimensions of the space where <code>image_location</code> is defined. <code>uploaded_image_url</code> <code>str</code> <code>\"\"</code> URL of the image if it has been uploaded to a separate storage location. <code>width</code> <code>int</code> <code>0</code> Width of the image in pixels. Clamped to be non-negative. <code>height</code> <code>int</code> <code>0</code> Height of the image in pixels. Clamped to be non-negative."},{"location":"extraction/metadata_documentation/#tablemetadataschema","title":"<code>TableMetadataSchema</code>","text":"<p>Specific metadata for tabular content.</p> Field Type Default Value Description <code>caption</code> <code>str</code> <code>\"\"</code> Caption associated with the table. <code>table_format</code> <code>TableFormatEnum</code> Required Format of the table (e.g., <code>csv</code>, <code>html</code>). Uses <code>TableFormatEnum</code>. <code>table_content</code> <code>str</code> <code>\"\"</code> String representation of the table's content (e.g., CSV string, HTML markup). <code>table_content_format</code> <code>Union[TableFormatEnum, str]</code> <code>\"\"</code> Specific format of <code>table_content</code>. <code>table_location</code> <code>tuple</code> <code>(0, 0, 0, 0)</code> Bounding box or coordinates of the table within its source. <code>table_location_max_dimensions</code> <code>tuple</code> <code>(0, 0)</code> Maximum dimensions of the space where <code>table_location</code> is defined. <code>uploaded_image_uri</code> <code>str</code> <code>\"\"</code> URI of an image representation of the table, if applicable."},{"location":"extraction/metadata_documentation/#chartmetadataschema","title":"<code>ChartMetadataSchema</code>","text":"<p>Specific metadata for chart content. (Currently identical in structure to <code>TableMetadataSchema</code> but semantically distinct).</p> Field Type Default Value Description <code>caption</code> <code>str</code> <code>\"\"</code> Caption associated with the chart. <code>table_format</code> <code>TableFormatEnum</code> Required Underlying data format of the chart (e.g., data might be in <code>csv</code> format). Uses <code>TableFormatEnum</code>. <code>table_content</code> <code>str</code> <code>\"\"</code> String representation of the chart's underlying data. <code>table_content_format</code> <code>Union[TableFormatEnum, str]</code> <code>\"\"</code> Specific format of <code>table_content</code>. <code>table_location</code> <code>tuple</code> <code>(0, 0, 0, 0)</code> Bounding box or coordinates of the chart within its source. <code>table_location_max_dimensions</code> <code>tuple</code> <code>(0, 0)</code> Maximum dimensions of the space where <code>table_location</code> is defined. <code>uploaded_image_uri</code> <code>str</code> <code>\"\"</code> URI of an image representation of the chart, if applicable."},{"location":"extraction/metadata_documentation/#audiometadataschema","title":"<code>AudioMetadataSchema</code>","text":"<p>Specific metadata for audio content.</p> Field Type Default Value Description <code>audio_transcript</code> <code>str</code> <code>\"\"</code> Transcript of the audio content. <code>audio_type</code> <code>str</code> <code>\"\"</code> Type or format of the audio (e.g., <code>mp3</code>, <code>wav</code>)."},{"location":"extraction/metadata_documentation/#errormetadataschema-currently-unused","title":"<code>ErrorMetadataSchema</code> (Currently Unused)","text":"<p>Metadata describing errors encountered during processing.</p> Field Type Default Value Description <code>task</code> <code>TaskTypeEnum</code> Required The task that was being performed when the error occurred. Uses <code>TaskTypeEnum</code>. <code>status</code> <code>StatusEnum</code> Required The status indicating failure. Uses <code>StatusEnum</code>. <code>source_id</code> <code>str</code> <code>\"\"</code> Identifier of the source item that caused the error, if applicable. <code>error_msg</code> <code>str</code> Required The error message."},{"location":"extraction/metadata_documentation/#infomessagemetadataschema-currently-unused","title":"<code>InfoMessageMetadataSchema</code> (Currently Unused)","text":"<p>Informational messages related to processing.</p> Field Type Default Value Description <code>task</code> <code>TaskTypeEnum</code> Required The task associated with this informational message. Uses <code>TaskTypeEnum</code>. <code>status</code> <code>StatusEnum</code> Required The status related to this message (e.g., <code>INFO</code>, <code>WARNING</code>). Uses <code>StatusEnum</code>. <code>message</code> <code>str</code> Required The informational message content. <code>filter</code> <code>bool</code> Required A flag indicating if this message should be used for filtering purposes."},{"location":"extraction/metadata_documentation/#enums-used","title":"Enums Used","text":"<p>This schema relies on several enums defined in <code>nv_ingest_api.internal.enums.common</code>:</p> <ul> <li><code>AccessLevelEnum</code>: Defines access levels (e.g., <code>PUBLIC</code>, <code>CONFIDENTIAL</code>, <code>UNKNOWN</code>).</li> <li><code>ContentTypeEnum</code>: Defines types of content (e.g., <code>TEXT</code>, <code>IMAGE</code>, <code>AUDIO</code>, <code>STRUCTURED</code>, <code>NONE</code>).</li> <li><code>TextTypeEnum</code>: Defines types of text (e.g., <code>DOCUMENT</code>, <code>TITLE</code>, <code>OCR</code>, <code>CAPTION</code>).</li> <li><code>LanguageEnum</code>: Defines languages (e.g., <code>ENGLISH</code> (<code>en</code>), <code>SPANISH</code> (<code>es</code>)).</li> <li><code>TableFormatEnum</code>: Defines table formats (e.g., <code>CSV</code>, <code>HTML</code>, <code>TEXT</code>).</li> <li><code>StatusEnum</code>: Defines processing statuses (e.g., <code>SUCCESS</code>, <code>FAILURE</code>, <code>PROCESSING</code>, <code>INFO</code>, <code>WARNING</code>).</li> <li><code>DocumentTypeEnum</code>: Defines types of source documents (e.g., <code>PDF</code>, <code>DOCX</code>, <code>TXT</code>, <code>URL</code>, <code>PNG</code>, <code>MP3</code>).</li> <li><code>TaskTypeEnum</code>: Defines types of processing tasks (e.g., <code>EXTRACTION</code>, <code>EMBEDDING</code>, <code>STORAGE</code>).</li> </ul>"},{"location":"extraction/nemoretriever-parse/","title":"Use NeMo Retriever Extraction with nemoretriever-parse","text":"<p>This documentation describes two methods to run NeMo Retriever extraction  with nemoretriever-parse.</p> <ul> <li>Run the NIM locally by using Docker Compose</li> <li>Use NVIDIA Cloud Functions (NVCF) endpoints for cloud-based inference</li> </ul> <p>Note</p> <p>NeMo Retriever extraction is also known as NVIDIA Ingest and nv-ingest.</p>"},{"location":"extraction/nemoretriever-parse/#limitations","title":"Limitations","text":"<p>Currently, the limitations to using <code>nemoretriever-parse</code> with Nemo Retriever Extraction are the following:</p> <ul> <li>Extraction with <code>nemoretriever-parse</code> only supports PDFs, not image files. For more information, refer to Troubleshoot Nemo Retriever Extraction.</li> <li><code>nemoretriever-parse</code> is not supported on RTX Pro 6000 or B200. For more information, refer to Support Matrix.</li> </ul>"},{"location":"extraction/nemoretriever-parse/#run-the-nim-locally-by-using-docker-compose","title":"Run the NIM Locally by Using Docker Compose","text":"<p>Use the following procedure to run the NIM locally.</p> <p>Important</p> <p>Due to limitations in available VRAM controls in the current release of nemoretriever_parse, it must run on a dedicated additional GPU. Edit docker-compose.yaml to set nemoretriever_parse's device_id to a dedicated GPU: device_ids: [\"1\"] or higher.</p> <ol> <li> <p>Start the nv-ingest services with the <code>nemoretriever-parse</code> profile. This profile includes the necessary components for extracting text and metadata from images. Use the following command.</p> <ul> <li>The --profile nemoretriever-parse flag ensures that vision-language retrieval services are launched.  For more information, refer to Profile Information.</li> </ul> <pre><code>docker compose --profile nemoretriever-parse up\n</code></pre> </li> <li> <p>After the services are running, you can interact with nv-ingest by using Python.</p> <ul> <li>The <code>Ingestor</code> object initializes the ingestion process.</li> <li>The <code>files</code> method specifies the input files to process.</li> <li>The <code>extract</code> method tells nv-ingest to use <code>nemoretriever-parse</code> for extracting text and metadata from images.</li> <li>The <code>document_type</code> parameter is optional, because <code>Ingestor</code> should detect the file type automatically.</li> </ul> <pre><code>ingestor = (\n    Ingestor()\n    .files(\"./data/*.pdf\")\n    .extract(\n        document_type=\"pdf\",  # Ingestor should detect type automatically in most cases\n        extract_method=\"nemoretriever_parse\"\n    )\n)\n</code></pre> <p>Tip</p> <p>For more Python examples, refer to NV-Ingest: Python Client Quick Start Guide.</p> </li> </ol>"},{"location":"extraction/nemoretriever-parse/#using-nvcf-endpoints-for-cloud-based-inference","title":"Using NVCF Endpoints for Cloud-Based Inference","text":"<p>Instead of running NV-Ingest locally, you can use NVCF to perform inference by using remote endpoints.</p> <ol> <li> <p>Set the authentication token in the <code>.env</code> file.</p> <pre><code>NVIDIA_API_KEY=nvapi-...\n</code></pre> </li> <li> <p>Modify <code>docker-compose.yaml</code> to use the hosted <code>nemoretriever-parse</code> service.</p> <pre><code># build.nvidia.com hosted nemoretriever-parse\n- NEMORETRIEVER_PARSE_HTTP_ENDPOINT=https://integrate.api.nvidia.com/v1/chat/completions\n#- NEMORETRIEVER_PARSE_HTTP_ENDPOINT=http://nemoretriever-parse:8000/v1/chat/completions\n</code></pre> </li> <li> <p>Run inference by using Python.</p> <ul> <li>The <code>Ingestor</code> object initializes the ingestion process.</li> <li>The <code>files</code> method specifies the input files to process.</li> <li>The <code>extract</code> method tells nv-ingest to use <code>nemoretriever-parse</code> for extracting text and metadata from images.</li> <li>The <code>document_type</code> parameter is optional, because <code>Ingestor</code> should detect the file type automatically.</li> </ul> <pre><code>ingestor = (\n    Ingestor()\n    .files(\"./data/*.pdf\")\n    .extract(\n        document_type=\"pdf\",  # Ingestor should detect type automatically in most cases\n        extract_method=\"nemoretriever_parse\"\n    )\n)\n</code></pre> <p>Tip</p> <p>For more Python examples, refer to NV-Ingest: Python Client Quick Start Guide.</p> </li> </ol>"},{"location":"extraction/nemoretriever-parse/#related-topics","title":"Related Topics","text":"<ul> <li>Support Matrix</li> <li>Troubleshoot Nemo Retriever Extraction</li> <li>Use the NV-Ingest Python API</li> </ul>"},{"location":"extraction/ngc-api-key/","title":"Generate Your NGC Keys","text":"<p>NGC contains many public images, models, and datasets that can be pulled immediately without authentication.  To push and pull custom images, you must generate a key and authenticate with NGC.</p> <p>To create a key, go to https://org.ngc.nvidia.com/setup/api-keys.</p> <p>When you create an NGC key, select the following for Services Included.</p> <ul> <li>NGC Catalog</li> <li>Public API Endpoints</li> </ul> <p>Important</p> <p>Early Access participants must also select Private Registry.</p> <p></p>"},{"location":"extraction/ngc-api-key/#docker-login-to-ngc","title":"Docker Login to NGC","text":"<p>To pull the NIM container image from NGC, use your key to log in to the NGC registry by entering the following command and then following the prompts.  For the username, enter <code>$oauthtoken</code> exactly as shown.  It is a special authentication key for all users.</p> <pre><code>$ docker login nvcr.io\nUsername: $oauthtoken\nPassword: &lt;Your Key&gt;\n</code></pre>"},{"location":"extraction/notebooks/","title":"Notebooks for NeMo Retriever Extraction","text":"<p>To get started using NeMo Retriever extraction, you can try one of the ready-made notebooks that are available.</p> <p>Note</p> <p>NeMo Retriever extraction is also known as NVIDIA Ingest and nv-ingest.</p> <p>To get started with the basics, try one of the following notebooks:</p> <ul> <li>NV-Ingest: CLI Client Quick Start Guide</li> <li>NV-Ingest: Python Client Quick Start Guide</li> <li>How to add metadata to your documents and filter searches</li> <li>How to reindex a collection</li> </ul> <p>For more advanced scenarios, try one of the following notebooks:</p> <ul> <li>Build a Custom Vector Database Operator</li> <li>Try out the NVIDIA Multimodal PDF Data Extraction Blueprint</li> <li>Evaluate bo767 retrieval recall accuracy with NV-Ingest and Milvus</li> <li>Multimodal RAG with LangChain</li> <li>Multimodal RAG with LlamaIndex</li> </ul>"},{"location":"extraction/notebooks/#related-topics","title":"Related Topics","text":"<ul> <li>Prerequisites</li> <li>Support Matrix</li> <li>Deploy Without Containers (Library Mode)</li> <li>Deploy With Docker Compose (Self-Hosted)</li> <li>Deploy With Helm</li> </ul>"},{"location":"extraction/nv-ingest-python-api/","title":"Use the NeMo Retriever Extraction Python API","text":"<p>The NeMo Retriever extraction Python API provides a simple and flexible interface for processing and extracting information from various document types, including PDFs.</p> <p>Note</p> <p>NeMo Retriever extraction is also known as NVIDIA Ingest and nv-ingest.</p> <p>Tip</p> <p>There is a Jupyter notebook available to help you get started with the Python API. For more information, refer to Python Client Quick Start Guide.</p>"},{"location":"extraction/nv-ingest-python-api/#summary-of-key-methods","title":"Summary of Key Methods","text":"<p>The main class in the nv-ingest API is <code>Ingestor</code>.  The <code>Ingestor</code> class provides an interface for building, managing, and running data ingestion jobs, enabling for chainable task additions and job state tracking.  The following table describes methods of the <code>Ingestor</code> class.</p> Method Description <code>caption</code> Extract captions from images within the document. <code>embed</code> Generate embeddings from extracted content. <code>extract</code> Add an extraction task (text, tables, charts, infographics). <code>files</code> Add document paths for processing. <code>ingest</code> Submit jobs and retrieve results synchronously. <code>load</code> Ensure files are locally accessible (downloads if needed). <code>split</code> Split documents into smaller sections for processing. For more information, refer to Split Documents. <code>vdb_upload</code> Pushes extraction results to Milvus vector database. For more information, refer to Data Upload."},{"location":"extraction/nv-ingest-python-api/#track-job-progress","title":"Track Job Progress","text":"<p>For large document batches, you can enable a progress bar by setting <code>show_progress</code> to true.  Use the following code.</p> <pre><code># Return only successes\nresults = ingestor.ingest(show_progress=True)\n\nprint(len(results), \"successful documents\")\n</code></pre>"},{"location":"extraction/nv-ingest-python-api/#capture-job-failures","title":"Capture Job Failures","text":"<p>You can capture job failures by setting <code>return_failures</code> to true.  Use the following code.</p> <pre><code># Return both successes and failures\nresults, failures = ingestor.ingest(show_progress=True, return_failures=True)\n\nprint(f\"{len(results)} successful docs; {len(failures)} failures\")\n\nif failures:\n    print(\"Failures:\", failures[:1])\n</code></pre> <p>When you use the <code>vdb_upload</code> method, uploads are performed after ingestion completes.  The behavior of the upload depends on the following values of <code>return_failures</code>:</p> <ul> <li>False \u2013 If any job fails, the <code>ingest</code> method raises a runtime error and does not upload any data (all-or-nothing data upload). This is the default setting.</li> <li>True \u2013 If any jobs succeed, the results from those jobs are uploaded, and no errors are raised (partial data upload). The <code>ingest</code> method returns a failures object that contains the details for any jobs that failed. You can inspect the failures object and selectively retry or remediate the failed jobs.</li> </ul> <p>The following example uploads data to Milvus and returns any failures.</p> <pre><code>ingestor = (\n    Ingestor(client=client)\n    .files([\"/path/doc1.pdf\", \"/path/doc2.pdf\"])\n    .extract()\n    .embed()\n    .vdb_upload(collection_name=\"my_collection\", milvus_uri=\"milvus.db\")\n)\n\n# Use for large batches where you want successful chunks/pages to be committed, while collecting detailed diagnostics for failures.\nresults, failures = ingestor.ingest(return_failures=True)\n\nprint(f\"Uploaded {len(results)} successful docs; {len(failures)} failures\")\n\nif failures:\n    print(\"Failures:\", failures[:1])\n</code></pre>"},{"location":"extraction/nv-ingest-python-api/#quick-start-extracting-pdfs","title":"Quick Start: Extracting PDFs","text":"<p>The following example demonstrates how to initialize <code>Ingestor</code>, load a PDF file, and extract its contents. The <code>extract</code> method enables different types of data to be extracted.</p>"},{"location":"extraction/nv-ingest-python-api/#extract-a-single-pdf","title":"Extract a Single PDF","text":"<p>Use the following code to extract a single PDF file.</p> <pre><code>from nv_ingest_client.client.interface import Ingestor\n\n# Initialize Ingestor with a local PDF file\ningestor = Ingestor().files(\"path/to/document.pdf\")\n\n# Extract text, tables, and images\nresult = ingestor.extract().ingest()\n\nprint(result)\n</code></pre>"},{"location":"extraction/nv-ingest-python-api/#extract-multiple-pdfs","title":"Extract Multiple PDFs","text":"<p>Use the following code to process multiple PDFs at one time.</p> <pre><code>ingestor = Ingestor().files([\"path/to/doc1.pdf\", \"path/to/doc2.pdf\"])\n\n# Extract content from all PDFs\nresult = ingestor.extract().ingest()\n\nfor doc in result:\n    print(doc)\n</code></pre>"},{"location":"extraction/nv-ingest-python-api/#extract-specific-elements-from-pdfs","title":"Extract Specific Elements from PDFs","text":"<p>By default, the <code>extract</code> method extracts all supported content types.  You can customize the extraction behavior by using the following code.</p> <pre><code>ingestor = ingestor.extract(\n    extract_text=True,  # Extract text\n    text_depth=\"page\",\n    extract_tables=False,  # Skip table extraction\n    extract_charts=True,  # Extract charts\n    extract_infographics=True,  # Extract infographic images\n    extract_images=False  # Skip image extraction\n)\n</code></pre>"},{"location":"extraction/nv-ingest-python-api/#extract-non-standard-document-types","title":"Extract Non-standard Document Types","text":"<p>Use the following code to extract text from <code>.md</code>, <code>.sh</code>, and <code>.html</code> files.</p> <pre><code>ingestor = Ingestor().files([\"path/to/doc1.md\", \"path/to/doc2.html\"])\n\ningestor = ingestor.extract(\n    extract_text=True,  # Only extract text\n    extract_tables=False,\n    extract_charts=False,\n    extract_infographics=False,\n    extract_images=False\n)\n\nresult = ingestor.ingest()\n</code></pre>"},{"location":"extraction/nv-ingest-python-api/#extract-with-custom-document-type","title":"Extract with Custom Document Type","text":"<p>Use the following code to specify a custom document type for extraction.</p> <pre><code>ingestor = ingestor.extract(document_type=\"pdf\")\n</code></pre>"},{"location":"extraction/nv-ingest-python-api/#work-with-large-datasets-save-to-disk","title":"Work with Large Datasets: Save to Disk","text":"<p>By default, NeMo Retriever extraction stores the results from every document in system memory (RAM).  When you process a very large dataset with thousands of documents, you might encounter an Out-of-Memory (OOM) error.  The <code>save_to_disk</code> method configures the extraction pipeline to write the output for each document to a separate JSONL file on disk.</p>"},{"location":"extraction/nv-ingest-python-api/#basic-usage-save-to-a-directory","title":"Basic Usage: Save to a Directory","text":"<p>To save results to disk, simply chain the <code>save_to_disk</code> method to your ingestion task. By using <code>save_to_disk</code> the <code>ingest</code> method returns a list of <code>LazyLoadedList</code> objects,  which are memory-efficient proxies that read from the result files on disk.</p> <p>In the following example, the results are saved to a directory named <code>my_ingest_results</code>.  You are responsible for managing the created files.</p> <pre><code>ingestor = Ingestor().files(\"large_dataset/*.pdf\")\n\n# Use save_to_disk to configure the ingestor to save results to a specific directory.\n# Set cleanup=False to ensure that the directory is not deleted by any automatic process.\ningestor.save_to_disk(output_directory=\"./my_ingest_results\", cleanup=False)  # Offload results to disk to prevent OOM errors\n\n# 'results' is a list of LazyLoadedList objects that point to the new jsonl files.\nresults = ingestor.extract().ingest()\n\nprint(\"Ingestion results saved in ./my_ingest_results\")\n# You can now iterate over the results or inspect the files directly.\n</code></pre>"},{"location":"extraction/nv-ingest-python-api/#managing-disk-space-with-automatic-cleanup","title":"Managing Disk Space with Automatic Cleanup","text":"<p>When you use <code>save_to_disk</code>, NeMo Retriever extraction creates intermediate files.  For workflows where these files are temporary, NeMo Retriever extraction provides two automatic cleanup mechanisms.</p> <ul> <li> <p>Directory Cleanup with Context Manager \u2014 While not required for general use, the Ingestor can be used as a context manager (<code>with</code> statement). This enables the automatic cleanup of the entire output directory when <code>save_to_disk(cleanup=True)</code> is set (which is the default).</p> </li> <li> <p>File Purge After VDB Upload \u2013 The <code>vdb_upload</code> method includes a <code>purge_results_after_upload: bool = True</code> parameter (the default). After a successful VDB upload, this feature deletes the individual <code>.jsonl</code> files that were just uploaded.</p> </li> </ul> <p>You can also configure the output directory by using the <code>NV_INGEST_CLIENT_SAVE_TO_DISK_OUTPUT_DIRECTORY</code> environment variable.</p>"},{"location":"extraction/nv-ingest-python-api/#example-fully-automatic-cleanup","title":"Example (Fully Automatic Cleanup)","text":"<p>Fully Automatic cleanup is the recommended pattern for ingest-and-upload workflows where the intermediate files are no longer needed.  The entire process is temporary, and no files are left on disk. The following example includes automatic file purge. </p> <pre><code># After the 'with' block finishes, \n# the temporary directory and all its contents are automatically deleted.\n\nwith (\n    Ingestor()\n    .files(\"/path/to/large_dataset/*.pdf\")\n    .extract()\n    .embed()\n    .save_to_disk()  # cleanup=True is the default, enables directory deletion on exit\n    .vdb_upload()  # purge_results_after_upload=True is the default, deletes files after upload\n) as ingestor:\n    results = ingestor.ingest()\n</code></pre>"},{"location":"extraction/nv-ingest-python-api/#example-preserve-results-on-disk","title":"Example (Preserve Results on Disk)","text":"<p>In scenarios where you need to inspect or use the intermediate <code>jsonl</code> files, you can disable the cleanup features.  The following example disables automatic file purge. </p> <pre><code># After the 'with' block finishes, \n# the './permanent_results' directory and all jsonl files are preserved for inspection or other uses.\n\nwith (\n    Ingestor()\n    .files(\"/path/to/large_dataset/*.pdf\")\n    .extract()\n    .embed()\n    .save_to_disk(output_directory=\"./permanent_results\", cleanup=False)  # Specify a directory and disable directory-level cleanup\n    .vdb_upload(purge_results_after_upload=False)  # Disable automatic file purge after the VDB upload\n) as ingestor:\n    results = ingestor.ingest()\n</code></pre>"},{"location":"extraction/nv-ingest-python-api/#extract-captions-from-images","title":"Extract Captions from Images","text":"<p>The <code>caption</code> method generates image captions by using a vision-language model.  This can be used to describe images extracted from documents.</p> <p>Note</p> <p>The default model used by <code>caption</code> is <code>nvidia/llama-3.1-nemotron-nano-vl-8b-v1</code>.</p> <pre><code>ingestor = ingestor.caption()\n</code></pre> <p>To specify a different API endpoint, pass additional parameters to <code>caption</code>.</p> <pre><code>ingestor = ingestor.caption(\n    endpoint_url=\"https://integrate.api.nvidia.com/v1/chat/completions\",\n    model_name=\"nvidia/llama-3.1-nemotron-nano-vl-8b-v1\",\n    api_key=\"nvapi-\"\n)\n</code></pre>"},{"location":"extraction/nv-ingest-python-api/#extract-embeddings","title":"Extract Embeddings","text":"<p>The <code>embed</code> method in NV-Ingest generates text embeddings for document content.</p> <pre><code>ingestor = ingestor.embed()\n</code></pre> <p>Note</p> <p>By default, <code>embed</code> uses the llama-3.2-nv-embedqa-1b-v2 model.</p> <p>To use a different embedding model, such as nv-embedqa-e5-v5, specify a different <code>model_name</code> and <code>endpoint_url</code>.</p> <pre><code>ingestor = ingestor.embed(\n    endpoint_url=\"https://integrate.api.nvidia.com/v1\",\n    model_name=\"nvidia/nv-embedqa-e5-v5\",\n    api_key=\"nvapi-\"\n)\n</code></pre>"},{"location":"extraction/nv-ingest-python-api/#extract-audio","title":"Extract Audio","text":"<p>Use the following code to extract mp3 audio content.</p> <pre><code>from nv_ingest_client.client import Ingestor\n\ningestor = Ingestor().files(\"audio_file.mp3\")\n\ningestor = ingestor.extract(\n        document_type=\"mp3\",\n        extract_text=True,\n        extract_tables=False,\n        extract_charts=False,\n        extract_images=False,\n        extract_infographics=False,\n    ).split(\n        tokenizer=\"meta-llama/Llama-3.2-1B\",\n        chunk_size=150,\n        chunk_overlap=0,\n        params={\"split_source_types\": [\"mp3\"], \"hf_access_token\": \"hf_***\"}\n    )\n\nresults = ingestor.ingest()\n</code></pre>"},{"location":"extraction/nv-ingest-python-api/#related-topics","title":"Related Topics","text":"<ul> <li>Split Documents</li> <li>Troubleshoot Nemo Retriever Extraction</li> <li>Use Nemo Retriever Extraction with nemoretriever-parse</li> <li>Use NeMo Retriever Extraction with Riva for Audio Processing</li> <li>Use Multimodal Embedding</li> </ul>"},{"location":"extraction/nv-ingest_cli/","title":"Use the NV-Ingest Command Line Interface","text":"<p>After you install the Python dependencies, you can use the NV-Ingest command line interface (CLI).  To use the CLI, use the <code>nv-ingest-cli</code> command.</p> <p>To check the version of the CLI that you have installed, run the following command.</p> <pre><code>nv-ingest-cli --version\n</code></pre> <p>To get a list of the current CLI commands and their options, run the following command.</p> <pre><code>nv-ingest-cli --help\n</code></pre> <p>Tip</p> <p>There is a Jupyter notebook available to help you get started with the CLI. For more information, refer to CLI Client Quick Start Guide.</p>"},{"location":"extraction/nv-ingest_cli/#examples","title":"Examples","text":"<p>Use the following code examples to submit a document to the <code>nv-ingest-ms-runtime</code> service.</p> <p>Each of the following commands can be run from the host machine, or from within the <code>nv-ingest-ms-runtime</code> container.</p> <ul> <li>Host: <code>nv-ingest-cli ...</code></li> <li>Container: <code>nv-ingest-cli ...</code></li> </ul>"},{"location":"extraction/nv-ingest_cli/#example-text-file-with-no-splitting","title":"Example: Text File With No Splitting","text":"<p>To submit a text file with no splitting, run the following code.</p> <p>Note</p> <p>You receive a response that contains a single document, which is the entire text file. The data that is returned is wrapped in the appropriate metadata structure.</p> <pre><code>nv-ingest-cli \\\n  --doc ./data/test.pdf \\\n  --client_host=localhost \\\n  --client_port=7670\n</code></pre>"},{"location":"extraction/nv-ingest_cli/#example-pdf-file-with-splitting-only","title":"Example: PDF File With Splitting Only","text":"<p>To submit a .pdf file with only a splitting task, run the following code.</p> <pre><code>nv-ingest-cli \\\n  --doc ./data/test.pdf \\\n  --output_directory ./processed_docs \\\n  --task='split' \\\n  --client_host=localhost \\\n  --client_port=7670\n</code></pre>"},{"location":"extraction/nv-ingest_cli/#example-pdf-file-with-splitting-and-extraction","title":"Example: PDF File With Splitting and Extraction","text":"<p>To submit a .pdf file with both a splitting task and an extraction task, run the following code.</p> <p>Note</p> <p>This currently only works for pdfium, nemoretriever_parse, and Unstructured.io. Haystack, Adobe, and LlamaParse have existing workflows, but have not been fully converted to use our unified metadata schema.</p> <pre><code>nv-ingest-cli \\\n  --doc ./data/test.pdf \\\n  --output_directory ./processed_docs \\\n  --task='extract:{\"document_type\": \"pdf\", \"extract_method\": \"pdfium\"}' \\\n  --task='extract:{\"document_type\": \"docx\", \"extract_method\": \"python_docx\"}' \\\n  --task='split' \\\n  --client_host=localhost \\\n  --client_port=7670\n</code></pre>"},{"location":"extraction/nv-ingest_cli/#example-process-a-dataset","title":"Example: Process a Dataset","text":"<p>To submit a dataset for processing, run the following code.  To create a dataset, refer to Command Line Dataset Creation with Enumeration and Sampling.</p> <pre><code>nv-ingest-cli \\\n  --dataset dataset.json \\\n  --output_directory ./processed_docs \\\n  --task='extract:{\"document_type\": \"pdf\", \"extract_method\": \"pdfium\"}' \\\n  --client_host=localhost \\\n  --client_port=7670\n</code></pre> <p>Submit a PDF file with extraction tasks and upload extracted images to MinIO.</p> <pre><code>nv-ingest-cli \\\n  --doc ./data/test.pdf \\\n  --output_directory ./processed_docs \\\n  --task='extract:{\"document_type\": \"pdf\", \"extract_method\": \"pdfium\"}' \\\n  --client_host=localhost \\\n  --client_port=7670\n</code></pre>"},{"location":"extraction/nv-ingest_cli/#command-line-dataset-creation-with-enumeration-and-sampling","title":"Command Line Dataset Creation with Enumeration and Sampling","text":"<p>The <code>gen_dataset.py</code> script samples files from a specified source directory according to defined proportions and a total size target.  It offers options for caching the file list, outputting a sampled file list, and validating the output.</p> <pre><code>python ./src/util/gen_dataset.py --source_directory=./data --size=1GB --sample pdf=60 --sample txt=40 --output_file \\\n  dataset.json --validate-output\n</code></pre>"},{"location":"extraction/overview/","title":"What is NeMo Retriever Extraction?","text":"<p>NeMo Retriever extraction is a scalable, performance-oriented document content and metadata extraction microservice.  NeMo Retriever extraction uses specialized NVIDIA NIM microservices  to find, contextualize, and extract text, tables, charts and infographics that you can use in downstream generative applications.</p> <p>Note</p> <p>NeMo Retriever extraction is also known as NVIDIA Ingest and nv-ingest.</p> <p>NeMo Retriever extraction enables parallelization of splitting documents into pages where artifacts are classified (such as text, tables, charts, and infographics), extracted, and further contextualized through optical character recognition (OCR) into a well defined JSON schema.  From there, NeMo Retriever extraction can optionally manage computation of embeddings for the extracted content,  and optionally manage storing into a vector database Milvus.</p> <p>Note</p> <p>Cached and Deplot are deprecated. Instead, NeMo Retriever extraction now uses the yolox-graphic-elements NIM. With this change, you should now be able to run NeMo Retriever extraction on a single 24GB A10G or better GPU. If you want to use the old pipeline, with Cached and Deplot, use the NeMo Retriever extraction 24.12.1 release.</p>"},{"location":"extraction/overview/#what-nemo-retriever-extraction-is","title":"What NeMo Retriever Extraction Is \u2714\ufe0f","text":"<p>The following diagram shows the Nemo Retriever extraction pipeline.</p> <p></p> <p>NeMo Retriever extraction is a microservice service that does the following:</p> <ul> <li>Accept a JSON job description, containing a document payload, and a set of ingestion tasks to perform on that payload.</li> <li>Allow the results of a job to be retrieved. The result is a JSON dictionary that contains a list of metadata describing objects extracted from the base document, and processing annotations and timing/trace data.</li> <li>Support multiple methods of extraction for each document type to balance trade-offs between throughput and accuracy. For example, for .pdf documents, extraction is performed by using pdfium, nemoretriever-parse, Unstructured.io, and Adobe Content Extraction Services.</li> <li>Support various types of pre- and post- processing operations, including text splitting and chunking, transform and filtering, embedding generation, and image offloading to storage.</li> </ul> <p>NeMo Retriever extraction supports the following file types:</p> <ul> <li><code>bmp</code></li> <li><code>docx</code></li> <li><code>html</code> (converted to markdown format)</li> <li><code>jpeg</code></li> <li><code>json</code> (treated as text)</li> <li><code>md</code> (treated as text)</li> <li><code>pdf</code></li> <li><code>png</code></li> <li><code>pptx</code></li> <li><code>sh</code> (treated as text)</li> <li><code>tiff</code></li> <li><code>txt</code></li> </ul>"},{"location":"extraction/overview/#what-nemo-retriever-extraction-isnt","title":"What NeMo Retriever Extraction Isn't \u2716\ufe0f","text":"<p>NeMo Retriever extraction does not do the following:</p> <ul> <li>Run a static pipeline or fixed set of operations on every submitted document.</li> <li>Act as a wrapper for any specific document parsing library.</li> </ul>"},{"location":"extraction/overview/#related-topics","title":"Related Topics","text":"<ul> <li>Prerequisites</li> <li>Support Matrix</li> <li>Deploy Without Containers (Library Mode)</li> <li>Deploy With Docker Compose (Self-Hosted)</li> <li>Deploy With Helm</li> <li>Notebooks</li> <li>Multimodal PDF Data Extraction</li> </ul>"},{"location":"extraction/prerequisites/","title":"Prerequisites for NeMo Retriever Extraction","text":"<p>Before you begin using NeMo Retriever extraction, ensure the following software prerequisites are met.</p>"},{"location":"extraction/prerequisites/#software","title":"Software","text":"<ul> <li>Linux operating systems (Ubuntu 22.04 or later recommended)</li> <li>Docker</li> <li>Docker Compose</li> <li>CUDA Toolkit (NVIDIA Driver &gt;= <code>535</code>, CUDA &gt;= <code>12.2</code>)</li> <li>NVIDIA Container Toolkit</li> <li>Conda Python environment and package manager</li> </ul> <p>Note</p> <p>You install Python later.</p>"},{"location":"extraction/prerequisites/#related-topics","title":"Related Topics","text":"<ul> <li>Support Matrix</li> <li>Deploy Without Containers (Library Mode)</li> <li>Deploy With Docker Compose (Self-Hosted)</li> <li>Deploy With Helm</li> </ul>"},{"location":"extraction/quickstart-guide/","title":"Deploy With Docker Compose (Self-Hosted) for NeMo Retriever Extraction","text":"<p>Use this documentation to get started using NeMo Retriever extraction in self-hosted mode.</p>"},{"location":"extraction/quickstart-guide/#step-1-starting-containers","title":"Step 1: Starting Containers","text":"<p>This example demonstrates how to use the provided docker-compose.yaml to start all needed services with a few commands.</p> <p>Warning</p> <p>NIM containers on their first startup can take 10-15 minutes to pull and fully load models.</p> <p>If you prefer, you can run on Kubernetes by using our Helm chart. Also, there are additional environment variables you might want to configure.</p> <ol> <li> <p>Git clone the repo:</p> <p><code>git clone https://github.com/nvidia/nv-ingest</code></p> </li> <li> <p>Change the directory to the cloned repo</p> <p><code>cd nv-ingest</code>.</p> </li> <li> <p>Generate API keys and authenticate with NGC with the <code>docker login</code> command:</p> <pre><code># This is required to access pre-built containers and NIM microservices\n$ docker login nvcr.io\nUsername: $oauthtoken\nPassword: &lt;Your Key&gt;\n</code></pre> </li> <li> <p>Create a .env file that contains your NVIDIA Build API key.</p> <p>Note</p> <p>If you use an NGC personal key, then you should provide the same value for all keys, but you must specify each environment variable individually. In the past, you could create an API key. If you have an API key, you can still use that. For more information, refer to Generate Your NGC Keys and Environment Configuration Variables.</p> <pre><code># Container images must access resources from NGC.\n\nNGC_API_KEY=&lt;key to download containers from NGC&gt;\nNIM_NGC_API_KEY=&lt;key to download model files after containers start&gt;\n</code></pre> </li> <li> <p>(Optional) For faster OCR performance, you can use the nemoretriever-ocr-v1 container instead of the default PaddleOCR. Currently, the NemoRetriever OCR v1 container is in early access preview. Configure Helm to deploy nemoretriever-ocr-v1 and then set these values in your .env file:</p> <pre><code>OCR_IMAGE=nvcr.io/nvidia/nemo-microservices/nemoretriever-ocr-v1\nOCR_TAG=latest\nOCR_MODEL_NAME=scene_text_ensemble\n</code></pre> </li> </ol> <p>Alternatively, you can modify the OCR service directly in your docker-compose.yaml file with these image tags.</p> <ol> <li> <p>Make sure NVIDIA is set as your default container runtime before running the docker compose command with the command:</p> <p><code>sudo nvidia-ctk runtime configure --runtime=docker --set-as-default</code></p> </li> <li> <p>Start core services. This example uses the table-structure profile.  For more information about other profiles, see Profile Information.</p> <p><code>docker compose --profile retrieval --profile table-structure up</code></p> <p>Tip</p> <p>By default, we have configured log levels to be verbose. It's possible to observe service startup proceeding. You will notice a lot of log messages. Disable verbose logging by configuring <code>NIM_TRITON_LOG_VERBOSE=0</code> for each NIM in docker-compose.yaml.</p> <p>Tip</p> <p>For optimal performance on specific hardware, you can use <code>docker-compose</code> override files. Override files adjust settings, such as memory allocation, for different GPU architectures. To use an override file, include it in your <code>docker compose up</code> command by using a second <code>-f</code> flag after the base <code>docker-compose.yaml</code> file. The settings in the second file override the values that are set in the first file.</p> <p>The following example uses an override file that contains settings that are optimized for an NVIDIA A100 GPU with 40GB of VRAM. <pre><code>docker compose \\\n  -f docker-compose.yaml \\\n  -f docker-compose.a100-40gb.yaml \\\n  --profile retrieval --profile table-structure up\n</code></pre></p> </li> <li> <p>When core services have fully started, <code>nvidia-smi</code> should show processes like the following:</p> <pre><code># If it's taking &gt; 1m for `nvidia-smi` to return, the bus will likely be busy setting up the models.\n+---------------------------------------------------------------------------------------+\n| Processes:                                                                            |\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n|        ID   ID                                                             Usage      |\n|=======================================================================================|\n|    0   N/A  N/A     80461      C   milvus                                     1438MiB |\n|    0   N/A  N/A     83791      C   tritonserver                               2492MiB |\n|    0   N/A  N/A     85605      C   tritonserver                               1896MiB |\n|    0   N/A  N/A     85889      C   tritonserver                               2824MiB |\n|    0   N/A  N/A     88253      C   tritonserver                               2824MiB |\n|    0   N/A  N/A     91194      C   tritonserver                               4546MiB |\n+---------------------------------------------------------------------------------------+\n</code></pre> </li> <li> <p>Run the command <code>docker ps</code>. You should see output similar to the following. Confirm that the status of the containers is <code>Up</code>.</p> <pre><code>CONTAINER ID  IMAGE                                            COMMAND                 CREATED         STATUS                  PORTS            NAMES\n1b885f37c991  nvcr.io/nvidia/nemo-microservices/nv-ingest:...  \"/opt/conda/envs/nv_\u2026\"  3 minutes ago   Up 3 minutes (healthy)  0.0.0.0:7670...  nv-ingest-nv-ingest-ms-runtime-1\n62c6b999c413  zilliz/attu:...                                  \"docker-entrypoint.s\u2026\"  13 minutes ago  Up 3 minutes            0.0.0.0:3001...  milvus-attu\n14ef31ed7f49  milvusdb/milvus:...                              \"/tini -- milvus run\u2026\"  13 minutes ago  Up 3 minutes (healthy)  0.0.0.0:9091...  milvus-standalone\ndceaf36cc5df  otel/opentelemetry-collector-contrib:...         \"/otelcol-contrib --\u2026\"  13 minutes ago  Up 3 minutes            0.0.0.0:4317...  nv-ingest-otel-collector-1\nfb252020e4d2  nvcr.io/nvidia/nim/nemoretriever-graphic-ele...  \"/opt/nim/start_serv\u2026\"  13 minutes ago  Up 3 minutes            0.0.0.0:8003...  nv-ingest-graphic-elements-1\nc944a9d76831  nvcr.io/nvidia/nim/paddleocr:...                 \"/opt/nim/start_serv\u2026\"  13 minutes ago  Up 3 minutes            0.0.0.0:8009...  nv-ingest-paddle-1\n5bea344526a2  nvcr.io/nvidia/nim/nemoretriever-page-elements   \"/opt/nim/start_serv\u2026\"  13 minutes ago  Up 3 minutes            0.0.0.0:8000...  nv-ingest-page-elements-1\n16dc2311a6cc  nvcr.io/nvidia/nim/llama-3.2-nv-embedqa...       \"/opt/nim/start_serv\u2026\"  13 minutes ago  Up 3 minutes            0.0.0.0:8012...  nv-ingest-embedding-1\ncea3ce001888  nvcr.io/nvidia/nim/nemoretriever-table-struc...  \"/opt/nim/start_serv\u2026\"  13 minutes ago  Up 3 minutes            0.0.0.0:8006...  nv-ingest-table-structure-1\n7ddbf7690036  openzipkin/zipkin                                \"start-zipkin\"          13 minutes ago  Up 3 minutes (healthy)  9410/tcp...      nv-ingest-zipkin-1\nb73bbe0c202d  minio/minio:RELEASE...                           \"/usr/bin/docker-ent\u2026\"  13 minutes ago  Up 3 minutes (healthy)  0.0.0.0:9000...  minio\n97fa798dbe4f  prom/prometheus:latest                           \"/bin/prometheus --w\u2026\"  13 minutes ago  Up 3 minutes            0.0.0.0:9090...  nv-ingest-prometheus-1\nf17cb556b086  grafana/grafana                                  \"/run.sh\"               13 minutes ago  Up 3 minutes            0.0.0.0:3000...  grafana-service\n3403c5a0e7be  redis/redis-stack                                \"/entrypoint.sh\"        13 minutes ago  Up 3 minutes            0.0.0.0:6379...  nv-ingest-redis-1\n</code></pre> </li> </ol>"},{"location":"extraction/quickstart-guide/#step-2-install-python-dependencies","title":"Step 2: Install Python Dependencies","text":"<p>You can interact with the NV-Ingest service from the host, or by using <code>docker exec</code> to run commands in the NV-Ingest container.</p> <p>To interact from the host, you'll need a Python environment and install the client dependencies:</p> <pre><code># conda not required but makes it easy to create a fresh Python environment\nconda create --name nv-ingest-dev python=3.12.11\nconda activate nv-ingest-dev\npip install nv-ingest==25.9.0 nv-ingest-api==25.9.0 nv-ingest-client==25.9.0\n</code></pre> <p>Tip</p> <p>To confirm that you have activated your Conda environment, run <code>which pip</code> and <code>which python</code>, and confirm that you see <code>nvingest</code> in the result. You can do this before any pip or python command that you run.</p> <p>Note</p> <p>Interacting from the host depends on the appropriate port being exposed from the nv-ingest container to the host as defined in docker-compose.yaml. If you prefer, you can disable exposing that port and interact with the NV-Ingest service directly from within its container. To interact within the container run <code>docker exec -it nv-ingest-nv-ingest-ms-runtime-1 bash</code>. You'll be in the <code>/workspace</code> directory with <code>DATASET_ROOT</code> from the .env file mounted at <code>./data</code>. The pre-activated <code>nv_ingest_runtime</code> conda environment has all the Python client libraries pre-installed and you should see <code>(nv_ingest_runtime) root@aba77e2a4bde:/workspace#</code>. From the bash prompt above, you can run the nv-ingest-cli and Python examples described following.</p>"},{"location":"extraction/quickstart-guide/#step-3-ingesting-documents","title":"Step 3: Ingesting Documents","text":"<p>You can submit jobs programmatically in Python or using the NV-Ingest CLI.</p> <p>In the following examples, we do text, chart, table, and image extraction.</p> <ul> <li>extract_text \u2014 Uses PDFium to find and extract text from pages.</li> <li>extract_images \u2014 Uses PDFium to extract images.</li> <li>extract_tables \u2014 Uses object detection family of NIMs to find tables and charts, and either PaddleOCR NIM or NemoRetriever OCR for table extraction.</li> <li>extract_charts \u2014 Enables or disables chart extraction, also based on the object detection NIM family.</li> </ul>"},{"location":"extraction/quickstart-guide/#in-python","title":"In Python","text":"<p>Tip</p> <p>For more Python examples, refer to NV-Ingest: Python Client Quick Start Guide.</p> <pre><code>import logging, os, time\nfrom nv_ingest_client.client import Ingestor, NvIngestClient\nfrom nv_ingest_client.util.process_json_files import ingest_json_results_to_blob\nclient = NvIngestClient(                                                                         \n    message_client_port=7670,                                                               \n    message_client_hostname=\"localhost\"        \n)                                                                 \n# do content extraction from files                               \ningestor = (\n    Ingestor(client=client)\n    .files(\"data/multimodal_test.pdf\")\n    .extract(             \n        extract_text=True,\n        extract_tables=True,\n        extract_charts=True,\n        extract_images=True,\n        table_output_format=\"markdown\",\n        extract_infographics=True,\n        # extract_method=\"nemoretriever_parse\", # Slower, but maximally accurate, especially for PDFs with pages that are scanned images\n        text_depth=\"page\"\n    ).embed()\n    .vdb_upload(\n        collection_name=\"test\",\n        sparse=False,\n        # for llama-3.2 embedder, use 1024 for e5-v5\n        dense_dim=2048\n    )\n)\n\nprint(\"Starting ingestion..\")\nt0 = time.time()\n\n# Return both successes and failures\n# Use for large batches where you want successful chunks/pages to be committed, while collecting detailed diagnostics for failures.\nresults, failures = ingestor.ingest(show_progress=True, return_failures=True)\n\n# Return only successes\n# results = ingestor.ingest(show_progress=True)\n\nt1 = time.time()\nprint(f\"Total time: {t1-t0} seconds\")\n\n# results blob is directly inspectable\nprint(ingest_json_results_to_blob(results[0]))\n\nif failures:\n    print(f\"There were {len(failures)} failures. Sample: {failures[0]}\")\n</code></pre> <p>Note</p> <p>To use library mode with nemoretriever_parse, uncomment <code>extract_method=\"nemoretriever_parse\"</code> in the previous code. For more information, refer to Use Nemo Retriever Extraction with nemoretriever-parse.</p> <p>The output looks similar to the following.</p> <pre><code>Starting ingestion..\n1 records to insert to milvus\nlogged 8 records\nTotal time: 5.479151725769043 seconds\nThis chart shows some gadgets, and some very fictitious costs. Gadgets and their cost   Chart 1 - Hammer - Powerdrill - Bluetooth speaker - Minifridge - Premium desk fan Dollars $- - $20.00 - $40.00 - $60.00 - $80.00 - $100.00 - $120.00 - $140.00 - $160.00 Cost\nTable 1\n| This table describes some animals, and some activities they might be doing in specific locations. | This table describes some animals, and some activities they might be doing in specific locations. | This table describes some animals, and some activities they might be doing in specific locations. |\n| Animal | Activity | Place |\n| Giraffe | Driving a car | At the beach |\n| Lion | Putting on sunscreen | At the park |\n| Cat | Jumping onto a laptop | In a home office |\n| Dog | Chasing a squirrel | In the front yard |\nTestingDocument\nA sample document with headings and placeholder text\nIntroduction\nThis is a placeholder document that can be used for any purpose. It contains some \nheadings and some placeholder text to fill the space. The text is not important and contains \nno real value, but it is useful for testing. Below, we will have some simple tables and charts \nthat we can use to confirm Ingest is working as expected.\nTable 1\nThis table describes some animals, and some activities they might be doing in specific \nlocations.\nAnimal Activity Place\nGira@e Driving a car At the beach\nLion Putting on sunscreen At the park\nCat Jumping onto a laptop In a home o@ice\nDog Chasing a squirrel In the front yard\nChart 1\nThis chart shows some gadgets, and some very fictitious costs.\nimage_caption:[]\nimage_caption:[]\nBelow,is a high-quality picture of some shapes          Picture\nTable 2\n| This table shows some popular colors that cars might come in | This table shows some popular colors that cars might come in | This table shows some popular colors that cars might come in | This table shows some popular colors that cars might come in |\n| Car | Color1 | Color2 | Color3 |\n| Coupe | White | Silver | Flat Gray |\n| Sedan | White | Metallic Gray | Matte Gray |\n| Minivan | Gray | Beige | Black |\n| Truck | Dark Gray | Titanium Gray | Charcoal |\n| Convertible | Light Gray | Graphite | Slate Gray |\nSection One\nThis is the first section of the document. It has some more placeholder text to show how \nthe document looks like. The text is not meant to be meaningful or informative, but rather to \ndemonstrate the layout and formatting of the document.\n\u2022 This is the first bullet point\n\u2022 This is the second bullet point\n\u2022 This is the third bullet point\nSection Two\nThis is the second section of the document. It is more of the same as we\u2019ve seen in the rest \nof the document. The content is meaningless, but the intent is to create a very simple \nsmoke test to ensure extraction is working as intended. This will be used in CI as time goes \non to ensure that changes we make to the library do not negatively impact our accuracy.\nTable 2\nThis table shows some popular colors that cars might come in.\nCar Color1 Color2 Color3\nCoupe White Silver Flat Gray\nSedan White Metallic Gray Matte Gray\nMinivan Gray Beige Black\nTruck Dark Gray Titanium Gray Charcoal\nConvertible Light Gray Graphite Slate Gray\nPicture\nBelow, is a high-quality picture of some shapes.\nimage_caption:[]\nimage_caption:[]\nThis chart shows some average frequency ranges for speaker drivers. Frequency Ranges ofSpeaker Drivers   Tweeter - Midrange - Midwoofer - Subwoofer Chart2 Hertz (log scale) 1 - 10 - 100 - 1000 - 10000 - 100000 FrequencyRange Start (Hz) - Frequency Range End (Hz)\nChart 2\nThis chart shows some average frequency ranges for speaker drivers.\nConclusion\nThis is the conclusion of the document. It has some more placeholder text, but the most \nimportant thing is that this is the conclusion. As we end this document, we should have \nbeen able to extract 2 tables, 2 charts, and some text including 3 bullet points.\nimage_caption:[]\n</code></pre>"},{"location":"extraction/quickstart-guide/#using-the-nv-ingest-cli","title":"Using the <code>nv-ingest-cli</code>","text":"<p>Tip</p> <p>There is a Jupyter notebook available to help you get started with the CLI. For more information, refer to CLI Client Quick Start Guide.</p> <pre><code>nv-ingest-cli \\\n  --doc ./data/multimodal_test.pdf \\\n  --output_directory ./processed_docs \\\n  --task='extract:{\"document_type\": \"pdf\", \"extract_method\": \"pdfium\", \"extract_tables\": \"true\", \"extract_images\": \"true\", \"extract_charts\": \"true\"}' \\\n  --client_host=localhost \\\n  --client_port=7670\n</code></pre> <p>You should see output that indicates the document processing status followed by a breakdown of time spent during job execution.</p> <pre><code>None of PyTorch, TensorFlow &gt;= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n[nltk_data] Downloading package punkt_tab to\n[nltk_data]     /raid/jdyer/miniforge3/envs/nv-ingest-\n[nltk_data]     dev/lib/python3.10/site-\n[nltk_data]     packages/llama_index/core/_static/nltk_cache...\n[nltk_data]   Package punkt_tab is already up-to-date!\nINFO:nv_ingest_client.nv_ingest_cli:Processing 1 documents.\nINFO:nv_ingest_client.nv_ingest_cli:Output will be written to: ./processed_docs\nProcessing files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:02&lt;00:00,  2.34s/file, pages_per_sec=1.28]\nINFO:nv_ingest_client.cli.util.processing:message_broker_task_source: Avg: 2.39 ms, Median: 2.39 ms, Total Time: 2.39 ms, Total % of Trace Computation: 0.06%\nINFO:nv_ingest_client.cli.util.processing:broker_source_network_in: Avg: 9.51 ms, Median: 9.51 ms, Total Time: 9.51 ms, Total % of Trace Computation: 0.25%\nINFO:nv_ingest_client.cli.util.processing:job_counter: Avg: 1.47 ms, Median: 1.47 ms, Total Time: 1.47 ms, Total % of Trace Computation: 0.04%\nINFO:nv_ingest_client.cli.util.processing:job_counter_channel_in: Avg: 0.46 ms, Median: 0.46 ms, Total Time: 0.46 ms, Total % of Trace Computation: 0.01%\nINFO:nv_ingest_client.cli.util.processing:metadata_injection: Avg: 3.52 ms, Median: 3.52 ms, Total Time: 3.52 ms, Total % of Trace Computation: 0.09%\nINFO:nv_ingest_client.cli.util.processing:metadata_injection_channel_in: Avg: 0.16 ms, Median: 0.16 ms, Total Time: 0.16 ms, Total % of Trace Computation: 0.00%\nINFO:nv_ingest_client.cli.util.processing:pdf_content_extractor: Avg: 475.64 ms, Median: 163.77 ms, Total Time: 2378.21 ms, Total % of Trace Computation: 62.73%\nINFO:nv_ingest_client.cli.util.processing:pdf_content_extractor_channel_in: Avg: 0.31 ms, Median: 0.31 ms, Total Time: 0.31 ms, Total % of Trace Computation: 0.01%\nINFO:nv_ingest_client.cli.util.processing:image_content_extractor: Avg: 0.67 ms, Median: 0.67 ms, Total Time: 0.67 ms, Total % of Trace Computation: 0.02%\nINFO:nv_ingest_client.cli.util.processing:image_content_extractor_channel_in: Avg: 0.21 ms, Median: 0.21 ms, Total Time: 0.21 ms, Total % of Trace Computation: 0.01%\nINFO:nv_ingest_client.cli.util.processing:docx_content_extractor: Avg: 0.46 ms, Median: 0.46 ms, Total Time: 0.46 ms, Total % of Trace Computation: 0.01%\nINFO:nv_ingest_client.cli.util.processing:docx_content_extractor_channel_in: Avg: 0.20 ms, Median: 0.20 ms, Total Time: 0.20 ms, Total % of Trace Computation: 0.01%\nINFO:nv_ingest_client.cli.util.processing:pptx_content_extractor: Avg: 0.68 ms, Median: 0.68 ms, Total Time: 0.68 ms, Total % of Trace Computation: 0.02%\nINFO:nv_ingest_client.cli.util.processing:pptx_content_extractor_channel_in: Avg: 0.46 ms, Median: 0.46 ms, Total Time: 0.46 ms, Total % of Trace Computation: 0.01%\nINFO:nv_ingest_client.cli.util.processing:audio_data_extraction: Avg: 1.08 ms, Median: 1.08 ms, Total Time: 1.08 ms, Total % of Trace Computation: 0.03%\nINFO:nv_ingest_client.cli.util.processing:audio_data_extraction_channel_in: Avg: 0.20 ms, Median: 0.20 ms, Total Time: 0.20 ms, Total % of Trace Computation: 0.01%\nINFO:nv_ingest_client.cli.util.processing:dedup_images: Avg: 0.42 ms, Median: 0.42 ms, Total Time: 0.42 ms, Total % of Trace Computation: 0.01%\nINFO:nv_ingest_client.cli.util.processing:dedup_images_channel_in: Avg: 0.42 ms, Median: 0.42 ms, Total Time: 0.42 ms, Total % of Trace Computation: 0.01%\nINFO:nv_ingest_client.cli.util.processing:filter_images: Avg: 0.59 ms, Median: 0.59 ms, Total Time: 0.59 ms, Total % of Trace Computation: 0.02%\nINFO:nv_ingest_client.cli.util.processing:filter_images_channel_in: Avg: 0.57 ms, Median: 0.57 ms, Total Time: 0.57 ms, Total % of Trace Computation: 0.02%\nINFO:nv_ingest_client.cli.util.processing:table_data_extraction: Avg: 240.75 ms, Median: 240.75 ms, Total Time: 481.49 ms, Total % of Trace Computation: 12.70%\nINFO:nv_ingest_client.cli.util.processing:table_data_extraction_channel_in: Avg: 0.38 ms, Median: 0.38 ms, Total Time: 0.38 ms, Total % of Trace Computation: 0.01%\nINFO:nv_ingest_client.cli.util.processing:chart_data_extraction: Avg: 300.54 ms, Median: 299.94 ms, Total Time: 901.62 ms, Total % of Trace Computation: 23.78%\nINFO:nv_ingest_client.cli.util.processing:chart_data_extraction_channel_in: Avg: 0.23 ms, Median: 0.23 ms, Total Time: 0.23 ms, Total % of Trace Computation: 0.01%\nINFO:nv_ingest_client.cli.util.processing:infographic_data_extraction: Avg: 0.77 ms, Median: 0.77 ms, Total Time: 0.77 ms, Total % of Trace Computation: 0.02%\nINFO:nv_ingest_client.cli.util.processing:infographic_data_extraction_channel_in: Avg: 0.25 ms, Median: 0.25 ms, Total Time: 0.25 ms, Total % of Trace Computation: 0.01%\nINFO:nv_ingest_client.cli.util.processing:caption_ext: Avg: 0.55 ms, Median: 0.55 ms, Total Time: 0.55 ms, Total % of Trace Computation: 0.01%\nINFO:nv_ingest_client.cli.util.processing:caption_ext_channel_in: Avg: 0.51 ms, Median: 0.51 ms, Total Time: 0.51 ms, Total % of Trace Computation: 0.01%\nINFO:nv_ingest_client.cli.util.processing:embed_text: Avg: 1.21 ms, Median: 1.21 ms, Total Time: 1.21 ms, Total % of Trace Computation: 0.03%\nINFO:nv_ingest_client.cli.util.processing:embed_text_channel_in: Avg: 0.21 ms, Median: 0.21 ms, Total Time: 0.21 ms, Total % of Trace Computation: 0.01%\nINFO:nv_ingest_client.cli.util.processing:store_embedding_minio: Avg: 0.32 ms, Median: 0.32 ms, Total Time: 0.32 ms, Total % of Trace Computation: 0.01%\nINFO:nv_ingest_client.cli.util.processing:store_embedding_minio_channel_in: Avg: 1.18 ms, Median: 1.18 ms, Total Time: 1.18 ms, Total % of Trace Computation: 0.03%\nINFO:nv_ingest_client.cli.util.processing:message_broker_task_sink_channel_in: Avg: 0.42 ms, Median: 0.42 ms, Total Time: 0.42 ms, Total % of Trace Computation: 0.01%\nINFO:nv_ingest_client.cli.util.processing:No unresolved time detected. Trace times account for the entire elapsed duration.\nINFO:nv_ingest_client.cli.util.processing:Processed 1 files in 2.34 seconds.\nINFO:nv_ingest_client.cli.util.processing:Total pages processed: 3\nINFO:nv_ingest_client.cli.util.processing:Throughput (Pages/sec): 1.28\nINFO:nv_ingest_client.cli.util.processing:Throughput (Files/sec): 0.43\n</code></pre>"},{"location":"extraction/quickstart-guide/#step-4-inspecting-and-consuming-results","title":"Step 4: Inspecting and Consuming Results","text":"<p>After the ingestion steps above have been completed, you should be able to find the <code>text</code> and <code>image</code> subfolders inside your processed docs folder. Each will contain JSON-formatted extracted content and metadata.</p> <p>When processing has completed, you'll have separate result files for text and image data: <pre><code>ls -R processed_docs/\n</code></pre> <pre><code>processed_docs/:\nimage  structured  text\n\nprocessed_docs/image:\nmultimodal_test.pdf.metadata.json\n\nprocessed_docs/structured:\nmultimodal_test.pdf.metadata.json\n\nprocessed_docs/text:\nmultimodal_test.pdf.metadata.json\n</code></pre></p> <p>For the full metadata definitions, refer to Content Metadata. </p> <p>We also provide a script for inspecting extracted images.</p> <p>First, install <code>tkinter</code> by running the following code. Choose the code for your OS.</p> <ul> <li> <p>For Ubuntu/Debian Linux:</p> <pre><code>sudo apt-get update\nsudo apt-get install python3-tk\n</code></pre> </li> <li> <p>For Fedora/RHEL Linux:</p> <pre><code>sudo dnf install python3-tkinter\n</code></pre> </li> <li> <p>For macOS using Homebrew:</p> <pre><code>brew install python-tk\n</code></pre> </li> </ul> <p>Then, run the following command to execute the script for inspecting the extracted image:</p> <pre><code>python src/util/image_viewer.py --file_path ./processed_docs/image/multimodal_test.pdf.metadata.json\n</code></pre> <p>Tip</p> <p>Beyond inspecting the results, you can read them into things like llama-index or langchain retrieval pipelines. Also, checkout our demo using a retrieval pipeline on build.nvidia.com to query over document content pre-extracted with NV-Ingest.</p>"},{"location":"extraction/quickstart-guide/#profile-information","title":"Profile Information","text":"<p>The values that you specify in the <code>--profile</code> option of your <code>docker compose up</code> command are explained in the following table.  You can specify multiple <code>--profile</code> options.</p> Profile Type Description <code>retrieval</code> Core Enables the embedding NIM and (GPU accelerated) Milvus. <code>table-structure</code> Core Enables the yolox table structure NIM which enhances markdown formatting of extracted table content. This benefits answer generation by downstream LLMs. <code>audio</code> Advanced Use Riva for processing audio files. For more information, refer to Audio Processing. <code>nemoretriever-parse</code> Advanced Use nemoretriever-parse, which adds state-of-the-art text and table extraction. For more information, refer to Use Nemo Retriever Extraction with nemoretriever-parse. <code>vlm</code> Advanced Use llama 3.1 Nemotron 8B Vision for experimental image captioning of unstructured images. <p>Important</p> <p>Advanced features require additional GPU support and disk space. For more information, refer to Support Matrix.</p>"},{"location":"extraction/quickstart-guide/#related-topics","title":"Related Topics","text":"<ul> <li>Troubleshoot</li> <li>Prerequisites</li> <li>Support Matrix</li> <li>Deploy Without Containers (Library Mode)</li> <li>Deploy With Helm</li> <li>Notebooks</li> <li>Multimodal PDF Data Extraction</li> </ul>"},{"location":"extraction/quickstart-library-mode/","title":"Deploy Without Containers (Library Mode) for NeMo Retriever Extraction","text":"<p>NeMo Retriever extraction is typically deployed as a cluster of containers for robust, scalable production use. </p> <p>Note</p> <p>NeMo Retriever extraction is also known as NVIDIA Ingest and nv-ingest.</p> <p>In addition, you can use library mode, which is intended for the following cases:</p> <ul> <li>Local development</li> <li>Experimentation and testing</li> <li>Small-scale workloads, such as workloads of fewer than 100 documents</li> </ul> <p>By default, library mode depends on NIMs that are hosted on build.nvidia.com.  In library mode you launch the main pipeline service directly within a Python process,  while all other services (such as embedding and storage) are hosted remotely in the cloud.</p> <p>To get started using library mode, you need the following:</p> <ul> <li>Linux operating systems (Ubuntu 22.04 or later recommended)</li> <li>Python 3.12</li> <li>We strongly advise using an isolated Python virtual env, such as provided by uv or conda</li> </ul>"},{"location":"extraction/quickstart-library-mode/#step-1-prepare-your-environment","title":"Step 1: Prepare Your Environment","text":"<p>Use the following procedure to prepare your environment.</p> <ol> <li> <p>Run the following code to create your NV Ingest Conda environment.</p> <pre><code>   uv venv --python 3.12 nvingest &amp;&amp; \\\n     source nvingest/bin/activate &amp;&amp; \\\n     uv pip install nv-ingest==25.9.0 nv-ingest-api==25.9.0 nv-ingest-client==25.9.0 milvus-lite==2.4.12\n</code></pre> <p>Tip</p> <p>To confirm that you have activated your Conda environment, run <code>which python</code> and confirm that you see <code>nvingest</code> in the result. You can do this before any python command that you run.</p> </li> <li> <p>Set or create a .env file that contains your NVIDIA Build API key and other environment variables.</p> <p>Note</p> <p>If you have an NGC API key, you can use it here. For more information, refer to Generate Your NGC Keys and Environment Configuration Variables.</p> <ul> <li> <p>To set your variables, use the following code.</p> <p><pre><code>export NVIDIA_API_KEY=nvapi-&lt;your key&gt;\n</code></pre>     - To add your variables to a .env file, include the following.</p> <pre><code>NVIDIA_API_KEY=nvapi-&lt;your key&gt;\n</code></pre> </li> </ul> </li> </ol>"},{"location":"extraction/quickstart-library-mode/#step-2-ingest-documents","title":"Step 2: Ingest Documents","text":"<p>You can submit jobs programmatically by using Python.</p> <p>Tip</p> <p>For more Python examples, refer to NV-Ingest: Python Client Quick Start Guide.</p> <p>If you have a very high number of CPUs, and see the process hang without progress,  we recommend that you use <code>taskset</code> to limit the number of CPUs visible to the process.  Use the following code.</p> <pre><code>taskset -c 0-3 python your_ingestion_script.py\n</code></pre> <p>On a 4 CPU core low end laptop, the following code should take about 10 seconds.</p> <pre><code>import logging, os, time, sys\n\nfrom nv_ingest.framework.orchestration.ray.util.pipeline.pipeline_runners import run_pipeline\nfrom nv_ingest.framework.orchestration.ray.util.pipeline.pipeline_runners import PipelineCreationSchema\nfrom nv_ingest_api.util.logging.configuration import configure_logging as configure_local_logging\nfrom nv_ingest_client.client import Ingestor, NvIngestClient\nfrom nv_ingest_api.util.message_brokers.simple_message_broker import SimpleClient\nfrom nv_ingest_client.util.process_json_files import ingest_json_results_to_blob\n\n# Start the pipeline subprocess for library mode\nconfig = PipelineCreationSchema()\n\nrun_pipeline(config, block=False, disable_dynamic_scaling=True, run_in_subprocess=True)\n\nclient = NvIngestClient(\n    message_client_allocator=SimpleClient,\n    message_client_port=7671,\n    message_client_hostname=\"localhost\"\n)\n\n# gpu_cagra accelerated indexing is not available in milvus-lite\n# Provide a filename for milvus_uri to use milvus-lite\nmilvus_uri = \"milvus.db\"\ncollection_name = \"test\"\nsparse = False\n\n# do content extraction from files                                \ningestor = (\n    Ingestor(client=client)\n    .files(\"data/multimodal_test.pdf\")\n    .extract(\n        extract_text=True,\n        extract_tables=True,\n        extract_charts=True,\n        extract_images=True,\n        table_output_format=\"markdown\",\n        extract_infographics=True,\n        # Slower, but maximally accurate, especially for PDFs with pages that are scanned images\n        # extract_method=\"nemoretriever_parse\",\n        text_depth=\"page\"\n    ).embed()\n    .vdb_upload(\n        collection_name=collection_name,\n        milvus_uri=milvus_uri,\n        sparse=sparse,\n        # for llama-3.2 embedder, use 1024 for e5-v5\n        dense_dim=2048\n    )\n)\n\nprint(\"Starting ingestion..\")\nt0 = time.time()\n\n# Return both successes and failures\n# Use for large batches where you want successful chunks/pages to be committed, while collecting detailed diagnostics for failures.\nresults, failures = ingestor.ingest(show_progress=True, return_failures=True)\n\n# Return only successes\n# results = ingestor.ingest(show_progress=True)\n\nt1 = time.time()\nprint(f\"Total time: {t1 - t0} seconds\")\n\n# results blob is directly inspectable\nprint(ingest_json_results_to_blob(results[0]))\n\nif failures:\n    print(f\"There were {len(failures)} failures. Sample: {failures[0]}\")\n</code></pre> <p>Note</p> <p>To use library mode with nemoretriever_parse, uncomment <code>extract_method=\"nemoretriever_parse\"</code> in the previous code. For more information, refer to Use Nemo Retriever Extraction with nemoretriever-parse.</p> <p>You can see the extracted text that represents the content of the ingested test document.</p> <pre><code>Starting ingestion..\nTotal time: 9.243880033493042 seconds\n\nTestingDocument\nA sample document with headings and placeholder text\nIntroduction\nThis is a placeholder document that can be used for any purpose. It contains some \nheadings and some placeholder text to fill the space. The text is not important and contains \nno real value, but it is useful for testing. Below, we will have some simple tables and charts \nthat we can use to confirm Ingest is working as expected.\nTable 1\nThis table describes some animals, and some activities they might be doing in specific \nlocations.\nAnimal Activity Place\nGira@e Driving a car At the beach\nLion Putting on sunscreen At the park\nCat Jumping onto a laptop In a home o@ice\nDog Chasing a squirrel In the front yard\nChart 1\nThis chart shows some gadgets, and some very fictitious costs.\n\n... document extract continues ...\n</code></pre>"},{"location":"extraction/quickstart-library-mode/#step-3-query-ingested-content","title":"Step 3: Query Ingested Content","text":"<p>To query for relevant snippets of the ingested content, and use them with an LLM to generate answers, use the following code.</p> <pre><code>from openai import OpenAI\nfrom nv_ingest_client.util.milvus import nvingest_retrieval\nimport os\n\nmilvus_uri = \"milvus.db\"\ncollection_name = \"test\"\nsparse=False\n\nqueries = [\"Which animal is responsible for the typos?\"]\n\nretrieved_docs = nvingest_retrieval(\n    queries,\n    collection_name,\n    milvus_uri=milvus_uri,\n    hybrid=sparse,\n    top_k=1,\n)\n\n# simple generation example\nextract = retrieved_docs[0][0][\"entity\"][\"text\"]\nclient = OpenAI(\n  base_url = \"https://integrate.api.nvidia.com/v1\",\n  api_key = os.environ[\"NVIDIA_API_KEY\"]\n)\n\nprompt = f\"Using the following content: {extract}\\n\\n Answer the user query: {queries[0]}\"\nprint(f\"Prompt: {prompt}\")\ncompletion = client.chat.completions.create(\n  model=\"nvidia/llama-3.1-nemotron-70b-instruct\",\n  messages=[{\"role\":\"user\",\"content\": prompt}],\n)\nresponse = completion.choices[0].message.content\n\nprint(f\"Answer: {response}\")\n</code></pre> <pre><code>Prompt: Using the following content: TestingDocument\nA sample document with headings and placeholder text\nIntroduction\nThis is a placeholder document that can be used for any purpose. It contains some \nheadings and some placeholder text to fill the space. The text is not important and contains \nno real value, but it is useful for testing. Below, we will have some simple tables and charts \nthat we can use to confirm Ingest is working as expected.\nTable 1\nThis table describes some animals, and some activities they might be doing in specific \nlocations.\nAnimal Activity Place\nGira@e Driving a car At the beach\nLion Putting on sunscreen At the park\nCat Jumping onto a laptop In a home o@ice\nDog Chasing a squirrel In the front yard\nChart 1\nThis chart shows some gadgets, and some very fictitious costs.\n\n Answer the user query: Which animal is responsible for the typos?\nAnswer: A clever query!\n\nAfter carefully examining the provided content, I'd like to point out the potential \"typos\" (assuming you're referring to the unusual or intentionally incorrect text) and attempt to playfully \"assign blame\" to an animal based on the context:\n\n1. **Gira@e** (instead of Giraffe) - **Animal blamed: Giraffe** (Table 1, first row)\n    * The \"@\" symbol in \"Gira@e\" suggests a possible typo or placeholder character, which we'll humorously attribute to the Giraffe's alleged carelessness.\n2. **o@ice** (instead of Office) - **Animal blamed: Cat**\n    * The same \"@\" symbol appears in \"o@ice\", which is related to the Cat's activity in the same table. Perhaps the Cat was in a hurry while typing and introduced the error?\n\nSo, according to this whimsical analysis, both the **Giraffe** and the **Cat** are \"responsible\" for the typos, with the Giraffe possibly being the more egregious offender given the more blatant character substitution in its name.\n</code></pre>"},{"location":"extraction/quickstart-library-mode/#library-mode-communication-and-advanced-examples","title":"Library Mode Communication and Advanced Examples","text":"<p>Communication in library mode is handled through a simplified, 3-way handshake message broker called <code>SimpleBroker</code>.</p> <p>Attempting to run a library-mode process co-located with a Docker Compose deployment does not work by default.  The Docker Compose deployment typically creates a firewall rule or port mapping that captures traffic to port <code>7671</code>, which prevents the <code>SimpleBroker</code> from receiving messages.  Always ensure that you use library mode in isolation, without an active containerized deployment listening on the same port.</p>"},{"location":"extraction/quickstart-library-mode/#example-launch_libmode_servicepy","title":"Example <code>launch_libmode_service.py</code>","text":"<p>This example launches the pipeline service in a subprocess,  and keeps it running until it is interrupted (for example, by pressing <code>Ctrl+C</code>).  It listens for ingestion requests on port <code>7671</code> from an external client.</p> <pre><code>def main():\n\n    config_data = {}\n    config_data = {key: value for key, value in config_data.items() if value is not None}\n    ingest_config = PipelineCreationSchema(**config_data)\n\n    try:\n        _ = run_pipeline(\n            ingest_config,\n            block=True,\n            disable_dynamic_scaling=True,\n            run_in_subprocess=True,\n        )\n    except KeyboardInterrupt:\n        logger.info(\"Keyboard interrupt received. Shutting down...\")\n    except Exception as e:\n        logger.error(f\"Error running pipeline: {e}\")\n</code></pre>"},{"location":"extraction/quickstart-library-mode/#example-launch_libmode_and_run_ingestorpy","title":"Example <code>launch_libmode_and_run_ingestor.py</code>","text":"<p>This example starts the pipeline service in-process,  and immediately runs an ingestion client against it in the same parent process.</p> <pre><code>def run_ingestor():\n\n    client = NvIngestClient(\n        message_client_allocator=SimpleClient,\n        message_client_port=7671,\n        message_client_hostname=\"localhost\"\n    )\n\n    ingestor = (\n        Ingestor(client=client)\n        .files(\"./data/multimodal_test.pdf\")\n        .extract(\n            extract_text=True,\n            extract_tables=True,\n            extract_charts=True,\n            extract_images=True,\n            paddle_output_format=\"markdown\",\n            extract_infographics=False,\n            text_depth=\"page\",\n        )\n        .split(chunk_size=1024, chunk_overlap=150)\n    )\n\n    try:\n        results, failures = ingestor.ingest(show_progress=False, return_failures=True)\n        logger.info(\"Ingestion completed successfully.\")\n        if failures:\n            logger.warning(f\"Ingestion completed with {len(failures)} failures:\")\n            for i, failure in enumerate(failures):\n                logger.warning(f\"  [{i}] {failure}\")\n    except Exception as e:\n        logger.error(f\"Ingestion failed: {e}\")\n        raise\n\n    print(\"\\nIngest done.\")\n    print(f\"Got {len(results)} results.\")\n\n\ndef main():\n\n    config_data = {}\n    config_data = {key: value for key, value in config_data.items() if value is not None}\n    ingest_config = PipelineCreationSchema(**config_data)\n\n    try:\n        pipeline = run_pipeline(\n            ingest_config,\n            block=False,\n            disable_dynamic_scaling=True,\n            run_in_subprocess=True,\n        )\n        time.sleep(10)\n        run_ingestor()\n    except KeyboardInterrupt:\n        logger.info(\"Keyboard interrupt received. Shutting down...\")\n    except Exception as e:\n        logger.error(f\"Error running pipeline: {e}\")\n    finally:\n        pipeline.stop()\n        logger.info(\"Shutting down pipeline...\")\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"extraction/quickstart-library-mode/#the-run_pipeline-function-reference","title":"The <code>run_pipeline</code> Function Reference","text":"<p>The <code>run_pipeline</code> function is the main entry point to start the Nemo Retriever Extraction pipeline.  It can run in-process or as a subprocess.</p> <p>The <code>run_pipeline</code> function accepts the following parameters.</p> Parameter Type Default Required? Description ingest_config PipelineCreationSchema \u2014 Yes A configuration object that specifies how the pipeline should be constructed. run_in_subprocess bool False Yes <code>True</code> to launch the pipeline in a separate Python subprocess. <code>False</code> to run in the current process. block bool True Yes <code>True</code> to run the pipeline synchronously. The function returns after it finishes. <code>False</code> to return an interface for external pipeline control. disable_dynamic_scaling bool None No <code>True</code> to disable autoscaling regardless of global settings. <code>None</code> to use the global default behavior. dynamic_memory_threshold float None No A value between <code>0.0</code> and <code>1.0</code>. If dynamic scaling is enabled, triggers autoscaling when memory usage crosses this threshold. stdout TextIO None No Redirect the subprocess <code>stdout</code> to a file or stream. If <code>None</code>, defaults to <code>/dev/null</code>. stderr TextIO None No Redirect subprocess <code>stderr</code> to a file or stream. If <code>None</code>, defaults to <code>/dev/null</code>. <p>The <code>run_pipeline</code> function returns the following values, depending on the parameters that you set:</p> <ul> <li>run_in_subprocess=False and block=True  \u2014 The function returns a <code>float</code> that represents the elapsed time in seconds.</li> <li>run_in_subprocess=False and block=False \u2014 The function returns a <code>RayPipelineInterface</code> object.</li> <li>run_in_subprocess=True  and block=True  \u2014 The function returns <code>0.0</code>.</li> <li>run_in_subprocess=True  and block=False \u2014 The function returns a <code>RayPipelineInterface</code> object.</li> </ul> <p>The <code>run_pipeline</code> throws the following errors:</p> <ul> <li>RuntimeError \u2014 A subprocess failed to start, or exited with error.</li> <li>Exception \u2014 Any other failure during pipeline setup or execution.</li> </ul>"},{"location":"extraction/quickstart-library-mode/#related-topics","title":"Related Topics","text":"<ul> <li>Prerequisites</li> <li>Support Matrix</li> <li>Deploy With Docker Compose (Self-Hosted)</li> <li>Deploy With Helm</li> <li>Notebooks</li> <li>Multimodal PDF Data Extraction</li> </ul>"},{"location":"extraction/ray-logging/","title":"Configure Ray Logging","text":"<p>NeMo Retriever extraction uses Ray for logging.  You can use environment variables for fine-grained control over Ray's logging behavior.  In addition, NeMo Retriever extraction provides preset configurations that you can use to quickly update Ray logging behavior.</p> <p>Important</p> <p>You must set environment variables before you initialize the pipeline, and you must restart the pipeline if you change variable values.</p>"},{"location":"extraction/ray-logging/#quick-start-use-preset-configurations","title":"Quick Start - Use Preset Configurations","text":"<p>To get started quickly, use one of the NeMo Retriever extraction package-level preset variables.  Run the code below that corresponds to your use case; production, development, or debugging.  The log levels are explained following.</p> <p>Tip</p> <p>After you set a preset configuration, you can also override individual variables.</p> <pre><code># Production deployment - minimal logging, maximum performance\nexport INGEST_RAY_LOG_LEVEL=PRODUCTION\n\n# Development work (default) - balanced logging\nexport INGEST_RAY_LOG_LEVEL=DEVELOPMENT  \n\n# Debugging issues - maximum logging and visibility\nexport INGEST_RAY_LOG_LEVEL=DEBUG\n</code></pre>"},{"location":"extraction/ray-logging/#production-log-level","title":"PRODUCTION Log Level","text":"<p>The <code>PRODUCTION</code> log level is optimized for production deployments with minimal logging overhead. </p> <ul> <li>Storage Limit \u2013 10GB total (1GB \u00d7 10 files)</li> <li>Performance Impact \u2013 ~5% CPU reduction, ~200MB memory savings in large clusters</li> </ul> <p>This log level uses the following settings:</p> <ul> <li>Log Level \u2013 ERROR only</li> <li>Log to Driver \u2013 Disabled (worker logs stay in worker files)</li> <li>Import Warnings \u2013 Disabled</li> <li>Usage Stats \u2013 Disabled  </li> <li>Storage \u2013 10GB total (1GB \u00d7 10 files)</li> <li>Deduplication \u2013 Enabled</li> <li>Encoding \u2013 TEXT</li> </ul>"},{"location":"extraction/ray-logging/#development-log-level","title":"DEVELOPMENT Log Level","text":"<p>The <code>DEVELOPMENT</code> log level is a balanced configuration for development work,  and is the default log level.</p> <ul> <li>Storage Limit \u2013 20GB total (1GB \u00d7 20 files)  </li> <li>Performance Impact \u2013 Balanced performance and visibility</li> </ul> <p>This log level uses the following settings:</p> <ul> <li>Log Level \u2013 INFO</li> <li>Log to Driver \u2013 Enabled</li> <li>Import Warnings \u2013 Enabled</li> <li>Usage Stats \u2013 Enabled</li> <li>Storage \u2013 20GB total (1GB \u00d7 20 files) </li> <li>Deduplication \u2013 Enabled</li> <li>Encoding \u2013 TEXT</li> </ul>"},{"location":"extraction/ray-logging/#debug-log-level","title":"DEBUG Log Level","text":"<p>The <code>DEBUG</code> log level provides maximum visibility for troubleshooting issues. </p> <ul> <li>Storage Limit \u2013 20GB total (512MB \u00d7 40 files)</li> <li>Performance Impact \u2013 ~10% CPU overhead for detailed logging, higher memory usage</li> </ul> <p>This log level uses the following settings:</p> <ul> <li>Log Level \u2013 DEBUG</li> <li>Log to Driver \u2013 Enabled</li> <li>Import Warnings \u2013 Enabled</li> <li>Usage Stats \u2013 Enabled</li> <li>Storage \u2013 20GB total (512MB \u00d7 40 files)</li> <li>Deduplication \u2013 Disabled (see all duplicate messages)</li> <li>Encoding \u2013 JSON with function names and line numbers</li> </ul>"},{"location":"extraction/ray-logging/#configuration-reference","title":"Configuration Reference","text":"<p>The following are the environment variables that you can set to control Ray logging behavior.  If you specify an invalid value, the variable reverts to the default value with a warning message.</p> Variable Type Description Valid Values Default <code>INGEST_RAY_LOG_LEVEL</code> NeMo Retriever extraction preset Set multiple Ray logging variables to optimize for specific use cases. <code>PRODUCTION</code>, <code>DEVELOPMENT</code>, <code>DEBUG</code> <code>DEVELOPMENT</code> <code>RAY_DEDUP_LOGS</code> Log flow control Specify whether to log multiple instances of repeated events or to combine into a single entry. 1 to combine repeated messages (for example, <code>[repeated 5x]</code>). <code>0</code>, <code>1</code> <code>1</code> <code>RAY_DISABLE_IMPORT_WARNING</code> Ray internal logging <code>1</code> to suppresses <code>Ray X.Y.Z started</code> message and other warnings during initialization. <code>0</code>, <code>1</code> <code>0</code> <code>RAY_LOG_TO_DRIVER</code> Log flow control <code>true</code>to log worker messages in the main process. <code>false</code> to log worker messages in worker log files. <code>true</code>, <code>false</code> <code>true</code> <code>RAY_LOGGING_ADDITIONAL_ATTRS</code> Core logging control Add Python logger fields like function names, line numbers to each log entry. Comma-separated list (empty) <code>RAY_LOGGING_ENCODING</code> Core logging control Specify the format for log messages. <code>TEXT</code>, <code>JSON</code> <code>TEXT</code> <code>RAY_LOGGING_LEVEL</code> Core logging control Specify what events to log. <code>DEBUG</code> to log all Ray internals. <code>WARNING</code> to log only significant events. <code>DEBUG</code>, <code>INFO</code>, <code>WARNING</code>, <code>ERROR</code>, <code>CRITICAL</code> <code>INFO</code> <code>RAY_LOGGING_ROTATE_BACKUP_COUNT</code> File rotation Specify the number of old log files retained. Total storage = (count + 1) \u00d7 file size. Integer <code>19</code> <code>RAY_LOGGING_ROTATE_BYTES</code> File rotation Specify the log file size before Ray creates a new log file. Use this to prevent unbounded disk usage. Bytes <code>1073741824</code> (1GB) <code>RAY_USAGE_STATS_ENABLED</code> Ray internal logging <code>1</code> to enable telemetry collection and related log messages. <code>0</code> to disable. <code>0</code>, <code>1</code> <code>1</code>"},{"location":"extraction/ray-logging/#configuration-examples","title":"Configuration Examples","text":""},{"location":"extraction/ray-logging/#use-a-preset-with-a-manual-override","title":"Use a Preset With A Manual Override","text":"<p>The following example uses the <code>DEVELOPMENT</code> preset and then overrides the <code>RAY_LOGGING_LEVEL</code> behavior.</p> <pre><code>export INGEST_RAY_LOG_LEVEL=DEVELOPMENT  # Use the DEVELOPMENT preset\nexport RAY_LOGGING_LEVEL=WARNING         # Override just the log level\n</code></pre>"},{"location":"extraction/ray-logging/#log-verbosity-control","title":"Log Verbosity Control","text":"<p>By default, Ray generates significant logging output.  The following example configures Ray to reduce log volume.</p> <pre><code>export RAY_DISABLE_IMPORT_WARNING=1  # Suppress Ray initialization warnings\nexport RAY_LOGGING_LEVEL=WARNING     # Suppress informational messages, show only warnings and errors\nexport RAY_LOG_TO_DRIVER=false       # Prevent worker logs from appearing in driver process output\n</code></pre>"},{"location":"extraction/ray-logging/#minimal-logging-legacy","title":"Minimal Logging (Legacy)","text":"<p>The following example minimizes logging.  Only critical errors are logged.  Worker logs are isolated.  This reduces log volume by approximately 95%.</p> <p>Tip</p> <p>You can achieve the same effect by setting the <code>INGEST_RAY_LOG_LEVEL</code> to <code>PRODUCTION</code>.</p> <pre><code>export RAY_LOGGING_LEVEL=ERROR\nexport RAY_LOG_TO_DRIVER=false\nexport RAY_DISABLE_IMPORT_WARNING=1\nexport RAY_DEDUP_LOGS=1\n</code></pre>"},{"location":"extraction/ray-logging/#structured-logging-for-analysis","title":"Structured Logging for Analysis","text":"<p>The following example results in machine-parseable JSON with metadata for log aggregation systems.</p> <pre><code>export INGEST_RAY_LOG_LEVEL=DEVELOPMENT\nexport RAY_LOGGING_ENCODING=JSON\nexport RAY_LOGGING_ADDITIONAL_ATTRS=name,funcName,lineno,thread,process\n</code></pre>"},{"location":"extraction/ray-logging/#set-custom-storage-limits","title":"Set Custom Storage Limits","text":"<p>The following example automatically cleans up files when logs exceed 5GB. The oldest files are removed first.</p> <pre><code># 5GB total log storage (500MB \u00d7 10 files)\nexport RAY_LOGGING_ROTATE_BYTES=524288000\nexport RAY_LOGGING_ROTATE_BACKUP_COUNT=9\n</code></pre>"},{"location":"extraction/ray-logging/#log-output-examples","title":"Log Output Examples","text":""},{"location":"extraction/ray-logging/#info-level-default","title":"INFO level (Default)","text":"<pre><code>2024-01-15 10:30:15,123 INFO worker.py:1234 -- Task task_id=abc123 started\n2024-01-15 10:30:15,124 INFO worker.py:1235 -- Processing batch size=100\n2024-01-15 10:30:15,125 INFO worker.py:1236 -- Task task_id=abc123 completed\n</code></pre>"},{"location":"extraction/ray-logging/#warning-level","title":"WARNING level","text":"<pre><code>2024-01-15 10:30:20,456 WARNING worker.py:1240 -- Task retry attempt 2/3\n2024-01-15 10:30:25,789 ERROR worker.py:1245 -- Task failed: Connection timeout\n</code></pre>"},{"location":"extraction/ray-logging/#json-encoding-debug-preset","title":"JSON encoding (DEBUG preset)","text":"<pre><code>{\n    \"asctime\": \"2024-01-15 10:30:15,123\", \n    \"levelname\": \"INFO\", \n    \"filename\": \"worker.py\", \n    \"lineno\": 1234, \n    \"message\": \"Task started\", \n    \"name\": \"ray.worker\", \n    \"funcName\": \"execute_task\", \n    \"job_id\": \"01000000\", \n    \"worker_id\": \"abc123\", \n    \"task_id\": \"def456\"}\n</code></pre>"},{"location":"extraction/ray-logging/#related-topics","title":"Related Topics","text":"<ul> <li>Environment Variables</li> </ul>"},{"location":"extraction/releasenotes-nv-ingest/","title":"Release Notes for NeMo Retriever Extraction","text":"<p>This documentation contains the release notes for NeMo Retriever extraction.</p> <p>Note</p> <p>NeMo Retriever extraction is also known as NVIDIA Ingest and nv-ingest.</p>"},{"location":"extraction/releasenotes-nv-ingest/#release-2509-2590","title":"Release 25.09 (25.9.0)","text":"<p>The NeMo Retriever extraction 25.09 release adds new hardware and software support, and other improvements, including the following:</p> <ul> <li>Add functional support for RTX Pro 6000 Blackwell Server Edition.</li> <li>Add functional support for DGX B200.</li> <li>Add support for nemoretriever-ocr-v1. For details, refer to Deploy With Docker Compose (Self-Hosted) and NV-Ingest Helm Charts.</li> <li>Add support for llama-3.2-nemoretriever-1b-vlm-embed-v1.</li> <li>Add support for Llama Nemotron VLM 8b NIM for image captioning. For details, refer to Extract Captions from Images.</li> <li>Add support for custom vector database implementations. For details, refer to Build a Custom Vector Database Operator.</li> <li>Add support for custom Lambda stages.  For details, refer to Add User-defined Stages to Your NeMo Retriever Extraction Pipeline.</li> <li>Expanded documentation for Library Mode.</li> <li>New documentation Configure Ray Logging.</li> <li>New documentation Use Multimodal Embedding.</li> <li>Add support for Integer, float, boolean, and array in custom metadata during Milvus entity creation.</li> <li>Add support for running more than one VLM at a time by using Helm.  For details, refer to NV-Ingest Helm Charts.</li> </ul>"},{"location":"extraction/releasenotes-nv-ingest/#known-issues","title":"Known Issues","text":"<p>The following are the known issues for this release:</p> <ul> <li>A10G and L40S are not supported. To use A10G or L40S hardware, use release 25.6.x.</li> <li><code>nemoretriever-parse</code> is not supported on RTX Pro 6000 or B200. For details, refer to Support Matrix.</li> <li>The NeMo Retriever extraction pipeline does not support ingestion of batches that include individual files greater than approximately 400MB.</li> </ul>"},{"location":"extraction/releasenotes-nv-ingest/#breaking-changes","title":"Breaking Changes","text":"<p>There are no breaking changes in this version.</p>"},{"location":"extraction/releasenotes-nv-ingest/#upgrade","title":"Upgrade","text":"<p>To upgrade the Helm Charts for this version, refer to NV-Ingest Helm Charts.</p>"},{"location":"extraction/releasenotes-nv-ingest/#release-2563","title":"Release 25.6.3","text":"<p>The NeMo Retriever extraction 25.6.3 release is a patch release  that updates the client's dependency on pypdfium2 to the latest stable version.</p> <p>Only the release branch and the <code>nv-ingest-client</code> package have been updated in 25.6.3.  The previously released 25.6.2 container on NGC remains unchanged.</p>"},{"location":"extraction/releasenotes-nv-ingest/#known-issues_1","title":"Known Issues","text":"<p>The following are the known issues for this release:</p> <ul> <li>The NeMo Retriever extraction pipeline does not support ingestion of batches that include individual files greater than approximately 400MB.</li> </ul>"},{"location":"extraction/releasenotes-nv-ingest/#release-2562","title":"Release 25.6.2","text":"<p>The NeMo Retriever extraction 25.06 release focuses on accuracy improvements and feature expansions, including the following:</p> <ul> <li>Improve reranker accuracy.</li> <li>Upgrade Python version from 3.10 to 3.12</li> <li>Helm deployment now has similar throughput performance to docker deployment.</li> <li>Add support for the latest version of the OpenAI API.</li> <li>Add MIG support. For details, see Enable NVIDIA GPU MIG.</li> <li>Add time slicing support. For details, see Enable GPU time-slicing.</li> <li>Add support for RIVA NIM for optional audio extraction. For details, see helm/values.yaml.</li> <li>New notebook for How to add metadata to your documents and filter searches.</li> </ul>"},{"location":"extraction/releasenotes-nv-ingest/#known-issues_2","title":"Known Issues","text":"<p>The following are the known issues for this release:</p> <ul> <li>The NeMo Retriever extraction pipeline does not support ingestion of batches that include individual files greater than approximately 400MB.</li> </ul>"},{"location":"extraction/releasenotes-nv-ingest/#breaking-changes_1","title":"Breaking Changes","text":"<p>There are no breaking changes in this version.</p>"},{"location":"extraction/releasenotes-nv-ingest/#upgrade_1","title":"Upgrade","text":"<p>To upgrade the Helm Charts for this version, refer to NV-Ingest Helm Charts.</p>"},{"location":"extraction/releasenotes-nv-ingest/#release-2542","title":"Release 25.4.2","text":"<p>The NeMo Retriever extraction 25.04 release focuses on small bug fixes and improvements, including the following:</p> <ul> <li>Fixed a known issue where large text file ingestion failed.</li> <li>The REST service is now more resilient, and recovers from worker failures and connection errors.</li> <li>Various improvements on the client side to reduce retry rates, and improve overall quality of life.</li> <li>New notebook for How to reindex a collection.</li> <li>Expanded chunking documentation. For more information, refer to Split Documents.</li> </ul>"},{"location":"extraction/releasenotes-nv-ingest/#breaking-changes_2","title":"Breaking Changes","text":"<p>There are no breaking changes in this version.</p>"},{"location":"extraction/releasenotes-nv-ingest/#upgrade_2","title":"Upgrade","text":"<p>To upgrade the Helm Charts for this version, refer to NV-Ingest Helm Charts.</p>"},{"location":"extraction/releasenotes-nv-ingest/#release-2530","title":"Release 25.3.0","text":"<p>The NeMo Retriever extraction 25.03 release includes accuracy improvements, feature expansions, and throughput improvements.</p>"},{"location":"extraction/releasenotes-nv-ingest/#new-features","title":"New Features","text":"<ul> <li>Consolidated NeMo Retriever extraction to run on a single GPU (H100, A100, L40S, or A10G). For details, refer to Support Matrix.</li> <li>Added Library Mode for a lightweight no-GPU deployment that uses NIM endpoints hosted on build.nvidia.com. For details, refer to Deploy Without Containers (Library Mode).</li> <li>Added support for infographics extraction.</li> <li>Added support for RIVA NIM for Audio extraction (Early Access). For details, refer to Audio Processing.</li> <li>Added support for Llama-3.2 VLM for Image Captioning capability.</li> <li>docX, pptx, jpg, png support for image detection &amp; extraction.</li> <li>Deprecated DePlot and CACHED NIMs.</li> </ul>"},{"location":"extraction/releasenotes-nv-ingest/#release-24121","title":"Release 24.12.1","text":"<p>Cases where .split() tasks fail during ingestion are now fixed.</p>"},{"location":"extraction/releasenotes-nv-ingest/#release-2412","title":"Release 24.12","text":"<p>We currently do not support OCR-based text extraction. This was discovered in an unsupported use case and is not a product functionality issue.</p>"},{"location":"extraction/support-matrix/","title":"Support Matrix for NeMo Retriever Extraction","text":"<p>Before you begin using NeMo Retriever extraction, ensure that you have the hardware for your use case.</p> <p>Note</p> <p>NeMo Retriever extraction is also known as NVIDIA Ingest and nv-ingest.</p>"},{"location":"extraction/support-matrix/#core-and-advanced-pipeline-features","title":"Core and Advanced Pipeline Features","text":"<p>The Nemo Retriever extraction core pipeline features run on a single A10G or better GPU.  The core pipeline features include the following:</p> <ul> <li>llama3.2-nv-embedqa-1b-v2 \u2014 Embedding model for converting text chunks into vectors.</li> <li>nemoretriever-page-elements-v2 \u2014 Detects and classifies images on a page as a table, chart or infographic. </li> <li>nemoretriever-table-structure-v1 \u2014 Detects rows, columns, and cells within a table to preserve table structure and convert to Markdown format. </li> <li>nemoretriever-graphic-elements-v1 \u2014 Detects graphic elements within chart images such as titles, legends, axes, and numerical values. </li> <li>paddleocr \u2014 Image OCR model to detect and extract text from images.</li> <li>retrieval \u2014 Enables embedding and indexing into Milvus.</li> </ul> <p>Advanced features require additional GPU support and disk space.  This includes the following:</p> <ul> <li>Audio extraction \u2014 Use Riva for processing audio files. For more information, refer to Audio Processing.</li> <li><code>nemoretriever-parse</code> \u2014 Use nemoretriever-parse, which adds state-of-the-art text and table extraction. For more information, refer to Use Nemo Retriever Extraction with nemoretriever-parse.</li> <li>VLM image captioning \u2014 Use llama 3.1 nemotron 8B Vision for experimental image captioning of unstructured images. For more information, refer to Extract Captions from Images.</li> </ul>"},{"location":"extraction/support-matrix/#hardware-requirements","title":"Hardware Requirements","text":"<p>NeMo Retriever extraction supports the following GPU hardware.</p> <ul> <li>RTX Pro 6000 Blackwell Server Edition</li> <li>DGX B200</li> <li>H100 Tensor Core GPU</li> <li>A100 Tensor Core GPU</li> </ul> <p>Note</p> <p>Release 25.09 does not support A10G and L40S hardware. To use A10G or L40S hardware, use release 25.6.x.</p> <p>The following are the hardware requirements to run NeMo Retriever extraction.</p> GPU Option RTX Pro 6000 B200 H100 A100 Family PCIe SXM SXM SXM Memory 96GB 192GB 80GB 80GB Core Features Total GPUs 1 1 1 1 Core Features Total Disk Space ~150GB ~150GB ~150GB ~150GB Audio Additional Dedicated GPUs 1 1 1 1 Audio Additional Disk Space ~37GB ~37GB ~37GB ~37GB nemoretriever-parse Additional Dedicated GPUs Not supported Not supported 1 1 nemoretriever-parse Additional Disk Space Not supported Not supported ~16GB ~16GB VLM Additional Dedicated GPUs 1 1 1 1 VLM Additional Disk Space ~16GB ~16GB ~16GB ~16GB"},{"location":"extraction/support-matrix/#related-topics","title":"Related Topics","text":"<ul> <li>Prerequisites</li> <li>Release Notes</li> <li>Deploy Without Containers (Library Mode)</li> <li>Deploy With Docker Compose (Self-Hosted)</li> <li>Deploy With Helm</li> </ul>"},{"location":"extraction/telemetry/","title":"Telemetry with NeMo Retriever Extraction","text":"<p>You can view telemetry data for NeMo Retriever extraction.</p> <p>Note</p> <p>NeMo Retriever extraction is also known as NVIDIA Ingest and nv-ingest.</p>"},{"location":"extraction/telemetry/#opentelemetry","title":"OpenTelemetry","text":"<p>After OpenTelemetry and Zipkin are running, you can open your browser to explore traces: </p> <ul> <li>Docker \u2014 Use http://$YOUR_DOCKER_HOST:9411/zipkin/ </li> <li>Kubernetes \u2014 Use http://$YOUR_K8S_OTEL_POD:9411/zipkin/</li> </ul> <p></p>"},{"location":"extraction/telemetry/#prometheus","title":"Prometheus","text":"<p>After Prometheus is running, you can open your browser to explore metrics: </p> <ul> <li>Docker \u2014 Use http://$YOUR_DOCKER_HOST:9090/zipkin/</li> <li>Kubernetes \u2014 Use http://$YOUR_K8S_OTEL_POD:9090/zipkin/</li> </ul> <p></p>"},{"location":"extraction/troubleshoot/","title":"Troubleshoot NeMo Retriever Extraction","text":"<p>Use this documentation to troubleshoot issues that arise when you use NeMo Retriever extraction.</p> <p>Note</p> <p>NeMo Retriever extraction is also known as NVIDIA Ingest and nv-ingest.</p>"},{"location":"extraction/troubleshoot/#cant-process-long-non-language-text-strings","title":"Can't process long, non-language text strings","text":"<p>NeMo Retriever extraction is designed to process language and language-length strings.  If you submit a document that contains extremely long, or non-language text strings,  such as a DNA sequence, errors or unexpected results occur.</p>"},{"location":"extraction/troubleshoot/#cant-process-malformed-input-files","title":"Can't process malformed input files","text":"<p>When you run a job you might see errors similar to the following:</p> <ul> <li>Failed to process the message</li> <li>Failed to extract image</li> <li>File may be malformed</li> <li>Failed to format paragraph</li> </ul> <p>These errors can occur when your input file is malformed.  Verify or fix the format of your input file, and try resubmitting your job.</p>"},{"location":"extraction/troubleshoot/#cant-start-new-thread-error","title":"Can't start new thread error","text":"<p>In rare cases, when you run a job you might an see an error similar to <code>can't start new thread</code>.  This error occurs when the maximum number of processes available to a single user is too low. To resolve the issue, set or raise the maximum number of processes (<code>-u</code>) by using the ulimit command. Before you change the <code>-u</code> setting, consider the following:</p> <ul> <li>Apply the <code>-u</code> setting directly to the user (or the Docker container environment) that runs your ingest service.</li> <li>For <code>-u</code> we recommend 10,000 as a baseline, but you might need to raise or lower it based on your actual usage and system configuration.</li> </ul> <pre><code>ulimit -u 10,000\n</code></pre>"},{"location":"extraction/troubleshoot/#out-of-memory-oom-error-when-processing-large-datasets","title":"Out-of-Memory (OOM) Error when Processing Large Datasets","text":"<p>When you process a very large dataset with thousands of documents, you might encounter an Out-of-Memory (OOM) error.  This happens because, by default, NeMo Retriever extraction stores the results from every document in system memory (RAM).  If the total size of the results exceeds the available memory, the process fails.</p> <p>To resolve this issue, use the <code>save_to_disk</code> method.  For details, refer to Working with Large Datasets: Saving to Disk.</p>"},{"location":"extraction/troubleshoot/#embedding-service-fails-to-start-with-an-unsupported-batch-size-error","title":"Embedding service fails to start with an unsupported batch size error","text":"<p>On certain hardware, for example RTX 6000,  the embedding service might fail to start and you might see an error similar to the following.</p> <pre><code>ValueError: Configured max_batch_size (30) is larger than the model''s supported max_batch_size (3).\n</code></pre> <p>If you are using hardware where the embedding NIM uses the ONNX model profile,  you must set <code>EMBEDDER_BATCH_SIZE=3</code> in your environment.  You can set the variable in your .env file or directly in your environment.</p>"},{"location":"extraction/troubleshoot/#extract-method-nemoretriever-parse-doesnt-support-image-files","title":"Extract method nemoretriever-parse doesn't support image files","text":"<p>Currently, extraction with nemoretriever-parse doesn't support image files, only scanned PDFs.  To work around this issue, convert image files to PDFs before you use <code>extract_method=\"nemoretriever_parse\"</code>.</p>"},{"location":"extraction/troubleshoot/#too-many-open-files-error","title":"Too many open files error","text":"<p>In rare cases, when you run a job you might an see an error similar to <code>too many open files</code> or <code>max open file descriptor</code>.  This error occurs when the open file descriptor limit for your service user account is too low. To resolve the issue, set or raise the maximum number of open file descriptors (<code>-n</code>) by using the ulimit command. Before you change the <code>-n</code> setting, consider the following:</p> <ul> <li>Apply the <code>-n</code> setting directly to the user (or the Docker container environment) that runs your ingest service.</li> <li>For <code>-n</code> we recommend 10,000 as a baseline, but you might need to raise or lower it based on your actual usage and system configuration.</li> </ul> <pre><code>ulimit -n 10,000\n</code></pre>"},{"location":"extraction/troubleshoot/#triton-server-info-messages-incorrectly-logged-as-errors","title":"Triton server INFO messages incorrectly logged as errors","text":"<p>Sometimes messages are incorrectly logged as errors, when they are information.  When this happens, you can ignore the errors, and treat the messages as information.  For example, you might see log messages that look similar to the following.</p> <pre><code>ERROR 2025-04-24 22:49:44.266 nimutils.py:68] tritonserver: /usr/local/lib/libcurl.so.4: ...\nERROR 2025-04-24 22:49:44.268 nimutils.py:68] I0424 22:49:44.265292 98 cache_manager.cc:480] \"Create CacheManager with cache_dir: '/opt/tritonserver/caches'\"\nERROR 2025-04-24 22:49:44.431 nimutils.py:68] I0424 22:49:44.431796 98 pinned_memory_manager.cc:277] \"Pinned memory pool is created at '0x7f8e4a000000' with size 268435456\"\nERROR 2025-04-24 22:49:44.432 nimutils.py:68] I0424 22:49:44.432036 98 cuda_memory_manager.cc:107] \"CUDA memory pool is created on device 0 with size 67108864\"\nERROR 2025-04-24 22:49:44.433 nimutils.py:68] I0424 22:49:44.433448 98 model_config_utils.cc:753] \"Server side auto-completed config: \"\nERROR 2025-04-24 22:49:44.433 nimutils.py:68] name: \"yolox\"\nERROR 2025-04-24 22:49:44.433 nimutils.py:68] platform: \"tensorrt_plan\"\nERROR 2025-04-24 22:49:44.433 nimutils.py:68] max_batch_size: 32\nERROR 2025-04-24 22:49:44.433 nimutils.py:68] input {\nERROR 2025-04-24 22:49:44.433 nimutils.py:68] name: \"input\"\nERROR 2025-04-24 22:49:44.433 nimutils.py:68] data_type: TYPE_FP32\nERROR 2025-04-24 22:49:44.433 nimutils.py:68] dims: 3\nERROR 2025-04-24 22:49:44.433 nimutils.py:68] dims: 1024\nERROR 2025-04-24 22:49:44.433 nimutils.py:68] dims: 1024\nERROR 2025-04-24 22:49:44.433 nimutils.py:68] }\nERROR 2025-04-24 22:49:44.433 nimutils.py:68] output {\nERROR 2025-04-24 22:49:44.434 nimutils.py:68] name: \"output\"\nERROR 2025-04-24 22:49:44.434 nimutils.py:68] data_type: TYPE_FP32\nERROR 2025-04-24 22:49:44.434 nimutils.py:68] dims: 21504\nERROR 2025-04-24 22:49:44.434 nimutils.py:68] dims: 9\nERROR 2025-04-24 22:49:44.434 nimutils.py:68] }\n...\n</code></pre>"},{"location":"extraction/troubleshoot/#related-topics","title":"Related Topics","text":"<ul> <li>Support Matrix</li> <li>Prerequisites</li> <li>Deploy Without Containers (Library Mode)</li> <li>Deploy With Docker Compose (Self-Hosted)</li> <li>Deploy With Helm</li> </ul>"},{"location":"extraction/user-defined-stages/","title":"Add User-defined Stages to Your NeMo Retriever Extraction Pipeline","text":"<p>This documentation demonstrates how to add user-defined stages to your NeMo Retriever extraction pipeline. You can directly import a function, or use a string module path, and include robust signature validation.  By following these steps,  your Lambda stages are robust, signature-validated, plug-and-play for your RayPipeline,  and operate on a well-defined DataFrame payload and metadata structure.</p> <p>Note</p> <p>NeMo Retriever extraction is also known as NVIDIA Ingest and nv-ingest.</p> <p>To add user-defined stages to your pipeline, you need the following:</p> <ul> <li> <p>A callable function \u2014 Your function must have the following exact signature. For more information, refer to .</p> <pre><code>def my_fn(control_message: IngestControlMessage, stage_config: MyConfig) -&gt; IngestControlMessage:\n</code></pre> </li> <li> <p>A DataFrame payload \u2014 The <code>control_message.payload</code> field must be a pandas.DataFrame. For more information, refer to Create a DataFrame Payload.</p> </li> <li> <p>Valid metadata \u2014 The <code>metadata</code> field must conform to the nv-ingest metadata schema. For more information, refer to Update and Validate Metadata.</p> </li> </ul>"},{"location":"extraction/user-defined-stages/#create-a-lambda-function-and-config","title":"Create a Lambda Function and Config","text":"<p>Your function must have the following exact signature. </p> <pre><code>def my_fn(control_message: IngestControlMessage, stage_config: MyConfig) -&gt; IngestControlMessage:\n...\n</code></pre> <ul> <li>The first parameter is named <code>control_message</code> and is an <code>IngestControlMessage</code>.</li> <li>The second parameter is named <code>stage_config</code> and must be a subclass of <code>pydantic.BaseModel</code>.</li> <li>The return value is an <code>IngestControlMessage</code>.</li> </ul> <p>The following example demonstrates how to create a valid Lambda function and configuration.</p> <pre><code>import pandas as pd\nfrom pydantic import BaseModel\nfrom nv_ingest_api.internal.primitives.ingest_control_message import IngestControlMessage\nfrom nv_ingest_api.internal.schemas.meta.metadata_schema import validate_metadata\n\n# Config schema for your stage\nclass MyToyConfig(BaseModel):\n  set_processed: bool = True\n\ndef toy_stage_fn(control_message: IngestControlMessage, stage_config: MyToyConfig) -&gt; IngestControlMessage:\n  df = control_message.payload()\n\n  # Set 'processed' flag in the allowed 'content_metadata' dict (if present)\n  def update_metadata(meta):\n    meta = dict(meta)\n\n    # Only update if 'content_metadata' exists and is a dict\n    if \"content_metadata\" in meta and isinstance(meta[\"content_metadata\"], dict):\n      meta[\"content_metadata\"] = dict(meta[\"content_metadata\"])  # ensure copy\n      meta[\"content_metadata\"][\"processed\"] = stage_config.set_processed\n    validate_metadata(meta)\n    return meta\n\n  df[\"metadata\"] = df[\"metadata\"].apply(update_metadata)\n  control_message.payload(df)\n  return control_message\n</code></pre>"},{"location":"extraction/user-defined-stages/#create-a-dataframe-payload","title":"Create a DataFrame Payload","text":"<p>The <code>control_message.payload</code> field must be a pandas.DataFrame with the following columns.</p> <ul> <li><code>document_type</code></li> <li><code>source_id</code></li> <li><code>job_id</code></li> <li><code>metadata</code></li> </ul> <p>The following example demonstrates an input payload (before the stage runs) as a DataFrame with valid metadata.</p> <pre><code>import pandas as pd\n\ndf = pd.DataFrame([\n  {\n    \"document_type\": \"invoice\",\n    \"source_id\": \"A123\",\n    \"job_id\": \"job-001\",\n    \"metadata\": {\n      \"content\": \"example\",\n      \"content_metadata\": {\"type\": \"pdf\"},\n      \"source_metadata\": {\"source_id\": \"A123\", \"source_type\": \"pdf\"},\n    },\n  },\n  {\n    \"document_type\": \"report\",\n    \"source_id\": \"B456\",\n    \"job_id\": \"job-002\",\n    \"metadata\": {\n      \"content\": \"another\",\n      \"content_metadata\": {\"type\": \"pdf\"},\n      \"source_metadata\": {\"source_id\": \"B456\", \"source_type\": \"pdf\"},\n    }\n  }\n])\n</code></pre>"},{"location":"extraction/user-defined-stages/#add-a-stage-function-is-imported","title":"Add a Stage (Function is Imported)","text":"<p>If your function is already imported in Python,  use the following code to add your user-defined stage to the pipeline.</p> <pre><code>config = MyToyConfig(flag_field=\"my_flag\")\n\npipeline.add_stage(\n  name=\"toy_stage\",\n  stage_actor=toy_stage_fn,\n  config=config,\n  min_replicas=1,\n  max_replicas=2,\n)\n</code></pre>"},{"location":"extraction/user-defined-stages/#add-a-stage-function-is-defined-in-a-module","title":"Add a Stage (Function is Defined in a Module)","text":"<p>If your function is defined in a module,  use the following code to add your user-defined stage to the pipeline.  In this example, the function is defined in <code>my_project.stages:toy_stage_fn</code>.</p> <pre><code>config = MyToyConfig(flag_field=\"has_been_processed\")\n\npipeline.add_stage(\n  name=\"toy_stage\",\n  stage_actor=\"my_project.stages:toy_stage_fn\",\n  config=config,\n  min_replicas=1,\n  max_replicas=2,\n)\n</code></pre> <p>When the pipeline runs it does the following:</p> <ul> <li>Import and validate the function (using <code>resolve_callable_from_path</code>).</li> <li>Automatically wrap it as a Ray stage.</li> <li>Enforce the signature and parameter naming rules.</li> </ul>"},{"location":"extraction/user-defined-stages/#update-and-validate-metadata","title":"Update and Validate Metadata","text":"<p>The <code>metadata</code> column in each row is a dictionary (JSON object),  and must conform to the nv-ingest metadata schema. </p> <p>After you change any metadata, you can validate it by using the <code>validate_metadata</code> function  as demonstrated in the following code example.</p> <pre><code>from nv_ingest_api.internal.schemas.meta.metadata_schema import validate_metadata\n\ndef edit_metadata(control_message: IngestControlMessage, stage_config: MyToyConfig) -&gt; IngestControlMessage:\n  df = control_message.payload()\n\n  def ensure_valid(meta):\n    meta = dict(meta)\n    # Only update an allowed nested metadata field\n    if \"content_metadata\" in meta and isinstance(meta[\"content_metadata\"], dict):\n      meta[\"content_metadata\"] = dict(meta[\"content_metadata\"])\n      meta[\"content_metadata\"][\"checked\"] = True\n    validate_metadata(meta)\n    return meta\n\n  df[\"metadata\"] = df[\"metadata\"].apply(ensure_valid)\n  control_message.payload(df)\n  return control_message\n</code></pre>"},{"location":"extraction/user-defined-stages/#troubleshoot-validation-failures","title":"Troubleshoot Validation Failures","text":"<p>The following are some examples of reasons that Lambda functions are invalid and fail validation.</p>"},{"location":"extraction/user-defined-stages/#wrong-parameter-names","title":"Wrong parameter names","text":"<p>The following Lambda function fails validation because the parameter names are incorrect.  You should see an error message similar to <code>TypeError: Expected parameter names: 'control_message', 'config'</code>.</p> <pre><code>#Incorrect example, do not use\ndef bad_fn(msg: IngestControlMessage, cfg: MyToyConfig) -&gt; IngestControlMessage:\n...\n</code></pre>"},{"location":"extraction/user-defined-stages/#missing-type-annotations","title":"Missing type annotations","text":"<p>The following Lambda function fails validation because the parameter and return types are missing. You should see an error message similar to <code>TypeError</code>.</p> <pre><code>#Incorrect example, do not use\ndef bad_fn(control_message, stage_config):\n...\n</code></pre>"},{"location":"extraction/user-defined-stages/#best-practices","title":"Best Practices","text":"<p>Use the following best practices to avoid validation failures.</p> <ul> <li>Always use explicit type annotations and the required parameter names (<code>control_message</code>, <code>stage_config</code>).</li> <li>Your config can be any subclass of <code>pydantic.BaseModel</code>.</li> <li>Any errors in signature validation are raised with a clear message during pipeline construction.</li> <li>You can use <code>validate_metadata(meta)</code> to assert compliance after metadata changes.</li> </ul>"},{"location":"extraction/user-defined-stages/#minimal-complete-example","title":"Minimal Complete Example","text":"<p>The  following example adds user-defined stages to your NeMo Retriever extraction pipeline. </p> <ol> <li> <p>The following code creates a function for a user-defined stage.</p> <pre><code># my_pipeline/stages.py\nfrom pydantic import BaseModel\nfrom nv_ingest_api.internal.primitives.ingest_control_message import IngestControlMessage\nfrom nv_ingest_api.internal.schemas.meta.metadata_schema import validate_metadata\n\nclass DoubleConfig(BaseModel):\nmultiply_by: int = 2\n\ndef double_amount(control_message: IngestControlMessage, stage_config: DoubleConfig) -&gt; IngestControlMessage:\ndf = control_message.payload()\n\n# Suppose the metadata for each row includes 'amount' under 'content_metadata'\ndef double_meta(meta):\n    meta = dict(meta)\n    if \"content_metadata\" in meta and isinstance(meta[\"content_metadata\"], dict):\n    cm = dict(meta[\"content_metadata\"])\n    if \"amount\" in cm and isinstance(cm[\"amount\"], (int, float)):\n        cm[\"amount\"] *= stage_config.multiply_by\n    meta[\"content_metadata\"] = cm\n    validate_metadata(meta)\n    return meta\n\ndf[\"metadata\"] = df[\"metadata\"].apply(double_meta)\ncontrol_message.payload(df)\nreturn control_message\n</code></pre> </li> <li> <p>The following code adds the user-defined stage to the pipeline.</p> <ul> <li> <p>(Option 1)  For a function that is defined in the module my_pipeline.stages.</p> <pre><code>from my_pipeline.stages import double_amount, DoubleConfig\n\npipeline.add_stage(\nname=\"doubler\",\nstage_actor=\"my_pipeline.stages:double_amount\",\nconfig=DoubleConfig(multiply_by=3),\nmin_replicas=1,\nmax_replicas=2,\n)\n</code></pre> </li> <li> <p>(Option 2) For a function that you have already imported.</p> <pre><code>pipeline.add_stage(\nname=\"doubler\",\nstage_actor=double_amount,\nconfig=DoubleConfig(multiply_by=3),\nmin_replicas=1,\nmax_replicas=2,\n)\n</code></pre> </li> </ul> </li> </ol>"},{"location":"extraction/vlm-embed/","title":"Use Multimodal Embedding with NeMo Retriever Extraction","text":"<p>This documentation describes how to use NeMo Retriever extraction  with the multimodal embedding model Llama 3.2 NeMo Retriever Multimodal Embedding 1B.</p> <p>The <code>Llama 3.2 NeMo Retriever Multimodal Embedding 1B</code> model is optimized for multimodal question-answering retrieval.  The model can embed documents in the form of an image, text, or a combination of image and text.  Documents can then be retrieved given a user query in text form.  The model supports images that contain text, tables, charts, and infographics.</p> <p>Note</p> <p>NeMo Retriever extraction is also known as NVIDIA Ingest and nv-ingest.</p>"},{"location":"extraction/vlm-embed/#configure-and-run-the-multimodal-nim","title":"Configure and Run the Multimodal NIM","text":"<p>Use the following procedure to configure and run the multimodal embedding NIM locally.</p> <ol> <li> <p>Set the embedding model in your .env file. This tells NeMo Retriever extraction to use the Llama 3.2 Multimodal model instead of the default text-only embedding model.</p> <pre><code>EMBEDDING_IMAGE=nvcr.io/nvidia/nemo-microservices/llama-3.2-nemoretriever-1b-vlm-embed-v1\nEMBEDDING_TAG=1.7.0\nEMBEDDING_NIM_MODEL_NAME=nvidia/llama-3.2-nemoretriever-1b-vlm-embed-v1\n</code></pre> </li> <li> <p>Start the NeMo Retriever extraction services. The multimodal embedding service is included by default.</p> <pre><code>docker compose --profile retrieval --profile table-structure up\n</code></pre> </li> </ol> <p>After the services are running, you can interact with the extraction pipeline by using Python. The key to leveraging the multimodal model is  to configure the <code>extract</code> and <code>embed</code> methods to process different content types as either text or images.</p>"},{"location":"extraction/vlm-embed/#example-with-default-text-based-embedding","title":"Example with Default Text-Based Embedding","text":"<p>When you use the multimodal model, by default, all extracted content (text, tables, charts) is treated as plain text.  The following example provides a strong baseline for retrieval.</p> <ul> <li>The <code>extract</code> method is configured to pull out text, tables, and charts.</li> <li>The <code>embed</code> method is called with no arguments.</li> </ul> <pre><code>ingestor = (\n    Ingestor()\n    .files(\"./data/*.pdf\")\n    .extract(\n        extract_text=True,\n        extract_tables=True,\n        extract_charts=True,\n        extract_images=False,\n    )\n    .embed() # Default behavior embeds all content as text\n)\nresults = ingestor.ingest()\n</code></pre>"},{"location":"extraction/vlm-embed/#example-with-embedding-structured-elements-as-images","title":"Example with Embedding Structured Elements as Images","text":"<p>It is common to process PDFs by embedding standard text as text, and embed visual elements like tables and charts as images.  The following example enables the multimodal model to capture the spatial and structural information of the visual content.</p> <ul> <li>The <code>extract</code> method is configured to pull out text, tables, and charts.</li> <li>The <code>embed</code> method is configured with <code>structured_elements_modality=\"image\"</code> to embed the extracted tables and charts as images.</li> </ul> <pre><code>ingestor = (\n    Ingestor()\n    .files(\"./data/*.pdf\")\n    .extract(\n        extract_text=True,\n        extract_tables=True,\n        extract_charts=True,\n        extract_images=False,\n    )\n    .embed(\n        structured_elements_modality=\"image\",\n    )\n)\nresults = ingestor.ingest()\n</code></pre>"},{"location":"extraction/vlm-embed/#example-with-embedding-entire-pdf-pages-as-images","title":"Example with Embedding Entire PDF Pages as Images","text":"<p>For documents where the entire page layout is important (such as infographics, complex diagrams, or forms),  you can configure NeMo Retriever extraction to treat every page as a single image. The following example extracts and embeds each page as an image.</p> <p>Note</p> <p>The <code>extract_page_as_image</code> feature is experimental. Its behavior may change in future releases.</p> <ul> <li>The <code>extract method</code> uses the <code>extract_page_as_image=True</code> parameter. All other extraction types are set to <code>False</code>.</li> <li>The <code>embed method</code> processes the page images.</li> </ul> <pre><code>ingestor = (\n    Ingestor()\n    .files(\"./data/*.pdf\")\n    .extract(\n        extract_text=False,\n        extract_tables=False,\n        extract_charts=False,\n        extract_images=False,\n        extract_page_as_image=True,\n    )\n    .embed(\n        image_elements_modality=\"image\",\n    )\n)\nresults = ingestor.ingest()\n</code></pre>"},{"location":"extraction/vlm-embed/#related-topics","title":"Related Topics","text":"<ul> <li>Support Matrix</li> <li>Troubleshoot Nemo Retriever Extraction</li> <li>Use the NV-Ingest Python API</li> </ul>"}]}