{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7efe0f92-fdbb-4471-b74c-5bdaafed8102",
   "metadata": {},
   "source": [
    "# Multimodal RAG with Langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e78be0-9abe-4d1a-8aaa-61230b059792",
   "metadata": {},
   "source": [
    "This cookbook shows how to perform RAG on the table and text output of the nv-ingest pdf extraction pipeline. However, using RAG on tables can present some challenges as raw table data doesn't always work well with semantic similarity search. To account for this we will generate summaries of the table data to perform the similarity search on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2487f1eb-eea8-46f1-9ab0-0fd3bea77bd5",
   "metadata": {},
   "source": [
    "First, we'll load in the text and table json content from the extracted nv-ingest metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bcd6b2a-c832-4685-bd6e-53b60a107e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "text_data = json.loads(Path(\"/raid/cjarrett/ingest_gh/nv-ingest/processed_docs/text/multimodal_test.pdf.metadata.json\").read_text())\n",
    "table_data = json.loads(Path(\"/raid/cjarrett/ingest_gh/nv-ingest/processed_docs/structured/multimodal_test.pdf.metadata.json\").read_text())\n",
    "\n",
    "text_content = [doc[\"metadata\"][\"content\"] for doc in text_data]\n",
    "table_content = [table[\"metadata\"][\"table_metadata\"][\"table_content\"] for table in table_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22850a1-5915-4a09-a1e4-9f6669f0fd3b",
   "metadata": {},
   "source": [
    "Next, we'll create a chain that uses an llm to summarize table or text chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e1c9595-e508-49b0-99fb-0c200fe4cace",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c639273e-e206-4d82-9896-d87bf0be1745",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt_text = \"\"\"You are an assistant tasked with summarizing tables and text. \\ \n",
    "Give a concise summary of the table or text. Table or text chunk: {element} \"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(prompt_text)\n",
    "\n",
    "model = ChatOpenAI(temperature=0, model=\"gpt-4o\")\n",
    "summarize_chain = {\"element\": lambda x: x} | prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85309220-53f6-4f59-a771-966dea97abbd",
   "metadata": {},
   "source": [
    "And then we'll apply that chain to each of our text and table chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8495a47-2490-435c-9dbb-216f501a0016",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_summaries = summarize_chain.batch(table_content, {\"max_concurrency\": 5})\n",
    "text_summaries = summarize_chain.batch(text_content, {\"max_concurrency\": 5})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d14f63-31b8-4d72-a029-a54328ff1c5b",
   "metadata": {},
   "source": [
    "Next, we'll create a multi vectore retriever which allows us to store vectors both for the raw text and tables as well as the summaries we generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52539355-864b-4196-88b8-2d5493359346",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "vectorstore = Chroma(collection_name=\"summaries\", embedding_function=OpenAIEmbeddings())\n",
    "\n",
    "store = InMemoryStore()\n",
    "id_key = \"doc_id\"\n",
    "\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    id_key=id_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f74fbb4-2448-4929-886f-657662d1d50b",
   "metadata": {},
   "source": [
    "Now, we can add the text and table as well as the text and table summaries to our multi vectore retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff37bf98-9651-4d38-8c52-8a1875327010",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "doc_ids = [str(uuid.uuid4()) for _ in texts]\n",
    "summary_texts = [\n",
    "    Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
    "    for i, s in enumerate(text_summaries)\n",
    "]\n",
    "retriever.vectorstore.add_documents(summary_texts)\n",
    "retriever.docstore.mset(list(zip(doc_ids, texts)))\n",
    "\n",
    "table_ids = [str(uuid.uuid4()) for _ in tables]\n",
    "summary_tables = [\n",
    "    Document(page_content=s, metadata={id_key: table_ids[i]})\n",
    "    for i, s in enumerate(table_summaries)\n",
    "]\n",
    "retriever.vectorstore.add_documents(summary_tables)\n",
    "retriever.docstore.mset(list(zip(table_ids, tables)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87111b5-e5a8-45a0-9663-2ae6d9ea2ab6",
   "metadata": {},
   "source": [
    "Finally, we'll create an RAG chain that we can use to query our pdf in naturall language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16331bb-95dd-46d7-8b71-9d966a8ef3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "template = \"\"\"You are an assistant for question-answering tasks. \n",
    "Answer the question based only on the following context, which can include text and tables:\n",
    "{context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "model = ChatOpenAI(temperature=0, model=\"gpt-4o\")\n",
    "\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b547a19a-9ada-4a40-a246-6d7bc4d24482",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke(\"What activity was the giraffe performing?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
