{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c031327-2456-41a2-b0ef-975bf96823c7",
   "metadata": {},
   "source": [
    "## How to add metadata to your documents and filter searches\n",
    "This notebook will walk you through how to upload metadata that provides extra information about the corpus you are ingesting with nv-ingest. It will show the requirements for the metadata file and what file types are supported. Then we will go throught he process of filtering searches, in this case, on the metadata we provided.\n",
    "\n",
    "First step is to provide imports for all the tools we will be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d32ff2e-ab3c-4118-9d74-ef3c63837003",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nv_ingest_client.client import Ingestor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18ab4bf-6a00-4008-aa10-87741369fad1",
   "metadata": {},
   "source": [
    "Next we will annotate all the necessary variables to ensure our client connects to our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a902bd2d-cf8e-4b68-8a98-a5b535e440d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=\"nvidia/llama-3.2-nv-embedqa-1b-v2\"\n",
    "hostname=\"localhost\"\n",
    "collection_name = \"nv_ingest_collection\"\n",
    "sparse = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157d8909-542b-47fd-b01c-6689eefdaf11",
   "metadata": {},
   "source": [
    "Instantiate your ingestor object with all the stages you want in your pipeline. Ensure that you have a vdb_upload stage in place. As this is what will load your transformed elements(data) in to the vector database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f9e2a4-7e50-491d-a0c6-21a4d4f27db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ingestor = ( \n",
    "    Ingestor(message_client_hostname=hostname)\n",
    "    .files([\"data/woods_frost.pdf\", \"data/multimodal_test.pdf\"])\n",
    "    .extract(\n",
    "        extract_text=True,\n",
    "        extract_tables=True,\n",
    "        extract_charts=True,\n",
    "        extract_images=True,\n",
    "        text_depth=\"page\"\n",
    "    ).embed(text=True, tables=True\n",
    "    ).vdb_upload(\n",
    "        collection_name=collection_name, \n",
    "        milvus_uri=f\"http://{hostname}:19530\", \n",
    "        sparse=sparse, \n",
    "        minio_endpoint=f\"{hostname}:9000\", \n",
    "        dense_dim=2048\n",
    "    )\n",
    ")\n",
    "results = ingestor.ingest_async().result()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0a8aeb",
   "metadata": {},
   "source": [
    "Once you have completed the normal ingestion the collection has been loaded into your Vector Database. If you need to reindex that data for whatever reason, you can simply run the `reindex_collection` function and supply the necessary parameters. There is a full list in the docstring of the function, with many defaults already set for you. This function is desigend to be used when the results from your ingestor pipeline are no longer available, you might have ingested this information at a previous time and the ingestor results are no longer in available in memory. This function allows you to query the data from the vector database to recreate those results and send them back into a new collection or the same collection, effectively deleting the previous information stored in that collection. \n",
    "\n",
    "In this example we will be reindexing under the same collection name, replacing the data in the collection. You can always supply a `new_collection_name` as one of the arguments to the function so that you can save the reindex in another collection, if you need to perform any analysis between collection data. The function supplies a `write_dir` parameter which allows you to pull the data and write in to files in batches. Currently the batch_size is automatically set to the default query batch_size for the vector database. The `write_dir` option is meant to be used when the data is larger than the available resources, with this option reindexing is slower than when holding the data in host memory.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1396906e-321a-4ab6-af83-9e651a51cb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nv_ingest_client.util.milvus import reindex_collection\n",
    "\n",
    "reindex_collection(\n",
    "    collection_name, \n",
    "    sparse=sparse\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
