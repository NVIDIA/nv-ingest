## @section Deployment parameters
## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity
## @param affinity [object] [default: {}] Affinity settings for deployment.
affinity: {}

## @param nodeSelector [object] Sets node selectors for the NIM -- for example `nvidia.com/gpu.present: "true"`
nodeSelector: {}

## @param logLevel Log level of NVIngest service. Possible values of the variable are TRACE, DEBUG, INFO, WARNING, ERROR, CRITICAL.
logLevel: DEBUG

## @param extraEnvVarsCM [string] [default: ""] A Config map holding Enviroment variables to include in the NVIngest containerextraEnvVarsCM: ""
extraEnvVarsCM: ""

## @param extraEnvVarsSecret [string] [default: ""] A K8S Secret to map to Enviroment variables to include in the NVIngest container
extraEnvVarsSecret: ""

## @param fullnameOverride [string] [default: ""] A name to force the fullname of the NVIngest container to have, defaults to the Helm Release Name
fullnameOverride: ""

## @param nameOverride [string] [default: ""] A name to base the objects created by this helm chart
nameOverride: ""

## @param image.repository [string] NIM Image Repository
## @param image.tag [string] Image tag or version
## @param image.pullPolicy [string] Image pull policy
image:
  pullPolicy: IfNotPresent
  repository: "nvcr.io/nvidia/nemo-microservices/nv-ingest"
  tag: "25.3.0"

## @param podAnnotations [object] Sets additional annotations on the main deployment pods
podAnnotations:
  traffic.sidecar.istio.io/excludeOutboundPorts: '8007'

## @param podLabels [object] Specify extra labels to be add to on deployed pods.
podLabels: {}

## @extra podSecurityContext Specify privilege and access control settings for pod
## @param podSecurityContext.fsGroup Specify file system owner group id.
podSecurityContext:
  fsGroup: 1000

## @param extraVolumes [object] Adds arbitrary additional volumes to the deployment set definition
extraVolumes: {}

## @param extraVolumeMounts [object] Specify volume mounts to the main container from `extraVolumes`
extraVolumeMounts: {}

## @extra imagePullSecrets Specify list of secret names that are needed for the main container and any init containers.
## @skip imagePullSecrets[0].name
imagePullSecrets:
  - name: ngc-api
  - name: ngc-secret

## @param containerSecurityContext [object] Sets privilege and access control settings for container (Only affects the main container, not pod-level)
containerSecurityContext: {}

## @skip tolerations
## @extra tolerations [ default: Empty Array ] Specify tolerations for pod assignment. Allows the scheduler to schedule pods with matching taints.
## @skip tolerations[0].key
## @skip tolerations[0].operator
## @skip tolerations[0].effect
tolerations: []

## @param replicaCount [default: 1] The number of replicas for NVIngest when autoscaling is disabled
replicaCount: 1

## @skip resources Specify resources limits and requests for the running service.
## @extra resources.limits."nvidia.com/gpu" Specify number of GPUs to present to the running service.
## @param resources.limits.memory [default: 90Gi] Specify limit for memory
## @param resources.requests.memory [default: 24Gi] Specify request for memory
resources:
  limits:
    memory: 90Gi
    nvidia.com/gpu: 1
    cpu: "48000m"
  requests:
    memory: 24Gi
    cpu: "24000m"


## @param tmpDirSize [default: 16Gi] Specify the amount of space to reserve for temporary storage
tmpDirSize: 16Gi


## @section NIM Configuration
## @descriptionStart
## Define additional values to the dependent NIM helm charts by updating the "yolox-nim", "cached-nim", "deplot-nim", and "paddleocr-nim"
## values. A sane set of configurations are already included in this value file and only the "image.repository" and "image.tag" fields are
## explicitly called out here.
## @descriptionEnd

## @skip nemoretriever-page-elements-v2
## @param nemoretriever-page-elements-v2.deployed [default: true] true if the Yolox NIM should be deployed and false otherwise
## @param nemoretriever-page-elements-v2.image.repository The repository to override the location of the YOLOX
## @param nemoretriever-page-elements-v2.image.tag The tag override for YOLOX
## @param nemoretriever-page-elements-v2.image.pullPolicy [default: IfNotPresent]
nemoretriever-page-elements-v2:
  deployed: true
  image:
    repository: nvcr.io/nim/nvidia/nemoretriever-page-elements-v2
    tag: "1.2.0"
    pullPolicy: IfNotPresent
  podSecurityContext:
    runAsUser: 1000
    runAsGroup: 1000
    fsGroup: 1000
  replicaCount: 1
  serviceAccount:
    create: false
    name: ""
  statefuleSet:
    enabled: false
  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 10
    metrics: []
  service:
    type: "ClusterIP"
    name: nemoretriever-page-elements-v2
    httpPort: 8000
    grpcPort: 8001
    metricsPort: 0  # Generally unused and defaults to 0
  nim:
    grpcPort: 8001
    logLevel: "INFO"
  env:
    - name: NIM_HTTP_API_PORT
      value: "8000"

## @skip nemoretriever-graphic-elements-v1
## @param nemoretriever-graphic-elements-v1.deployed [default: true] true if the nemoretriever-graphic-elements NIM should be deployed and false otherwise
## @param nemoretriever-graphic-elements-v1.image.repository The repository to override the location of the nemoretriever-graphic-elements
## @param nemoretriever-graphic-elements-v1.image.tag The tag override for nemoretriever-graphic-elements
nemoretriever-graphic-elements-v1:
  deployed: true
  customCommand: []
  customArgs: []
  image:
    repository: nvcr.io/nim/nvidia/nemoretriever-graphic-elements-v1
    tag: "1.2.0"
  podSecurityContext:
    runAsUser: 1000
    runAsGroup: 1000
    fsGroup: 1000
  replicaCount: 1
  serviceAccount:
    create: false
    name: ""
  statefuleSet:
    enabled: false
  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 10
    metrics: []
  service:
    type: "ClusterIP"
    name: nemoretriever-graphic-elements-v1
    httpPort: 8000
    grpcPort: 8001
    metricsPort: 0  # Generally unused and defaults to 0
  nim:
    grpcPort: 8001
    logLevel: "INFO"
  env:
    - name: NIM_HTTP_API_PORT
      value: "8000"

## @skip nemoretriever-table-structure-v1
## @param nemoretriever-table-structure-v1.deployed [default: true] true if the nemoretriever-table-structure NIM should be deployed and false otherwise
## @param nemoretriever-table-structure-v1.image.repository The repository to override the location of the nemoretriever-table-structure
## @param nemoretriever-table-structure-v1.image.tag The tag override for nemoretriever-table-structure
nemoretriever-table-structure-v1:
  deployed: true
  # fullnameOverride: nemoretriever-table-structure-v1
  customCommand: []
  customArgs: []
  image:
    repository: nvcr.io/nim/nvidia/nemoretriever-table-structure-v1
    tag: "1.2.0"
  podSecurityContext:
    runAsUser: 1000
    runAsGroup: 1000
    fsGroup: 1000
  replicaCount: 1
  serviceAccount:
    create: false
    name: ""
  statefuleSet:
    enabled: false
  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 10
    metrics: []
  service:
    type: "ClusterIP"
    name: nemoretriever-table-structure-v1
    httpPort: 8000
    grpcPort: 8001
    metricsPort: 0  # Generally unused and defaults to 0
  nim:
    grpcPort: 8001
    logLevel: "INFO"
  env:
    - name: NIM_HTTP_API_PORT
      value: "8000"

## @skip nim-vlm-image-captioning
## @param nim-vlm-image-captioning.deployed [default: false] true if the vlm-nim should be deployed and false otherwise
## @param nim-vlm-image-captioning.image.repository The repository to override the location of the nim-vlm
## @param nim-vlm-image-captioning.image.tag The tag override for nim-vlm
nim-vlm-image-captioning:
  deployed: false
  fullnameOverride: nim-vlm-image-captioning
  customCommand: []
  customArgs: []
  image:
    repository: nvcr.io/nim/meta/llama-3.2-11b-vision-instruct
    tag: "latest"
  podSecurityContext:
    runAsUser: 1000
    runAsGroup: 1000
    fsGroup: 1000
  replicaCount: 1
  service:
    type: "ClusterIP"
    name: nim-vlm-image-captioning
    httpPort: 8000
    grpcPort: 8001
    metricsPort: 0  # Generally unused and defaults to 0
  nim:
    grpcPort: 8001
    logLevel: "INFO"
  env:
    - name: NIM_HTTP_API_PORT
      value: "8000"

## @skip nim-vlm-text-extraction
## @param nim-vlm-text-extraction.deployed [default: false] true if the vlm-nim should be deployed and false otherwise
## @param nim-vlm-text-extraction.image.repository The repository to override the location of the nim-vlm
## @param nim-vlm-text-extraction.image.tag The tag override for nim-vlm
nim-vlm-text-extraction:
  deployed: false
  fullnameOverride: nim-vlm-text-extraction-nemoretriever-parse
  customCommand: []
  customArgs: []
  image:
    repository: nvcr.io/nvidia/nemo-microservices/nemoretriever-parse
    tag: "1.2.0ea"
  podSecurityContext:
    runAsUser: 1000
    runAsGroup: 1000
    fsGroup: 1000
  replicaCount: 1
  service:
    type: "ClusterIP"
    name: nim-vlm-text-extraction-nemoretriever-parse
    httpPort: 8000
    grpcPort: 8001
    metricsPort: 0  # Generally unused and defaults to 0
  nim:
    grpcPort: 8001
    logLevel: "INFO"
  env:
    - name: NIM_HTTP_API_PORT
      value: "8000"

## @skip paddleocr-nim
## @param paddleocr-nim.deployed [default: true] true if the paddleocr-nim should be deployed and false otherwise
## @param paddleocr-nim.image.repository The repository to override the location of the Paddle OCR NIM
## @param paddleocr-nim.image.tag The tag override for Paddle OCR NIM
paddleocr-nim:
  paddleocr-nim.deployed: true
  fullnameOverride: nv-ingest-paddle
  customCommand: []
  customArgs: []
  image:
    repository: nvcr.io/nim/baidu/paddleocr
    tag: "1.2.0"
  podSecurityContext:
    runAsUser: 1000
    runAsGroup: 1000
    fsGroup: 1000
  replicaCount: 1
  serviceAccount:
    create: false
    name: ""
  statefuleSet:
    enabled: false
  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 10
    metrics: []
  service:
    type: "ClusterIP"
    name: nv-ingest-paddle
    httpPort: 8000
    grpcPort: 8001
    metricsPort: 0  # Generally unused and defaults to 0
  nim:
    grpcPort: 8001
    logLevel: "INFO"
  env:
    - name: NIM_HTTP_API_PORT
      value: "8000"


## @skip text-embedding-nim
## @param text-embedding-nim.deployed [default: false] true if the text-embedding-nim should be deployed and false otherwise
## @param text-embedding-nim.image.repository The repository to override the location of the text-embedding-nim
## @param text-embedding-nim.image.tag The tag override for text-embedding-nim
text-embedding-nim:
  deployed: false
  fullnameOverride: nv-ingest-embedqa # Share name with nvidia-nim-llama-32-nv-embedqa-1b-v2 to ease configuration
  customCommand: []
  customArgs: []
  image:
    repository: nvcr.io/nim/nvidia/nv-embedqa-e5-v5
    tag: "1.5.0"
  podSecurityContext:
    runAsUser: 1000
    runAsGroup: 1000
    fsGroup: 1000
  replicaCount: 1
  serviceAccount:
    create: false
    name: ""
  statefuleSet:
    enabled: false
  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 10
    metrics: []
  service:
    type: "ClusterIP"
    name: nv-ingest-embedqa
    httpPort: 8000
    grpcPort: 8001
    metricsPort: 0  # Generally unused and defaults to 0
  nim:
    grpcPort: 8001
    logLevel: "INFO"
  env:
    - name: NIM_HTTP_API_PORT


## @skip nvidia-nim-llama-32-nv-embedqa-1b-v2
## @param nvidia-nim-llama-32-nv-embedqa-1b-v2.deployed [default: true] true if nvidia-nim-llama-32-nv-embedqa-1b-v2 should be deployed and false otherwise
## @param nvidia-nim-llama-32-nv-embedqa-1b-v2.image.repository The repository to override the location of the nvEmbedqa NIM
## @param nvidia-nim-llama-32-nv-embedqa-1b-v2.image.tag The tag override for nvEmbedqa NIM
nvidia-nim-llama-32-nv-embedqa-1b-v2:
  deployed: true
  fullnameOverride: nv-ingest-embedqa # Share name with text-embedding-nim to ease configuration
  customCommand: []
  customArgs: []
  image:
    repository: nvcr.io/nim/nvidia/llama-3.2-nv-embedqa-1b-v2
    tag: "1.5.0"
  podSecurityContext:
    runAsUser: 1000
    runAsGroup: 1000
    fsGroup: 1000
  replicaCount: 1
  serviceAccount:
    create: false
    name: ""
  statefuleSet:
    enabled: false
  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 10
    metrics: []
  service:
    type: "ClusterIP"
    name: nv-ingest-embedqa
    httpPort: 8000
    grpcPort: 8001
    metricsPort: 0  # Generally unused and defaults to 0
  nim:
    grpcPort: 8001
    logLevel: "INFO"
  env:
    - name: NIM_HTTP_API_PORT


## @skip llama-32-nv-rerankqa-1b-v2
## @param llama-32-nv-rerankqa-1b-v2.deployed [default: true] true if llama-32-nv-rerankqa-1b-v2 should be deployed and false otherwise
## @param llama-32-nv-rerankqa-1b-v2.image.repository The repository to override the location of the reranker NIM
## @param llama-32-nv-rerankqa-1b-v2.image.tag The tag override for reranker NIM
llama-32-nv-rerankqa-1b-v2:
  deployed: false
  # fullnameOverride: llama-32-nv-rerankqa-1b-v2
  customCommand: []
  customArgs: []
  image:
    repository: nvcr.io/nim/nvidia/llama-3.2-nv-rerankqa-1b-v2
    tag: "1.3.1"
  podSecurityContext:
    runAsUser: 1000
    runAsGroup: 1000
    fsGroup: 1000
  replicaCount: 1
  serviceAccount:
    create: false
    name: ""
  statefuleSet:
    enabled: false
  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 10
    metrics: []
  service:
    type: "ClusterIP"
    name: llama-32-nv-rerankqa-1b-v2
    httpPort: 8000
    grpcPort: 8001
    metricsPort: 0  # Generally unused and defaults to 0
  nim:
    grpcPort: 8001
    logLevel: "INFO"
  env:
    - name: NIM_HTTP_API_PORT


## @section Milvus Deployment parameters
## @descriptionStart
## NVIngest uses Milvus and Minio to store extracted images from a document
## This chart by default sets up a Milvus standalone instance in the namespace using the
## Helm chart at found https://artifacthub.io/packages/helm/milvus-helm/milvus
## @descriptionEnd
## @param milvusDeployed [default: true] Whether to deploy Milvus and Minio from this helm chart
milvusDeployed: true
## @param milvus [default: sane {}] Find values at https://artifacthub.io/packages/helm/milvus-helm/milvus
milvus:
  cluster:
    enabled: false
  etcd:
    replicaCount: 1
    persistence:
      storageClass: null
  minio:
    mode: standalone
    persistence:
      size: 10Gi
      storageClass: null
  pulsar:
    enabled: false
  standalone:
    persistence:
      persistentVolumeClaim:
        size: 10Gi
        storageClass: null
    extraEnv:
      - name: LOG_LEVEL
        value: error



## @section Autoscaling parameters
## @descriptionStart
## Values used for creating a `Horizontal Pod Autoscaler`. If autoscaling is not enabled, the rest are ignored.
## NVIDIA recommends usage of the custom metrics API, commonly implemented with the prometheus-adapter.
## Standard metrics of CPU and memory are of limited use in scaling NIM.
## @descriptionEnd
## @param autoscaling.enabled Enables horizontal pod autoscaler.
## @param autoscaling.minReplicas Specify minimum replicas for autoscaling.
## @param autoscaling.maxReplicas Specify maximum replicas for autoscaling.
## @param autoscaling.metrics Array of metrics for autoscaling.
autoscaling:
  enabled: false
  maxReplicas: 100
  minReplicas: 1
  metrics: []


## @section Redis configurations
## @descriptionStart
## Include any redis configuration that you'd like with the deployed Redis
## Find values at https://github.com/bitnami/charts/tree/main/bitnami/redis
## @descriptionEnd
## @param redisDeployed [default: true] Whether to deploy Redis from this helm chart
redisDeployed: true
## @param redis [default: sane {}] Find values at https://github.com/bitnami/charts/tree/main/bitnami/redis
redis:
  auth:
    enabled: false
  replica:
    replicaCount: 1
    persistence:
      size: "50Gi"
    resources:
      requests:
        memory: "6Gi"
      limits:
        memory: "12Gi"
  master:
    persistence:
      size: "50Gi"
    resources:
      requests:
        memory: "6Gi"
      limits:
        memory: "12Gi"
    configmap: |-
      protected-mode no

## @section Environment Variables
## @descriptionStart
## Define environment variables as key/value dictionary pairs
## @descriptionEnd
## @param envVars [default: sane {}] Adds arbitrary environment variables to the main container using key-value pairs, for example NAME: value
## @param envVars.MAX_INGEST_PROCESS_WORKERS [default: "16"] Maximum Ingestion worker processes
## @param envVars.MESSAGE_CLIENT_HOST [default: "nv-ingest-ms-runtime"] Override this value to specify a differeing REST endpoint host.
## @param envVars.MESSAGE_CLIENT_PORT [default: "7670"] Override this value to specify a differing REST endpoint port.
## @param envVars.REDIS_MORPHEUS_TASK_QUEUE [default: "morpheus_task_queue"]
## @param envVars.NV_INGEST_DEFAULT_TIMEOUT_MS [default: "1234"] Override the Timeout of the NVIngest requests.
## @param envVars.MINIO_INTERNAL_ADDRESS [default: "nv-ingest-minio:9000"] Override this to the cluster local DNS name of minio
## @param envVars.MINIO_PUBLIC_ADDRESS [default: "http://localhost:9000"] Override this to publicly routable minio address, default assumes port-forwarding
## @param envVars.MINIO_BUCKET [default: "nv-ingest"] Override this for specific minio bucket to upload extracted images to
## @param envVars.PADDLE_GRPC_ENDPOINT [default: "nv-ingest-paddle:8001"]
## @param envVars.PADDLE_HTTP_ENDPOINT [default: "http://nv-ingest-paddle:8000/v1/infer"]
## @param envVars.PADDLE_INFER_PROTOCOL [default: "grpc"] Whether to use the GRPC or HTTP endpoint
## @param envVars.NEMORETRIEVER_PARSE_HTTP_ENDPOINT [default: "http://nim-vlm-text-extraction-nemoretriever-parse:8000/v1/chat/completions"]
## @param envVars.NEMORETRIEVER_PARSE_INFER_PROTOCOL [default: "http"]
## @param envVars.YOLOX_GRPC_ENDPOINT [default: "nemoretriever-page-elements-v2:8001"]
## @param envVars.YOLOX_HTTP_ENDPOINT [default: "http://nemoretriever-page-elements-v2:8000/v1/infer"]
## @param envVars.YOLOX_INFER_PROTOCOL [default: "grpc"]
## @param envVars.YOLOX_GRAPHIC_ELEMENTS_GRPC_ENDPOINT [default: "nemoretriever-graphic-elements-v1:8001"]
## @param envVars.YOLOX_GRAPHIC_ELEMENTS_HTTP_ENDPOINT [default: "http://nemoretriever-graphic-elements-v1:8000/v1/infer"]
## @param envVars.YOLOX_GRAPHIC_ELEMENTS_INFER_PROTOCOL [default: "grpc"]
## @param envVars.YOLOX_TABLE_STRUCTURE_GRPC_ENDPOINT [default: "nemoretriever-table-structure-v1:8001"]
## @param envVars.YOLOX_TABLE_STRUCTURE_HTTP_ENDPOINT [default: "http://nemoretriever-table-structure-v1:8000/v1/infer"]
## @param envVars.YOLOX_TABLE_STRUCTURE_INFER_PROTOCOL [default: "grpc"]
## @param envVars.EMBEDDING_NIM_ENDPOINT [default: "http://nv-ingest-embedqa:8000/v1"]
## @param envVars.EMBEDDING_NIM_MODEL_NAME [default: "nvidia/llama-3.2-nv-embedqa-1b-v2"]
## @param envVars.MILVUS_ENDPOINT [default: "http://nv-ingest-milvus:19530"]
## @param envVars.VLM_CAPTION_ENDPOINT [default: "https://ai.api.nvidia.com/v1/gr/meta/llama-3.2-11b-vision-instruct/chat/completions"] Override this for specific VLM caption endpoint
## @param envVars.VLM_CAPTION_MODEL_NAME [default: "meta/llama-3.2-11b-vision-instruct"]
envVars:
  INGEST_EDGE_BUFFER_SIZE: 64
  MAX_INGEST_PROCESS_WORKERS: 16
  MESSAGE_CLIENT_HOST: "nv-ingest-redis-master"
  MESSAGE_CLIENT_PORT: "6379"
  REDIS_MORPHEUS_TASK_QUEUE: "morpheus_task_queue"
  NV_INGEST_DEFAULT_TIMEOUT_MS: "1234"

  MINIO_INTERNAL_ADDRESS: nv-ingest-minio:9000
  MINIO_PUBLIC_ADDRESS: http://localhost:9000
  MINIO_BUCKET: nv-ingest

  PADDLE_GRPC_ENDPOINT: nv-ingest-paddle:8001
  PADDLE_HTTP_ENDPOINT: http://nv-ingest-paddle:8000/v1/infer
  PADDLE_INFER_PROTOCOL: grpc
  NEMORETRIEVER_PARSE_HTTP_ENDPOINT: http://nim-vlm-text-extraction-nemoretriever-parse:8000/v1/chat/completions
  NEMORETRIEVER_PARSE_INFER_PROTOCOL: http
  YOLOX_GRPC_ENDPOINT: nemoretriever-page-elements-v2:8001
  YOLOX_HTTP_ENDPOINT: http://nemoretriever-page-elements-v2:8000/v1/infer
  YOLOX_INFER_PROTOCOL: grpc
  YOLOX_GRAPHIC_ELEMENTS_GRPC_ENDPOINT: nemoretriever-graphic-elements-v1:8001
  YOLOX_GRAPHIC_ELEMENTS_HTTP_ENDPOINT: http://nemoretriever-graphic-elements-v1:8000/v1/infer
  YOLOX_GRAPHIC_ELEMENTS_INFER_PROTOCOL: grpc
  YOLOX_TABLE_STRUCTURE_GRPC_ENDPOINT: nemoretriever-table-structure-v1:8001
  YOLOX_TABLE_STRUCTURE_HTTP_ENDPOINT: http://nemoretriever-table-structure-v1:8000/v1/infer
  YOLOX_TABLE_STRUCTURE_INFER_PROTOCOL: grpc

  EMBEDDING_NIM_ENDPOINT: "http://nv-ingest-embedqa:8000/v1"
  EMBEDDING_NIM_MODEL_NAME: "nvidia/llama-3.2-nv-embedqa-1b-v2"
  MILVUS_ENDPOINT: "http://nv-ingest-milvus:19530"

  VLM_CAPTION_ENDPOINT: "https://ai.api.nvidia.com/v1/gr/meta/llama-3.2-11b-vision-instruct/chat/completions"
  VLM_CAPTION_MODEL_NAME: "meta/llama-3.2-11b-vision-instruct"

  AUDIO_GRPC_ENDPOINT: "audio:50051"
  AUDIO_INFER_PROTOCOL: "grpc"

  MRC_IGNORE_NUMA_CHECK: 1
  READY_CHECK_ALL_COMPONENTS: "true"
  MODEL_PREDOWNLOAD_PATH: "/workspace/models/"

## @section Open Telemetry
## @descriptionStart
## Define environment variables as key/value dictionary pairs for configuring OTEL Deployments
## A sane set of parameters is set for the deployed version of OpenTelemetry with this Helm Chart.
## Override any values to the Open Telemetry helm chart by overriding the `open-telemetry` value.
## @descriptionEnd

## @param otelEnabled [default: true] Whether to enable OTEL collection
otelEnabled: true
## @param otelDeployed [default: true] Whether to deploy OTEL from this helm chart
otelDeployed: true

## @skip opentelemetry-collector
## @extra opentelemetry-collector [default: sane {}] Configures the opentelemetry helm chart - see https://github.com/open-telemetry/opentelemetry-helm-charts/blob/main/charts/opentelemetry-collector/values.yaml
opentelemetry-collector:
  mode: deployment
  config:
    receivers:
      otlp:
        protocols:
          grpc:
            endpoint: '${env:MY_POD_IP}:4317'
          http:
            cors:
              allowed_origins:
                - "*"
    exporters:
      # NOTE: Prior to v0.86.0 use `logging` instead of `debug`.
      zipkin:
        endpoint: "http://nv-ingest-zipkin:9411/api/v2/spans"
      debug:
        verbosity: detailed
    extensions:
      health_check: {}
      zpages:
        endpoint: 0.0.0.0:55679
    processors:
      batch: {}
      tail_sampling:
        # filter out health checks
        # https://github.com/open-telemetry/opentelemetry-collector/issues/2310#issuecomment-1268157484
        policies:
          - name: drop_noisy_traces_url
            type: string_attribute
            string_attribute:
              key: http.target
              values:
                - \/health
              enabled_regex_matching: true
              invert_match: true
      transform:
        trace_statements:
          - context: span
            statements:
              - set(status.code, 1) where attributes["http.path"] == "/health"

              # after the http target has been anonymized, replace other aspects of the span
              - replace_match(attributes["http.route"], "/v1", attributes["http.target"]) where attributes["http.target"] != nil

              # replace the title of the span with the route to be more descriptive
              - replace_pattern(name, "/v1", attributes["http.route"]) where attributes["http.route"] != nil

              # set the route to equal the URL if it's nondescriptive (for the embedding case)
              - set(name, Concat([name, attributes["http.url"]], " ")) where name == "POST"
    service:
      extensions: [zpages, health_check]
      pipelines:
        traces:
          receivers: [otlp]
          exporters: [debug, zipkin]
          processors: [tail_sampling, transform]
        metrics:
          receivers: [otlp]
          exporters: [debug]
          processors: [batch]
        logs:
          receivers: [otlp]
          exporters: [debug]
          processors: [batch]

## @param otelEnvVars [default: sane {}] Adds arbitrary environment variables for configuring OTEL using key-value pairs, for example NAME: value
## @extra otelEnvVars.OTEL_EXPORTER_OTLP_ENDPOINT Default deployment target for GRPC otel - Default "http://{{ .Release.Name }}-opentelemetry-collector:4317"
## @param otelEnvVars.OTEL_SERVICE_NAME [default: "nemo-retrieval-service" ]
## @param otelEnvVars.OTEL_TRACES_EXPORTER [default: "otlp" ]
## @param otelEnvVars.OTEL_METRICS_EXPORTER [default: "otlp" ]
## @param otelEnvVars.OTEL_LOGS_EXPORTER [default: "none" ]
## @param otelEnvVars.OTEL_PROPAGATORS [default: "tracecontext baggage" ]
## @param otelEnvVars.OTEL_RESOURCE_ATTRIBUTES [default: "deployment.environment=$(NAMESPACE)" ]
## @param otelEnvVars.OTEL_PYTHON_EXCLUDED_URLS [default: "health" ]
otelEnvVars:
  # OpenTelemetry
  OTEL_SERVICE_NAME: "nemo-retrieval-service"
  OTEL_TRACES_EXPORTER: otlp
  OTEL_METRICS_EXPORTER: otlp
  OTEL_LOGS_EXPORTER: none
  OTEL_PROPAGATORS: "tracecontext,baggage"
  OTEL_RESOURCE_ATTRIBUTES: "deployment.environment=$(NAMESPACE)"
  OTEL_PYTHON_EXCLUDED_URLS: "health"


## @param zipkinDeployed [default: true] Whether to deploy Zipkin with OpenTelemetry from this helm chart
zipkinDeployed: true

## @section Ingress parameters
## @param ingress.enabled Enables ingress.
## @param ingress.className Specify class name for Ingress.
## @param ingress.annotations Specify additional annotations for ingress.
## @extra ingress.hosts Specify list of hosts each containing lists of paths.
## @param ingress.hosts[0].host Specify name of host.
## @param ingress.hosts[0].paths[0].path Specify ingress path.
## @param ingress.hosts[0].paths[0].pathType Specify path type.
## @param ingress.tls Specify list of pairs of TLS `secretName` and hosts.
ingress:
  enabled: false
  className: ""
  annotations: {}
  hosts:
  - host: chart-example.local
    paths:
    - path: /
      pathType: ImplementationSpecific
  tls: []

## @section Probe parameters
## @param livenessProbe.enabled Enables `livenessProbe``
## @param livenessProbe.httpGet.path `LivenessProbe`` endpoint path
## @param livenessProbe.httpGet.port `LivenessProbe`` endpoint port
## @param livenessProbe.initialDelaySeconds Initial delay seconds for `livenessProbe`
## @param livenessProbe.timeoutSeconds Timeout seconds for `livenessProbe`
## @param livenessProbe.periodSeconds Period seconds for `livenessProbe`
## @param livenessProbe.successThreshold Success threshold for `livenessProbe`
## @param livenessProbe.failureThreshold Failure threshold for `livenessProbe`
livenessProbe:
  enabled: false
  httpGet:
    path: /v1/health/live
    port: http
  initialDelaySeconds: 120
  periodSeconds: 10
  timeoutSeconds: 20
  failureThreshold: 20
  successThreshold: 1

## @section Probe parameters
## @param readinessProbe.enabled Enables `readinessProbe``
## @param readinessProbe.httpGet.path `ReadinessProbe`` endpoint path
## @param readinessProbe.httpGet.port `ReadinessProbe`` endpoint port
## @param readinessProbe.initialDelaySeconds Initial delay seconds for `readinessProbe`
## @param readinessProbe.timeoutSeconds Timeout seconds for `readinessProbe`
## @param readinessProbe.periodSeconds Period seconds for `readinessProbe`
## @param readinessProbe.successThreshold Success threshold for `readinessProbe`
## @param readinessProbe.failureThreshold Failure threshold for `readinessProbe`
readinessProbe:
  enabled: false
  httpGet:
    path: /v1/health/ready
    port: http
  initialDelaySeconds: 120
  periodSeconds: 30
  timeoutSeconds: 10
  failureThreshold: 220
  successThreshold: 1

## @section Service parameters
## @param service.type Specifies the service type for the deployment.
## @param service.name Overrides the default service name
## @param service.port Specifies the HTTP Port for the service.
## @param service.nodePort Specifies an optional HTTP Node Port for the service.
## @param service.annotations [object] Specify additional annotations to be added to service.
## @param service.labels [object] Specifies additional labels to be added to service.
service:
  type: ClusterIP
  port: 7670
  annotations: {}
  labels: {}
  name: ""  # override the default service name
  nodePort: null

## @extra serviceAccount Options to specify service account for the deployment.
## @param serviceAccount.create Specifies whether a service account should be created.
## @param serviceAccount.annotations [object] Sets annotations to be added to the service account.
## @param serviceAccount.name Specifies the name of the service account to use. If it is not set and create is "true", a name is generated using a "fullname" template.
## @skip serviceAccount.automount
serviceAccount:
  annotations: {}
  automount: true
  create: true
  name: ""

## @section Secret Creation
## @descriptionStart
## Manage the creation of secrets used by the helm chart
## @descriptionEnd

# ngcApi:
# # If set to false, the chart expects a secret with the name
#   create: false
#   password: ""

## @param ngcApiSecret.create Specifies whether to create the ngc api secret
## @param ngcApiSecret.password The password to use for the NGC Secret
ngcApiSecret:
  # If set to false, the chart expects a secret with name ngc-api to exist in the namespace
  # credentials are needed.
  create: false
  password: ""

## @param ngcImagePullSecret.create Specifies whether to create the NVCR Image Pull secret
## @param ngcImagePullSecret.password The password to use for the NVCR Image Pull Secret
## @skip ngcImagePullSecret.registry
## @skip ngcImagePullSecret.name
## @skip ngcImagePullSecret.username
ngcImagePullSecret:
  create: false
  # Leave blank, if no imagePullSecret is needed.
  registry: "nvcr.io"
  name: "ngcImagePullSecret"
  # If set to false, the chart expects either a imagePullSecret
  # with the name configured above to be present on the cluster or that no
  # credentials are needed.

  username: '$oauthtoken'
  password: ""

## @skip nemo
nemo:
  userID: "1000"
  groupID: "1000"

## @skip containerArgs
containerArgs: []
