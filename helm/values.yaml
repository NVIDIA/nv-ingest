## @section Deployment parameters
## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity
## @param affinity [object] [default: {}] Affinity settings for deployment.
affinity: {}

## @param nodeSelector [object] Sets node selectors for the NIM -- for example `nvidia.com/gpu.present: "true"`
nodeSelector: {}

## @param logLevel Log level of NVIngest service. Possible values of the variable are TRACE, DEBUG, INFO, WARNING, ERROR, CRITICAL.
logLevel: DEFAULT

## @param extraEnvVarsCM [string] [default: ""] A Config map holding Environment variables to include in the NVIngest container
extraEnvVarsCM: ""

## @param extraEnvVarsSecret [string] [default: ""] A K8S Secret to map to Environment variables to include in the NVIngest container
extraEnvVarsSecret: ""

## @param fullnameOverride [string] [default: ""] A name to force the fullname of the NVIngest container to have, defaults to the Helm Release Name
fullnameOverride: ""

## @param nameOverride [string] [default: ""] A name to base the objects created by this helm chart
nameOverride: ""

## @section Image Configuration
## @param image.repository [string] NIM Image Repository
## @param image.tag [string] Image tag or version
## @param image.pullPolicy [string] Image pull policy
image:
  pullPolicy: IfNotPresent
  repository: "nvcr.io/nvidia/nemo-microservices/nv-ingest"
  tag: "latest"

## @section Pod Configuration
## @param podAnnotations [object] Sets additional annotations on the main deployment pods
podAnnotations:
  traffic.sidecar.istio.io/excludeOutboundPorts: '8007'

## @param podLabels [object] Specify extra labels to be add to on deployed pods.
podLabels: {}

## @param podSecurityContext.fsGroup Specify file system owner group id.
podSecurityContext:
  fsGroup: 1000

## @param extraVolumes [object] Adds arbitrary additional volumes to the deployment set definition
extraVolumes: {}

## @param extraVolumeMounts [object] Specify volume mounts to the main container from `extraVolumes`
extraVolumeMounts: {}

## @section Image Pull Secrets
## @param imagePullSecrets[].name List of secret names needed for the main container and any init containers
imagePullSecrets:
  - name: ngc-api
  - name: ngc-secret

## @param containerSecurityContext [object] Sets privilege and access control settings for container (Only affects the main container, not pod-level)
containerSecurityContext: {}

## @param tolerations [array] Specify tolerations for pod assignment. Allows the scheduler to schedule pods with matching taints.
tolerations: []

## @param replicaCount [default: 1] The number of replicas for NVIngest when autoscaling is disabled
replicaCount: 1

## @section Resource Configuration
## @param resources.limits.memory [default: 200Gi] Specify limit for memory
## @param resources.limits.cpu [default: "48000m"] Specify limit for CPU
## @param resources.requests.memory [default: 24Gi] Specify request for memory
## @param resources.requests.cpu [default: "24000m"] Specify request for CPU
resources:
  limits:
    memory: 200Gi
    cpu: "48000m"
  requests:
    memory: 24Gi
    cpu: "24000m"

## @param tmpDirSize [default: 50Gi] Specify the amount of space to reserve for temporary storage
tmpDirSize: 50Gi

## @section NIM Configuration
## @descriptionStart
## Define additional values to the dependent NIM helm charts by updating the "nemoretriever-page-elements-v2", "nemoretriever-graphic-elements-v1",
## "nemoretriever-table-structure-v1", "nemoretriever-parse", and "paddle" values. A sane set of configurations are already included in this value
## file and only the "image.repository" and "image.tag" fields are explicitly called out here.
## @descriptionEnd

## @section NIM Components
## @param nemoretriever-page-elements-v2.deployed [default: true] true if the Yolox NIM should be deployed and false otherwise
## @param nemoretriever-page-elements-v2.image.repository The repository to override the location of the YOLOX
## @param nemoretriever-page-elements-v2.image.tag The tag override for YOLOX
## @param nemoretriever-page-elements-v2.image.pullPolicy [default: IfNotPresent] The pull policy for the YOLOX image
## @param nemoretriever-page-elements-v2.podSecurityContext.runAsUser [default: 1000] The user ID to run the YOLOX container as
## @param nemoretriever-page-elements-v2.podSecurityContext.runAsGroup [default: 1000] The group ID to run the YOLOX container as
## @param nemoretriever-page-elements-v2.podSecurityContext.fsGroup [default: 1000] The filesystem group ID for YOLOX volumes
## @param nemoretriever-page-elements-v2.replicaCount [default: 1] Number of YOLOX replicas when autoscaling is disabled
## @param nemoretriever-page-elements-v2.serviceAccount.create [default: false] Whether to create a service account for YOLOX
## @param nemoretriever-page-elements-v2.serviceAccount.name [default: ""] The name of the service account to use
## @param nemoretriever-page-elements-v2.statefuleSet.enabled [default: false] Whether to deploy YOLOX as a statefulset
## @param nemoretriever-page-elements-v2.autoscaling.enabled [default: false] Enable autoscaling for YOLOX
## @param nemoretriever-page-elements-v2.autoscaling.minReplicas [default: 1] Minimum number of YOLOX replicas
## @param nemoretriever-page-elements-v2.autoscaling.maxReplicas [default: 10] Maximum number of YOLOX replicas
## @param nemoretriever-page-elements-v2.autoscaling.metrics [default: []] Metrics to use for autoscaling
## @param nemoretriever-page-elements-v2.service.type [default: "ClusterIP"] The type of Kubernetes service to create
## @param nemoretriever-page-elements-v2.service.name [default: "nemoretriever-page-elements-v2"] The name of the service
## @param nemoretriever-page-elements-v2.service.httpPort [default: 8000] The HTTP port for the service
## @param nemoretriever-page-elements-v2.service.grpcPort [default: 8001] The gRPC port for the service
## @param nemoretriever-page-elements-v2.service.metricsPort [default: 0] The metrics port for the service
## @param nemoretriever-page-elements-v2.nim.grpcPort [default: 8001] The gRPC port for NIM communication
## @param nemoretriever-page-elements-v2.nim.logLevel [default: "INFO"] The log level for NIM
## @param nemoretriever-page-elements-v2.env Environment variables for the YOLOX container
nemoretriever-page-elements-v2:
  deployed: true
  image:
    repository: nvcr.io/nim/nvidia/nemoretriever-page-elements-v2
    tag: "1.4.0"
    pullPolicy: IfNotPresent
  podSecurityContext:
    runAsUser: 1000
    runAsGroup: 1000
    fsGroup: 1000
  replicaCount: 1
  serviceAccount:
    create: false
    name: ""
  statefuleSet:
    enabled: false
  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 10
    metrics: []
  service:
    type: "ClusterIP"
    name: nemoretriever-page-elements-v2
    httpPort: 8000
    grpcPort: 8001
    metricsPort: 0  # Generally unused and defaults to 0
  nim:
    grpcPort: 8001
    logLevel: "INFO"
  env:
    - name: NIM_HTTP_API_PORT
      value: "8000"
    - name: NIM_TRITON_OPTIMIZATION_MODE
      value: vram_opt
    - name: NIM_TRITON_CUDA_MEMORY_POOL_MB
      value: "768"

## @skip nemoretriever-graphic-elements-v1
## @param nemoretriever-graphic-elements-v1.deployed [default: true] true if the nemoretriever-graphic-elements NIM should be deployed and false otherwise
## @param nemoretriever-graphic-elements-v1.image.repository The repository to override the location of the nemoretriever-graphic-elements
## @param nemoretriever-graphic-elements-v1.image.tag The tag override for nemoretriever-graphic-elements
## @param nemoretriever-graphic-elements-v1.customCommand [default: []] Custom command to override the default container command
## @param nemoretriever-graphic-elements-v1.customArgs [default: []] Custom args to override the default container args
## @param nemoretriever-graphic-elements-v1.podSecurityContext [default: {}] Pod security context
## @param nemoretriever-graphic-elements-v1.replicaCount [default: 1] Number of replicas to deploy
## @param nemoretriever-graphic-elements-v1.serviceAccount.create [default: false] Whether to create a service account
## @param nemoretriever-graphic-elements-v1.serviceAccount.name [default: ""] The name of the service account to use
## @param nemoretriever-graphic-elements-v1.statefuleSet.enabled [default: false] Whether to deploy as a statefulset
## @param nemoretriever-graphic-elements-v1.autoscaling.enabled [default: false] Enable autoscaling
## @param nemoretriever-graphic-elements-v1.autoscaling.minReplicas [default: 1] Minimum number of replicas
## @param nemoretriever-graphic-elements-v1.autoscaling.maxReplicas [default: 10] Maximum number of replicas
## @param nemoretriever-graphic-elements-v1.autoscaling.metrics [default: []] Metrics to use for autoscaling
## @param nemoretriever-graphic-elements-v1.service.type [default: "ClusterIP"] The type of Kubernetes service to create
## @param nemoretriever-graphic-elements-v1.service.name [default: "nemoretriever-graphic-elements-v1"] The name of the service
## @param nemoretriever-graphic-elements-v1.service.httpPort [default: 8000] The HTTP port for the service
## @param nemoretriever-graphic-elements-v1.service.grpcPort [default: 8001] The gRPC port for the service
## @param nemoretriever-graphic-elements-v1.service.metricsPort [default: 0] The metrics port for the service
## @param nemoretriever-graphic-elements-v1.nim.grpcPort [default: 8001] The gRPC port for NIM communication
## @param nemoretriever-graphic-elements-v1.nim.logLevel [default: "INFO"] The log level for NIM
## @param nemoretriever-graphic-elements-v1.env Environment variables for the container
nemoretriever-graphic-elements-v1:
  deployed: true
  customCommand: []
  customArgs: []
  image:
    repository: nvcr.io/nim/nvidia/nemoretriever-graphic-elements-v1
    tag: "1.4.0"
    pullPolicy: IfNotPresent
  podSecurityContext:
    runAsUser: 1000
    runAsGroup: 1000
    fsGroup: 1000
  replicaCount: 1
  serviceAccount:
    create: false
    name: ""
  statefuleSet:
    enabled: false
  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 10
    metrics: []
  service:
    type: "ClusterIP"
    name: nemoretriever-graphic-elements-v1
    httpPort: 8000
    grpcPort: 8001
    metricsPort: 0  # Generally unused and defaults to 0
  nim:
    grpcPort: 8001
    logLevel: "INFO"
  env:
    - name: NIM_HTTP_API_PORT
      value: "8000"
    - name: NIM_TRITON_OPTIMIZATION_MODE
      value: vram_opt
    - name: NIM_TRITON_CUDA_MEMORY_POOL_MB
      value: "768"

## @skip nemoretriever-table-structure-v1
## @param nemoretriever-table-structure-v1.deployed [default: true] true if the nemoretriever-table-structure NIM should be deployed and false otherwise
## @param nemoretriever-table-structure-v1.image.repository The repository to override the location of the nemoretriever-table-structure
## @param nemoretriever-table-structure-v1.image.tag The tag override for nemoretriever-table-structure
## @param nemoretriever-table-structure-v1.customCommand [default: []] Custom command to override the default container command
## @param nemoretriever-table-structure-v1.customArgs [default: []] Custom args to override the default container args
## @param nemoretriever-table-structure-v1.podSecurityContext [default: {runAsUser: 1000, runAsGroup: 1000, fsGroup: 1000}] Security context for the pod
## @param nemoretriever-table-structure-v1.replicaCount [default: 1] Number of replicas to deploy
## @param nemoretriever-table-structure-v1.serviceAccount.create [default: false] Whether to create a service account
## @param nemoretriever-table-structure-v1.serviceAccount.name [default: ""] Name of the service account
## @param nemoretriever-table-structure-v1.statefuleSet.enabled [default: false] Whether to deploy as a statefulset
## @param nemoretriever-table-structure-v1.autoscaling.enabled [default: false] Enable autoscaling
## @param nemoretriever-table-structure-v1.autoscaling.minReplicas [default: 1] Minimum number of replicas
## @param nemoretriever-table-structure-v1.autoscaling.maxReplicas [default: 10] Maximum number of replicas
## @param nemoretriever-table-structure-v1.autoscaling.metrics [default: []] Metrics to use for autoscaling
## @param nemoretriever-table-structure-v1.service.type [default: "ClusterIP"] The type of Kubernetes service to create
## @param nemoretriever-table-structure-v1.service.name [default: "nemoretriever-table-structure-v1"] The name of the service
## @param nemoretriever-table-structure-v1.service.httpPort [default: 8000] The HTTP port for the service
## @param nemoretriever-table-structure-v1.service.grpcPort [default: 8001] The gRPC port for the service
## @param nemoretriever-table-structure-v1.service.metricsPort [default: 0] The metrics port for the service
## @param nemoretriever-table-structure-v1.nim.grpcPort [default: 8001] The gRPC port for NIM communication
## @param nemoretriever-table-structure-v1.nim.logLevel [default: "INFO"] The log level for NIM
## @param nemoretriever-table-structure-v1.env Environment variables for the container
nemoretriever-table-structure-v1:
  deployed: true
  # fullnameOverride: nemoretriever-table-structure-v1
  customCommand: []
  customArgs: []
  image:
    repository: nvcr.io/nim/nvidia/nemoretriever-table-structure-v1
    tag: "1.4.0"
    pullPolicy: IfNotPresent
  podSecurityContext:
    runAsUser: 1000
    runAsGroup: 1000
    fsGroup: 1000
  replicaCount: 1
  serviceAccount:
    create: false
    name: ""
  statefuleSet:
    enabled: false
  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 10
    metrics: []
  service:
    type: "ClusterIP"
    name: nemoretriever-table-structure-v1
    httpPort: 8000
    grpcPort: 8001
    metricsPort: 0  # Generally unused and defaults to 0
  nim:
    grpcPort: 8001
    logLevel: "INFO"
  env:
    - name: NIM_HTTP_API_PORT
      value: "8000"
    - name: NIM_TRITON_OPTIMIZATION_MODE
      value: vram_opt
    - name: NIM_TRITON_CUDA_MEMORY_POOL_MB
      value: "768"

## @section NIM VLM Image Captioning Configuration
## @descriptionStart
## Configuration for the VLM (Vision Language Model) image captioning service.
## This service provides AI-powered image captioning capabilities.
## @descriptionEnd
## @param nim-vlm-image-captioning.deployed [default: false] Whether to deploy the VLM image captioning service
## @param nim-vlm-image-captioning.fullnameOverride [default: "nim-vlm-image-captioning"] Override the full name of the deployment
## @param nim-vlm-image-captioning.customCommand [default: []] Custom command to run in the container
## @param nim-vlm-image-captioning.customArgs [default: []] Custom arguments to pass to the container command
## @param nim-vlm-image-captioning.image.repository [default: "nvcr.io/nim/nvidia/nemotron-nano-12b-v2-vl"] The container image repository
## @param nim-vlm-image-captioning.image.tag [default: "latest"] The container image tag
## @param nim-vlm-image-captioning.podSecurityContext [default: {}] Pod security context configuration
## @param nim-vlm-image-captioning.replicaCount [default: 1] Number of replicas to deploy
## @param nim-vlm-image-captioning.service.type [default: "ClusterIP"] The type of Kubernetes service to create
## @param nim-vlm-image-captioning.service.name [default: "nim-vlm-image-captioning"] The name of the service
## @param nim-vlm-image-captioning.service.httpPort [default: 8000] The HTTP port for the service
## @param nim-vlm-image-captioning.service.grpcPort [default: 8001] The gRPC port for the service
## @param nim-vlm-image-captioning.service.metricsPort [default: 0] The metrics port for the service
## @param nim-vlm-image-captioning.nim.grpcPort [default: 8001] The gRPC port for NIM communication
## @param nim-vlm-image-captioning.nim.logLevel [default: "INFO"] The log level for NIM
## @param nim-vlm-image-captioning.env [default: []] Environment variables for the container
nim-vlm-image-captioning:
  deployed: false
  fullnameOverride: nim-vlm-image-captioning
  customCommand: []
  customArgs: []
  image:
    repository: nvcr.io/nim/nvidia/nemotron-nano-12b-v2-vl
    tag: "1"
  podSecurityContext:
    runAsUser: 1000
    runAsGroup: 1000
    fsGroup: 1000
  replicaCount: 1
  service:
    type: "ClusterIP"
    name: nim-vlm-image-captioning
    httpPort: 8000
    grpcPort: 8001
    metricsPort: 0  # Generally unused and defaults to 0
  nim:
    grpcPort: 8001
    logLevel: "INFO"
  env:
    - name: NIM_HTTP_API_PORT
      value: "8000"
    - name: NIM_TRITON_OPTIMIZATION_MODE
      value: vram_opt
    - name: NIM_TRITON_CUDA_MEMORY_POOL_MB
      value: "768"

## @section NIM VLM Text Extraction Configuration
## @descriptionStart
## Configuration for the VLM (Vision Language Model) text extraction service.
## This service provides AI-powered text extraction capabilities from images using NeMo Retriever.
## @descriptionEnd
## @param nim-vlm-text-extraction.deployed [default: false] Whether to deploy the VLM text extraction service
## @param nim-vlm-text-extraction.fullnameOverride [default: "nim-vlm-text-extraction-nemoretriever-parse"] Override the full name of the deployment
## @param nim-vlm-text-extraction.customCommand [default: []] Custom command to run in the container
## @param nim-vlm-text-extraction.customArgs [default: []] Custom arguments to pass to the container command
## @param nim-vlm-text-extraction.image.repository [default: "nvcr.io/nvidia/nemo-microservices/nemoretriever-parse"] The container image repository
## @param nim-vlm-text-extraction.image.tag [default: "1.2.0ea"] The container image tag
## @param nim-vlm-text-extraction.podSecurityContext [default: {}] Pod security context configuration
## @param nim-vlm-text-extraction.replicaCount [default: 1] Number of replicas to deploy
## @param nim-vlm-text-extraction.service.type [default: "ClusterIP"] The type of Kubernetes service to create
## @param nim-vlm-text-extraction.service.name [default: "nim-vlm-text-extraction-nemoretriever-parse"] The name of the service
## @param nim-vlm-text-extraction.service.httpPort [default: 8000] The HTTP port for the service
## @param nim-vlm-text-extraction.service.grpcPort [default: 8001] The gRPC port for the service
## @param nim-vlm-text-extraction.service.metricsPort [default: 0] The metrics port for the service
## @param nim-vlm-text-extraction.nim.grpcPort [default: 8001] The gRPC port for NIM communication
## @param nim-vlm-text-extraction.nim.logLevel [default: "INFO"] The log level for NIM
## @param nim-vlm-text-extraction.env [default: []] Environment variables for the container
nim-vlm-text-extraction:
  deployed: false
  fullnameOverride: nim-vlm-text-extraction-nemoretriever-parse
  customCommand: []
  customArgs: []
  image:
    repository: nvcr.io/nvidia/nemo-microservices/nemoretriever-parse
    tag: "1.2.0ea"
  podSecurityContext:
    runAsUser: 1000
    runAsGroup: 1000
    fsGroup: 1000
  replicaCount: 1
  service:
    type: "ClusterIP"
    name: nim-vlm-text-extraction-nemoretriever-parse
    httpPort: 8000
    grpcPort: 8001
    metricsPort: 0  # Generally unused and defaults to 0
  nim:
    grpcPort: 8001
    logLevel: "INFO"
  env:
    - name: NIM_HTTP_API_PORT
      value: "8000"
    - name: NIM_TRITON_OPTIMIZATION_MODE
      value: vram_opt
    - name: NIM_TRITON_CUDA_MEMORY_POOL_MB
      value: "768"

## @section NemoRetriever OCR NIM Configuration
## @descriptionStart
## Configuration for the NemoRetriever OCR NIM service.
## This service provides OCR (Optical Character Recognition) capabilities using NemoRetriever OCR.
## @descriptionEnd
## @param nemoretriever-ocr.deployed [default: false] Whether to deploy the PaddleOCR NIM service
## @param nemoretriever-ocr.fullnameOverride [default: "nv-ingest-ocr"] Override the full name of the deployment
## @param nemoretriever-ocr.customCommand [default: []] Custom command to run in the container
## @param nemoretriever-ocr.customArgs [default: []] Custom arguments to pass to the container command
## @param nemoretriever-ocr.image.repository [default: "nvcr.io/nim/baidu/paddleocr"] The container image repository
## @param nemoretriever-ocr.image.tag [default: "1.2.0"] The container image tag
## @param nemoretriever-ocr.podSecurityContext [default: {}] Pod security context configuration
## @param nemoretriever-ocr.replicaCount [default: 1] Number of replicas to deploy
## @param nemoretriever-ocr.serviceAccount.create [default: false] Whether to create a service account
## @param nemoretriever-ocr.serviceAccount.name [default: ""] The name of the service account
## @param nemoretriever-ocr.statefuleSet.enabled [default: false] Whether to deploy as a StatefulSet
## @param nemoretriever-ocr.service.type [default: "ClusterIP"] The type of Kubernetes service to create
## @param nemoretriever-ocr.service.name [default: "nv-ingest-ocr"] The name of the service
## @param nemoretriever-ocr.service.httpPort [default: 8000] The HTTP port for the service
## @param nemoretriever-ocr.service.grpcPort [default: 8001] The gRPC port for the service
## @param nemoretriever-ocr.service.metricsPort [default: 0] The metrics port for the service
## @param nemoretriever-ocr.nim.grpcPort [default: 8001] The gRPC port for NIM communication
## @param nemoretriever-ocr.nim.logLevel [default: "INFO"] The log level for NIM
## @param nemoretriever-ocr.env [default: []] Environment variables for the container
nemoretriever-ocr:
  deployed: false
  fullnameOverride: nv-ingest-ocr
  customCommand: []
  customArgs: []
  image:
    repository: nvcr.io/nvidia/nemo-microservices/nemoretriever-ocr-v1
    tag: "1.0.0"
  podSecurityContext:
    runAsUser: 1000
    runAsGroup: 1000
    fsGroup: 1000
  replicaCount: 1
  serviceAccount:
    create: false
    name: ""
  statefuleSet:
    enabled: false
  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 10
    metrics: []
  service:
    type: "ClusterIP"
    name: nv-ingest-ocr
    httpPort: 8000
    grpcPort: 8001
    metricsPort: 0  # Generally unused and defaults to 0
  nim:
    grpcPort: 8001
    logLevel: "INFO"
  env:
    - name: NIM_HTTP_API_PORT
      value: "8000"
    # - name: NIM_TRITON_MAX_BATCH_SIZE
    #   value: "32"
    - name: NIM_TRITON_OPTIMIZATION_MODE
      value: vram_opt
    - name: NIM_TRITON_CUDA_MEMORY_POOL_MB
      value: "768"

## @section PaddleOCR NIM Configuration
## @descriptionStart
## Configuration for the PaddleOCR NIM service.
## This service provides OCR (Optical Character Recognition) capabilities using PaddleOCR.
## @descriptionEnd
## @param paddleocr-nim.deployed [default: true] Whether to deploy the PaddleOCR NIM service
## @param paddleocr-nim.fullnameOverride [default: "nv-ingest-ocr"] Override the full name of the deployment
## @param paddleocr-nim.customCommand [default: []] Custom command to run in the container
## @param paddleocr-nim.customArgs [default: []] Custom arguments to pass to the container command
## @param paddleocr-nim.image.repository [default: "nvcr.io/nim/baidu/paddleocr"] The container image repository
## @param paddleocr-nim.image.tag [default: "1.2.0"] The container image tag
## @param paddleocr-nim.podSecurityContext [default: {}] Pod security context configuration
## @param paddleocr-nim.replicaCount [default: 1] Number of replicas to deploy
## @param paddleocr-nim.serviceAccount.create [default: false] Whether to create a service account
## @param paddleocr-nim.serviceAccount.name [default: ""] The name of the service account
## @param paddleocr-nim.statefuleSet.enabled [default: false] Whether to deploy as a StatefulSet
## @param paddleocr-nim.service.type [default: "ClusterIP"] The type of Kubernetes service to create
## @param paddleocr-nim.service.name [default: "nv-ingest-ocr"] The name of the service
## @param paddleocr-nim.service.httpPort [default: 8000] The HTTP port for the service
## @param paddleocr-nim.service.grpcPort [default: 8001] The gRPC port for the service
## @param paddleocr-nim.service.metricsPort [default: 0] The metrics port for the service
## @param paddleocr-nim.nim.grpcPort [default: 8001] The gRPC port for NIM communication
## @param paddleocr-nim.nim.logLevel [default: "INFO"] The log level for NIM
## @param paddleocr-nim.env [default: []] Environment variables for the container
paddleocr-nim:
  deployed: true
  fullnameOverride: nv-ingest-ocr
  customCommand: []
  customArgs: []
  image:
    repository: nvcr.io/nim/baidu/paddleocr
    tag: "1.4.0"
  podSecurityContext:
    runAsUser: 1000
    runAsGroup: 1000
    fsGroup: 1000
  replicaCount: 1
  serviceAccount:
    create: false
    name: ""
  statefuleSet:
    enabled: false
  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 10
    metrics: []
  service:
    type: "ClusterIP"
    name: nv-ingest-ocr
    httpPort: 8000
    grpcPort: 8001
    metricsPort: 0  # Generally unused and defaults to 0
  nim:
    grpcPort: 8001
    logLevel: "INFO"
  env:
    - name: NIM_HTTP_API_PORT
      value: "8000"
    - name: NIM_TRITON_OPTIMIZATION_MODE
      value: vram_opt
    - name: NIM_TRITON_CUDA_MEMORY_POOL_MB
      value: "768"


## @skip text-embedding-nim
## @param text-embedding-nim.deployed [default: false] true if the text-embedding-nim should be deployed and false otherwise
## @param text-embedding-nim.image.repository The repository to override the location of the text-embedding-nim
## @param text-embedding-nim.image.tag The tag override for text-embedding-nim
## @param text-embedding-nim.fullnameOverride [default: "nv-ingest-embedqa"] The name override for the deployment
## @param text-embedding-nim.customCommand [default: []] Custom command to override container entrypoint
## @param text-embedding-nim.customArgs [default: []] Custom arguments to pass to the container
## @param text-embedding-nim.podSecurityContext Security context for the pod
## @param text-embedding-nim.replicaCount [default: 1] Number of replicas to deploy
## @param text-embedding-nim.serviceAccount.create [default: false] Whether to create a service account
## @param text-embedding-nim.serviceAccount.name [default: ""] The name of the service account
## @param text-embedding-nim.statefuleSet.enabled [default: false] Whether to deploy as a StatefulSet
## @param text-embedding-nim.autoscaling.enabled [default: false] Enable autoscaling
## @param text-embedding-nim.autoscaling.minReplicas [default: 1] Minimum number of replicas
## @param text-embedding-nim.autoscaling.maxReplicas [default: 10] Maximum number of replicas
## @param text-embedding-nim.autoscaling.metrics [default: []] Metrics for autoscaling
## @param text-embedding-nim.service.type [default: "ClusterIP"] The type of Kubernetes service to create
## @param text-embedding-nim.service.name [default: "nv-ingest-embedqa"] The name of the service
## @param text-embedding-nim.service.httpPort [default: 8000] The HTTP port for the service
## @param text-embedding-nim.service.grpcPort [default: 8001] The gRPC port for the service
## @param text-embedding-nim.service.metricsPort [default: 0] The metrics port for the service
## @param text-embedding-nim.nim.grpcPort [default: 8001] The gRPC port for NIM communication
## @param text-embedding-nim.nim.logLevel [default: "INFO"] The log level for NIM
## @param text-embedding-nim.env [default: []] Environment variables for the container
text-embedding-nim:
  deployed: false
  fullnameOverride: nv-ingest-embedqa # Share name with nvidia-nim-llama-32-nv-embedqa-1b-v2 to ease configuration
  customCommand: []
  customArgs: []
  image:
    repository: nvcr.io/nim/nvidia/nv-embedqa-e5-v5
    tag: "1.9.0"
  podSecurityContext:
    runAsUser: 1000
    runAsGroup: 1000
    fsGroup: 1000
  replicaCount: 1
  serviceAccount:
    create: false
    name: ""
  statefuleSet:
    enabled: false
  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 10
    metrics: []
  service:
    type: "ClusterIP"
    name: nv-ingest-embedqa
    httpPort: 8000
    grpcPort: 8001
    metricsPort: 0  # Generally unused and defaults to 0
  nim:
    grpcPort: 8001
    logLevel: "INFO"
  env:
    - name: NIM_HTTP_API_PORT
      value: "8000"
    - name: NIM_TRITON_MAX_BATCH_SIZE
      value: "1"


## @skip nvidia-nim-llama-32-nv-embedqa-1b-v2
## @param nvidia-nim-llama-32-nv-embedqa-1b-v2.deployed [default: true] true if nvidia-nim-llama-32-nv-embedqa-1b-v2 should be deployed and false otherwise
## @param nvidia-nim-llama-32-nv-embedqa-1b-v2.fullnameOverride [default: "nv-ingest-embedqa"] Override for the full name of the deployment
## @param nvidia-nim-llama-32-nv-embedqa-1b-v2.customCommand [default: []] Custom command to run in the container
## @param nvidia-nim-llama-32-nv-embedqa-1b-v2.customArgs [default: []] Custom arguments to pass to the container
## @param nvidia-nim-llama-32-nv-embedqa-1b-v2.image.repository The repository to override the location of the nvEmbedqa NIM
## @param nvidia-nim-llama-32-nv-embedqa-1b-v2.image.tag The tag override for nvEmbedqa NIM
## @param nvidia-nim-llama-32-nv-embedqa-1b-v2.podSecurityContext.runAsUser [default: 1000] The user ID to run the container as
## @param nvidia-nim-llama-32-nv-embedqa-1b-v2.podSecurityContext.runAsGroup [default: 1000] The group ID to run the container as
## @param nvidia-nim-llama-32-nv-embedqa-1b-v2.podSecurityContext.fsGroup [default: 1000] The filesystem group ID to run the container as
## @param nvidia-nim-llama-32-nv-embedqa-1b-v2.replicaCount [default: 1] Number of replicas to deploy
## @param nvidia-nim-llama-32-nv-embedqa-1b-v2.serviceAccount.create [default: false] Whether to create a service account
## @param nvidia-nim-llama-32-nv-embedqa-1b-v2.serviceAccount.name [default: ""] The name of the service account to use
## @param nvidia-nim-llama-32-nv-embedqa-1b-v2.statefuleSet.enabled [default: false] Whether to deploy as a statefulset
## @param nvidia-nim-llama-32-nv-embedqa-1b-v2.autoscaling.enabled [default: false] Whether to enable autoscaling
## @param nvidia-nim-llama-32-nv-embedqa-1b-v2.autoscaling.minReplicas [default: 1] Minimum number of replicas
## @param nvidia-nim-llama-32-nv-embedqa-1b-v2.autoscaling.maxReplicas [default: 10] Maximum number of replicas
## @param nvidia-nim-llama-32-nv-embedqa-1b-v2.autoscaling.metrics [default: []] Metrics for autoscaling
## @param nvidia-nim-llama-32-nv-embedqa-1b-v2.service.type [default: "ClusterIP"] The type of Kubernetes service to create
## @param nvidia-nim-llama-32-nv-embedqa-1b-v2.service.name [default: "nv-ingest-embedqa"] The name of the service
## @param nvidia-nim-llama-32-nv-embedqa-1b-v2.service.httpPort [default: 8000] The HTTP port for the service
## @param nvidia-nim-llama-32-nv-embedqa-1b-v2.service.grpcPort [default: 8001] The gRPC port for the service
## @param nvidia-nim-llama-32-nv-embedqa-1b-v2.service.metricsPort [default: 0] The metrics port for the service
## @param nvidia-nim-llama-32-nv-embedqa-1b-v2.nim.grpcPort [default: 8001] The gRPC port for NIM communication
## @param nvidia-nim-llama-32-nv-embedqa-1b-v2.nim.logLevel [default: "INFO"] The log level for NIM
## @param nvidia-nim-llama-32-nv-embedqa-1b-v2.env [default: []] Environment variables for the container
nvidia-nim-llama-32-nv-embedqa-1b-v2:
  deployed: true
  fullnameOverride: nv-ingest-embedqa # Share name with text-embedding-nim to ease configuration
  customCommand: []
  customArgs: []
  image:
    repository: nvcr.io/nim/nvidia/llama-3.2-nv-embedqa-1b-v2
    tag: "1.9.0"
  podSecurityContext:
    runAsUser: 1000
    runAsGroup: 1000
    fsGroup: 1000
  replicaCount: 1
  serviceAccount:
    create: false
    name: ""
  statefuleSet:
    enabled: false
  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 10
    metrics: []
  service:
    type: "ClusterIP"
    name: nv-ingest-embedqa
    httpPort: 8000
    grpcPort: 8001
    metricsPort: 0  # Generally unused and defaults to 0
  nim:
    grpcPort: 8001
    logLevel: "INFO"
  env:
    - name: NIM_HTTP_API_PORT
      value: "8000"
    - name: NIM_TRITON_MAX_BATCH_SIZE
      value: "1"


## @skip vlm-embedding-nim
## @param vlm-embedding-nim.deployed [default: false] true if the vlm-embedding-nim should be deployed and false otherwise
## @param vlm-embedding-nim.image.repository The repository to override the location of the vlm-embedding-nim
## @param vlm-embedding-nim.image.tag The tag override for vlm-embedding-nim
## @param vlm-embedding-nim.fullnameOverride [default: "nv-ingest-embedqa"] The name override for the deployment
## @param vlm-embedding-nim.customCommand [default: []] Custom command to override container entrypoint
## @param vlm-embedding-nim.customArgs [default: []] Custom arguments to pass to the container
## @param vlm-embedding-nim.podSecurityContext Security context for the pod
## @param vlm-embedding-nim.replicaCount [default: 1] Number of replicas to deploy
## @param vlm-embedding-nim.serviceAccount.create [default: false] Whether to create a service account
## @param vlm-embedding-nim.serviceAccount.name [default: ""] The name of the service account
## @param vlm-embedding-nim.statefuleSet.enabled [default: false] Whether to deploy as a StatefulSet
## @param vlm-embedding-nim.autoscaling.enabled [default: false] Enable autoscaling
## @param vlm-embedding-nim.autoscaling.minReplicas [default: 1] Minimum number of replicas
## @param vlm-embedding-nim.autoscaling.maxReplicas [default: 10] Maximum number of replicas
## @param vlm-embedding-nim.autoscaling.metrics [default: []] Metrics for autoscaling
## @param vlm-embedding-nim.service.type [default: "ClusterIP"] The type of Kubernetes service to create
## @param vlm-embedding-nim.service.name [default: "nv-ingest-embedqa"] The name of the service
## @param vlm-embedding-nim.service.httpPort [default: 8000] The HTTP port for the service
## @param vlm-embedding-nim.service.grpcPort [default: 8001] The gRPC port for the service
## @param vlm-embedding-nim.service.metricsPort [default: 0] The metrics port for the service
## @param vlm-embedding-nim.nim.grpcPort [default: 8001] The gRPC port for NIM communication
## @param vlm-embedding-nim.nim.logLevel [default: "INFO"] The log level for NIM
## @param vlm-embedding-nim.env [default: []] Environment variables for the container
vlm-embedding-nim:
  deployed: false
  fullnameOverride: nv-ingest-embedqa # Share name with nvidia-nim-llama-32-nv-embedqa-1b-v2 to ease configuration
  customCommand: []
  customArgs: []
  image:
    repository: nvcr.io/nvidia/nemo-microservices/llama-3.2-nemoretriever-1b-vlm-embed-v1
    tag: "1.7.0"
  podSecurityContext:
    runAsUser: 1000
    runAsGroup: 1000
    fsGroup: 1000
  replicaCount: 1
  serviceAccount:
    create: false
    name: ""
  statefuleSet:
    enabled: false
  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 10
    metrics: []
  service:
    type: "ClusterIP"
    name: nv-ingest-embedqa
    httpPort: 8000
    grpcPort: 8001
    metricsPort: 0  # Generally unused and defaults to 0
  nim:
    grpcPort: 8001
    logLevel: "INFO"
  env:
    - name: NIM_HTTP_API_PORT
      value: "8000"
    - name: NIM_TRITON_MAX_BATCH_SIZE
      value: "1"


## @skip llama-32-nv-rerankqa-1b-v2
## @param llama-32-nv-rerankqa-1b-v2.deployed [default: true] true if llama-32-nv-rerankqa-1b-v2 should be deployed and false otherwise
## @param llama-32-nv-rerankqa-1b-v2.fullnameOverride [default: "llama-32-nv-rerankqa-1b-v2"] Override for the full name of the deployment
## @param llama-32-nv-rerankqa-1b-v2.customCommand [default: []] Custom command to run in the container
## @param llama-32-nv-rerankqa-1b-v2.customArgs [default: []] Custom arguments to pass to the container
## @param llama-32-nv-rerankqa-1b-v2.image.repository The repository to override the location of the reranker NIM
## @param llama-32-nv-rerankqa-1b-v2.image.tag The tag override for reranker NIM
## @param llama-32-nv-rerankqa-1b-v2.podSecurityContext [default: {}] Security context for the pod
## @param llama-32-nv-rerankqa-1b-v2.replicaCount [default: 1] Number of replicas to deploy
## @param llama-32-nv-rerankqa-1b-v2.serviceAccount.create [default: false] Whether to create a service account
## @param llama-32-nv-rerankqa-1b-v2.serviceAccount.name [default: ""] Name of the service account
## @param llama-32-nv-rerankqa-1b-v2.statefuleSet.enabled [default: false] Whether to deploy as a statefulset
## @param llama-32-nv-rerankqa-1b-v2.autoscaling.enabled [default: false] Whether to enable autoscaling
## @param llama-32-nv-rerankqa-1b-v2.autoscaling.minReplicas [default: 1] Minimum number of replicas
## @param llama-32-nv-rerankqa-1b-v2.autoscaling.maxReplicas [default: 10] Maximum number of replicas
## @param llama-32-nv-rerankqa-1b-v2.autoscaling.metrics [default: []] Metrics for autoscaling
## @param llama-32-nv-rerankqa-1b-v2.service.type [default: "ClusterIP"] The type of Kubernetes service to create
## @param llama-32-nv-rerankqa-1b-v2.service.name [default: "llama-32-nv-rerankqa-1b-v2"] The name of the service
## @param llama-32-nv-rerankqa-1b-v2.service.httpPort [default: 8000] The HTTP port for the service
## @param llama-32-nv-rerankqa-1b-v2.service.grpcPort [default: 8001] The gRPC port for the service
## @param llama-32-nv-rerankqa-1b-v2.service.metricsPort [default: 0] The metrics port for the service
## @param llama-32-nv-rerankqa-1b-v2.nim.grpcPort [default: 8001] The gRPC port for NIM communication
## @param llama-32-nv-rerankqa-1b-v2.nim.logLevel [default: "INFO"] The log level for NIM
## @param llama-32-nv-rerankqa-1b-v2.env [default: []] Environment variables for the container
llama-32-nv-rerankqa-1b-v2:
  deployed: false
  customCommand: []
  customArgs: []
  image:
    repository: nvcr.io/nim/nvidia/llama-3.2-nv-rerankqa-1b-v2
    tag: "1.7.0"
  podSecurityContext:
    runAsUser: 1000
    runAsGroup: 1000
    fsGroup: 1000
  replicaCount: 1
  serviceAccount:
    create: false
    name: ""
  statefuleSet:
    enabled: false
  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 10
    metrics: []
  service:
    type: "ClusterIP"
    name: llama-32-nv-rerankqa-1b-v2
    httpPort: 8000
    grpcPort: 8001
    metricsPort: 0  # Generally unused and defaults to 0
  nim:
    grpcPort: 8001
    logLevel: "INFO"
  env:
    - name: NIM_HTTP_API_PORT
      value: "8000"
    - name: NIM_TRITON_MAX_BATCH_SIZE
      value: "1"


## @skip riva-nim
## @param riva-nim.deployed [default: true] true if riva-nim should be deployed and false otherwise
## @param riva-nim.fullnameOverride [default: "riva-nim"] Override for the full name of the deployment
## @param riva-nim.customCommand [default: []] Custom command to run in the container
## @param riva-nim.customArgs [default: []] Custom arguments to pass to the container
## @param riva-nim.image.repository The repository to override the location of the riva-asr NIM
## @param riva-nim.image.tag The tag override for riva-asr NIM
## @param riva-nim.podSecurityContext [default: {}] Security context for the pod
## @param riva-nim.replicaCount [default: 1] Number of replicas to deploy
## @param riva-nim.serviceAccount.create [default: false] Whether to create a service account
## @param riva-nim.serviceAccount.name [default: ""] Name of the service account
## @param riva-nim.statefuleSet.enabled [default: false] Whether to deploy as a statefulset
## @param riva-nim.autoscaling.enabled [default: false] Whether to enable autoscaling
## @param riva-nim.autoscaling.minReplicas [default: 1] Minimum number of replicas
## @param riva-nim.autoscaling.maxReplicas [default: 10] Maximum number of replicas
## @param riva-nim.autoscaling.metrics [default: []] Metrics for autoscaling
## @param riva-nim.service.type [default: "ClusterIP"] The type of Kubernetes service to create
## @param riva-nim.service.name [default: "riva-nim"] The name of the service
## @param riva-nim.service.httpPort [default: 8000] The HTTP port for the service
## @param riva-nim.service.grpcPort [default: 8001] The gRPC port for the service
## @param riva-nim.service.metricsPort [default: 0] The metrics port for the service
## @param riva-nim.nim.grpcPort [default: 8001] The gRPC port for NIM communication
## @param riva-nim.nim.logLevel [default: "INFO"] The log level for NIM
## @param riva-nim.env [default: []] Environment variables for the container
riva-nim:
  deployed: false
  fullnameOverride: nv-ingest-riva-nim
  customCommand: []
  customArgs: []
  image:
    repository: nvcr.io/nim/nvidia/parakeet-1-1b-ctc-en-us
    tag: "1.3.0"
  podSecurityContext:
    runAsUser: 1000
    runAsGroup: 1000
    fsGroup: 1000
  replicaCount: 1
  serviceAccount:
    create: false
    name: ""
  statefuleSet:
    enabled: false
  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 10
    metrics: []
  service:
    type: "ClusterIP"
    name: nv-ingest-riva-nim
    httpPort: 9000
    grpcPort: 50051
    metricsPort: 0  # Generally unused and defaults to 0
  nim:
    grpcPort: 50051
    logLevel: "INFO"
  env:
    - name: NIM_HTTP_API_PORT
      value: "9000"
    - name: NIM_TAGS_SELECTOR
      value: "name=parakeet-1-1b-ctc-en-us,mode=ofl"


## @section Milvus Deployment parameters
## @descriptionStart
## NVIngest uses Milvus and Minio to store extracted images from a document
## This chart by default sets up a Milvus standalone instance in the namespace using the
## Helm chart at found https://artifacthub.io/packages/helm/milvus-helm/milvus
## @descriptionEnd
## @param milvusDeployed [default: true] Whether to deploy Milvus and Minio from this helm chart
milvusDeployed: true
## @section Milvus parameters
## @descriptionStart
## Milvus is used as the vector database for storing and searching embeddings.
## The chart uses the official Milvus Helm chart found at https://artifacthub.io/packages/helm/milvus-helm/milvus
## By default this deploys Milvus in standalone mode with GPU support.
## @descriptionEnd
## @param milvus.image.all.repository [default: milvusdb/milvus] The Milvus container image repository
## @param milvus.image.all.tag [default: v2.5.3-gpu] The Milvus container image tag
## @param milvus.cluster.enabled [default: false] Whether to deploy Milvus in cluster mode
## @param milvus.standalone.resources.limits.nvidia.com/gpu [default: 1] Number of GPUs to allocate to Milvus
## @param milvus.standalone.persistence.persistentVolumeClaim.size [default: 50Gi] Size of the PVC for Milvus data
## @param milvus.standalone.persistence.persistentVolumeClaim.storageClass Storage class to use for the PVC
## @param milvus.minio.mode [default: standalone] MinIO deployment mode
## @param milvus.minio.bucketName [default: nv-ingest] Name of the MinIO bucket to create
## @param milvus.minio.persistence.size [default: 50Gi] Size of the PVC for MinIO data
## @param milvus.minio.persistence.storageClass Storage class to use for MinIO PVC
milvus:
  image:
    all:
      repository: milvusdb/milvus
      tag: v2.5.17-gpu
  cluster:
    enabled: false
  etcd:
    image:
      repository: "milvusdb/etcd"
      tag: "3.5.22-r1"
    replicaCount: 1
    persistence:
      storageClass: null
  minio:
    enabled: true
    mode: standalone
    image:
      tag: "RELEASE.2025-09-07T16-13-09Z"
    bucketName: nv-ingest
    persistence:
      size: 50Gi
      storageClass: null
  pulsarv3:
    enabled: false
  pulsar:
    enabled: false
  standalone:
    resources:
      limits:
        nvidia.com/gpu: 1
    persistence:
      persistentVolumeClaim:
        size: 50Gi
        storageClass: null
    extraEnv:
      - name: LOG_LEVEL
        value: error



## @section Autoscaling parameters
## @descriptionStart
## Values used for creating a `Horizontal Pod Autoscaler`. If autoscaling is not enabled, the rest are ignored.
## NVIDIA recommends usage of the custom metrics API, commonly implemented with the prometheus-adapter.
## Standard metrics of CPU and memory are of limited use in scaling NIM.
## @descriptionEnd
## @param autoscaling.enabled Enables horizontal pod autoscaler.
## @param autoscaling.minReplicas Specify minimum replicas for autoscaling.
## @param autoscaling.maxReplicas Specify maximum replicas for autoscaling.
## @param autoscaling.metrics Array of metrics for autoscaling.
autoscaling:
  enabled: false
  maxReplicas: 100
  minReplicas: 1
  metrics: []


## @section Redis configurations
## @descriptionStart
## Include any redis configuration that you'd like with the deployed Redis
## Find values at https://github.com/bitnami/charts/tree/main/bitnami/redis
## @descriptionEnd
## @param redisDeployed [default: true] Whether to deploy Redis from this helm chart
redisDeployed: true
## @section Redis parameters
## @descriptionStart
## Configure Redis settings for the deployment. These values are passed directly to the Redis Helm chart.
## For a complete list of configuration options, see: https://github.com/bitnami/charts/tree/main/bitnami/redis
## @descriptionEnd
## @param redis Redis configuration options
## @param redis.auth.enabled [default: false] Enable Redis authentication
## @param redis.replica.replicaCount [default: 1] Number of Redis replicas
## @param redis.replica.persistence.size [default: "50Gi"] Size of persistent volume for Redis replicas
## @param redis.replica.resources.requests.memory [default: "6Gi"] Memory requests for Redis replicas
## @param redis.replica.resources.limits.memory [default: "12Gi"] Memory limits for Redis replicas
## @param redis.master.persistence.size [default: "50Gi"] Size of persistent volume for Redis master
## @param redis.master.resources.requests.memory [default: "6Gi"] Memory requests for Redis master
## @param redis.master.resources.limits.memory [default: "12Gi"] Memory limits for Redis master
## @param redis.master.configmap [default: "protected-mode no"] Redis master configuration

redis:
  image:
    tag: "8.2.1-debian-12-r0"
  auth:
    enabled: false
  replica:
    replicaCount: 1
    persistence:
      size: "50Gi"
    resources:
      requests:
        memory: "6Gi"
      limits:
        memory: "12Gi"
  master:
    persistence:
      size: "50Gi"
    resources:
      requests:
        memory: "6Gi"
      limits:
        memory: "12Gi"
    configmap: |-
      protected-mode no

## @section Prometheus configurations
## @descriptionStart
## Include any Prometheus configuration that you'd like with the deployed Prometheus
## For a complete list of configuration options see: https://github.com/prometheus-community/helm-charts/blob/main/charts/prometheus/values.yaml
## @descriptionEnd
## @param prometheus Prometheus configuration options
## @param prometheus.enabled [default: true] Whether to deploy Prometheus from this helm chart
## @param prometheus.server.enabled [default: true] Enable the Prometheus server
## @param prometheus.alertmanager.enabled [default: false] Enable the Prometheus alertmanager

prometheus:
  enabled: false
  server:
    enabled: false
  alertmanager:
    enabled: false

## @section Environment Variables
## @descriptionStart
## Define environment variables as key/value dictionary pairs
## @descriptionEnd
## @param envVars [default: sane {}] Adds arbitrary environment variables to the main container using key-value pairs, for example NAME: value
## @param envVars.ARROW_DEFAULT_MEMORY_POOL [default: "system"] Memory pool configuration for Apache Arrow
## @param envVars.INGEST_LOG_LEVEL [default: "DEFAULT"] Log level for the ingest service
## @param envVars.INGEST_EDGE_BUFFER_SIZE [default: "64"] Size of the edge buffer for ingestion
## @param envVars.INGEST_DYNAMIC_MEMORY_THRESHOLD [default: "0.80"] Dynamic memory threshold for ingestion
## @param envVars.MAX_INGEST_PROCESS_WORKERS [default: "16"] Maximum Ingestion worker processes
## @param envVars.MESSAGE_CLIENT_HOST [default: "nv-ingest-redis-master"] Override this value to specify a differing REST endpoint host
## @param envVars.MESSAGE_CLIENT_PORT [default: "6379"] Override this value to specify a differing REST endpoint port
## @param envVars.MESSAGE_CLIENT_TYPE [default: "redis"] Type of message client to use
## @param envVars.REDIS_INGEST_TASK_QUEUE [default: "ingest_task_queue"] Name of the Redis queue for ingest tasks
## @param envVars.NV_INGEST_DEFAULT_TIMEOUT_MS [default: "1234"] Override the Timeout of the NVIngest requests
## @param envVars.NV_INGEST_MAX_UTIL [default: "48"] Maximum number of CPU cores to utilize for processing. Defaults to number of available CPU cores if not set
## @param envVars.MINIO_INTERNAL_ADDRESS [default: "nv-ingest-minio:9000"] Override this to the cluster local DNS name of minio
## @param envVars.MINIO_PUBLIC_ADDRESS [default: "http://localhost:9000"] Override this to publicly routable minio address, default assumes port-forwarding
## @param envVars.MINIO_BUCKET [default: "nv-ingest"] Override this for specific minio bucket to upload extracted images to
## @param envVars.PADDLE_GRPC_ENDPOINT [default: "nv-ingest-paddle:8001"] gRPC endpoint for Paddle service
## @param envVars.PADDLE_HTTP_ENDPOINT [default: "http://nv-ingest-paddle:8000/v1/infer"] HTTP endpoint for Paddle service
## @param envVars.PADDLE_INFER_PROTOCOL [default: "grpc"] Whether to use the GRPC or HTTP endpoint for Paddle
## @param envVars.NEMORETRIEVER_PARSE_HTTP_ENDPOINT [default: "http://nim-vlm-text-extraction-nemoretriever-parse:8000/v1/chat/completions"] HTTP endpoint for NeMo Retriever Parse service
## @param envVars.NEMORETRIEVER_PARSE_INFER_PROTOCOL [default: "http"] Protocol for NeMo Retriever Parse service
## @param envVars.NEMORETRIEVER_PARSE_MODEL_NAME [default: "nvidia/nemoretriever-parse"] Model name for NeMo Retriever Parse
## @param envVars.YOLOX_GRPC_ENDPOINT [default: "nemoretriever-page-elements-v2:8001"] gRPC endpoint for YOLOX page elements
## @param envVars.YOLOX_HTTP_ENDPOINT [default: "http://nemoretriever-page-elements-v2:8000/v1/infer"] HTTP endpoint for YOLOX page elements
## @param envVars.YOLOX_INFER_PROTOCOL [default: "grpc"] Protocol for YOLOX page elements
## @param envVars.YOLOX_GRAPHIC_ELEMENTS_GRPC_ENDPOINT [default: "nemoretriever-graphic-elements-v1:8001"] gRPC endpoint for YOLOX graphic elements
## @param envVars.YOLOX_GRAPHIC_ELEMENTS_HTTP_ENDPOINT [default: "http://nemoretriever-graphic-elements-v1:8000/v1/infer"] HTTP endpoint for YOLOX graphic elements
## @param envVars.YOLOX_GRAPHIC_ELEMENTS_INFER_PROTOCOL [default: "grpc"] Protocol for YOLOX graphic elements
## @param envVars.YOLOX_TABLE_STRUCTURE_GRPC_ENDPOINT [default: "nemoretriever-table-structure-v1:8001"] gRPC endpoint for YOLOX table structure
## @param envVars.YOLOX_TABLE_STRUCTURE_HTTP_ENDPOINT [default: "http://nemoretriever-table-structure-v1:8000/v1/infer"] HTTP endpoint for YOLOX table structure
## @param envVars.YOLOX_TABLE_STRUCTURE_INFER_PROTOCOL [default: "grpc"] Protocol for YOLOX table structure
## @param envVars.EMBEDDING_NIM_ENDPOINT [default: "http://nv-ingest-embedqa:8000/v1"] Endpoint for embedding service
## @param envVars.EMBEDDING_NIM_MODEL_NAME [default: "nvidia/llama-3.2-nv-embedqa-1b-v2"] Model name for embedding service
## @param envVars.MILVUS_ENDPOINT [default: "http://nv-ingest-milvus:19530"] Endpoint for Milvus vector database
## @param envVars.VLM_CAPTION_ENDPOINT [default: "https://integrate.api.nvidia.com/v1/chat/completions"] Endpoint for VLM caption service
## @param envVars.VLM_CAPTION_MODEL_NAME [default: "nvidia/nemotron-nano-12b-v2-vl"] Model name for VLM caption service
## @param envVars.AUDIO_GRPC_ENDPOINT [default: "nv-ingest-riva-nim:50051"] gRPC endpoint for audio service
## @param envVars.AUDIO_INFER_PROTOCOL [default: "grpc"] Protocol for audio service
## @param envVars.COMPONENTS_TO_READY_CHECK [default: "ALL"] Components to check during readiness probe
## @param envVars.MODEL_PREDOWNLOAD_PATH [default: "/workspace/models/"] Path for pre-downloading models
envVars:
  ARROW_DEFAULT_MEMORY_POOL: "system"
  INGEST_LOG_LEVEL: "DEFAULT"
  INGEST_EDGE_BUFFER_SIZE: 64
  INGEST_DYNAMIC_MEMORY_THRESHOLD: 0.80
  INGEST_DISABLE_DYNAMIC_SCALING: false
  MAX_INGEST_PROCESS_WORKERS: 16
  NV_INGEST_MAX_UTIL: 48
  MESSAGE_CLIENT_HOST: "nv-ingest-redis-master"
  MESSAGE_CLIENT_PORT: "6379"
  MESSAGE_CLIENT_TYPE: "redis"
  REDIS_INGEST_TASK_QUEUE: "ingest_task_queue"
  NV_INGEST_DEFAULT_TIMEOUT_MS: "1234"

  MINIO_INTERNAL_ADDRESS: nv-ingest-minio:9000
  MINIO_PUBLIC_ADDRESS: http://localhost:9000
  MINIO_BUCKET: nv-ingest

  PADDLE_GRPC_ENDPOINT: nv-ingest-paddle:8001
  PADDLE_HTTP_ENDPOINT: http://nv-ingest-paddle:8000/v1/infer
  PADDLE_INFER_PROTOCOL: grpc
  NEMORETRIEVER_PARSE_HTTP_ENDPOINT: http://nim-vlm-text-extraction-nemoretriever-parse:8000/v1/chat/completions
  NEMORETRIEVER_PARSE_INFER_PROTOCOL: http
  NEMORETRIEVER_PARSE_MODEL_NAME: nvidia/nemoretriever-parse
  YOLOX_GRPC_ENDPOINT: nemoretriever-page-elements-v2:8001
  YOLOX_HTTP_ENDPOINT: http://nemoretriever-page-elements-v2:8000/v1/infer
  YOLOX_INFER_PROTOCOL: grpc
  YOLOX_GRAPHIC_ELEMENTS_GRPC_ENDPOINT: nemoretriever-graphic-elements-v1:8001
  YOLOX_GRAPHIC_ELEMENTS_HTTP_ENDPOINT: http://nemoretriever-graphic-elements-v1:8000/v1/infer
  YOLOX_GRAPHIC_ELEMENTS_INFER_PROTOCOL: grpc
  YOLOX_TABLE_STRUCTURE_GRPC_ENDPOINT: nemoretriever-table-structure-v1:8001
  YOLOX_TABLE_STRUCTURE_HTTP_ENDPOINT: http://nemoretriever-table-structure-v1:8000/v1/infer
  YOLOX_TABLE_STRUCTURE_INFER_PROTOCOL: grpc
  OCR_GRPC_ENDPOINT: nv-ingest-ocr:8001
  OCR_HTTP_ENDPOINT: http://nv-ingest-ocr:8000/v1/infer
  OCR_INFER_PROTOCOL: grpc
  OCR_MODEL_NAME: paddle

  EMBEDDING_NIM_ENDPOINT: "http://nv-ingest-embedqa:8000/v1"
  EMBEDDING_NIM_MODEL_NAME: "nvidia/llama-3.2-nv-embedqa-1b-v2"
  MILVUS_ENDPOINT: "http://nv-ingest-milvus:19530"

  # This environment variable is controled in the helm/deployment.yaml. There is a ternary operator that
  # sets this value to "nim-vlm-image-captioning:8001" to use the locally deployed NIM if the nim-vlm-image-captioning.deployed
  # condition is true, otherwise it is set to "https://integrate.api.nvidia.com/v1/chat/completions" to use the NGC hosted NIM
  # VLM_CAPTION_ENDPOINT: "DO_NOT_USE_HERE_FOR_REFERENCE_ONLY"
  VLM_CAPTION_MODEL_NAME: "nvidia/nemotron-nano-12b-v2-vl"

  AUDIO_GRPC_ENDPOINT: "nv-ingest-riva-nim:50051"
  AUDIO_INFER_PROTOCOL: "grpc"

  COMPONENTS_TO_READY_CHECK: "ALL"
  MODEL_PREDOWNLOAD_PATH: "/workspace/models/"

## @section Open Telemetry
## @descriptionStart
## Define environment variables as key/value dictionary pairs for configuring OTEL Deployments
## A sane set of parameters is set for the deployed version of OpenTelemetry with this Helm Chart.
## Override any values to the Open Telemetry helm chart by overriding the `open-telemetry` value.
## @descriptionEnd

## @param otelEnabled [default: true] Whether to enable OTEL collection
otelEnabled: true
## @param otelDeployed [default: true] Whether to deploy OTEL from this helm chart
otelDeployed: true

## @skip opentelemetry-collector
## @extra opentelemetry-collector [default: sane {}] Configures the opentelemetry helm chart - see https://github.com/open-telemetry/opentelemetry-helm-charts/blob/main/charts/opentelemetry-collector/values.yaml
opentelemetry-collector:
  image:
    repository: "otel/opentelemetry-collector-contrib"
    tag: "0.133.0"
  mode: deployment
  config:
    receivers:
      otlp:
        protocols:
          grpc:
            endpoint: "${env:MY_POD_IP}:4317"
          http:
            endpoint: "${env:MY_POD_IP}:4318"
            cors:
              allowed_origins: ["*"]

    exporters:
      zipkin:
        endpoint: "http://nv-ingest-zipkin:9411/api/v2/spans"
      debug:
        verbosity: detailed

    extensions:
      health_check: {}
      zpages:
        endpoint: 0.0.0.0:55679

    processors:
      batch: {}
      tail_sampling:
        policies:
          - name: drop_noisy_traces_url
            type: string_attribute
            string_attribute:
              key: http.target
              values: ["/health"]
              enabled_regex_matching: true
              invert_match: true
      transform:
        trace_statements:
          - context: span
            statements:
              - set(status.code, 1) where attributes["http.path"] == "/health"

              # replace aspects of the span after anonymization
              - replace_match(attributes["http.route"], "/v1", attributes["http.target"]) where attributes["http.target"] != nil
              - replace_pattern(name, "/v1", attributes["http.route"]) where attributes["http.route"] != nil
              - set(name, Concat([name, attributes["http.url"]], " ")) where name == "POST"

    service:
      extensions: [zpages, health_check]
      telemetry:
        logs:
          level: "debug"
      pipelines:
        traces:
          receivers: [otlp]
          processors: [batch, tail_sampling, transform]
          exporters: [debug, zipkin]
        metrics:
          receivers: [otlp]
          processors: [batch]
          exporters: [debug]
        logs:
          receivers: [otlp]
          processors: [batch]
          exporters: [debug]

## @param otelEnvVars [default: sane {}] Adds arbitrary environment variables for configuring OTEL using key-value pairs, for example NAME: value
## @extra otelEnvVars.OTEL_EXPORTER_OTLP_ENDPOINT Default deployment target for GRPC otel - Default "http://{{ .Release.Name }}-opentelemetry-collector:4317"
## @param otelEnvVars.OTEL_SERVICE_NAME [default: "nemo-retrieval-service" ]
## @param otelEnvVars.OTEL_TRACES_EXPORTER [default: "otlp" ]
## @param otelEnvVars.OTEL_METRICS_EXPORTER [default: "otlp" ]
## @param otelEnvVars.OTEL_LOGS_EXPORTER [default: "none" ]
## @param otelEnvVars.OTEL_PROPAGATORS [default: "tracecontext baggage" ]
## @param otelEnvVars.OTEL_RESOURCE_ATTRIBUTES [default: "deployment.environment=$(NAMESPACE)" ]
## @param otelEnvVars.OTEL_PYTHON_EXCLUDED_URLS [default: "health" ]
otelEnvVars:
  # OpenTelemetry
  OTEL_SERVICE_NAME: "nemo-retrieval-service"
  OTEL_TRACES_EXPORTER: otlp
  OTEL_METRICS_EXPORTER: otlp
  OTEL_LOGS_EXPORTER: none
  OTEL_PROPAGATORS: "tracecontext,baggage"
  OTEL_RESOURCE_ATTRIBUTES: "deployment.environment=$(NAMESPACE)"
  OTEL_PYTHON_EXCLUDED_URLS: "health"


## @param zipkinDeployed [default: true] Whether to deploy Zipkin with OpenTelemetry from this helm chart
zipkinDeployed: true

zipkin:
  image:
    repository: "openzipkin/zipkin"
    tag: "3.5.0"
  zipkin:
    extraEnv:
      JAVA_OPTS: "-Xms2g -Xmx4g -XX:+ExitOnOutOfMemoryError"
  resources:
    limits:
      cpu: 500m
      memory: 4.5Gi
    requests:
      cpu: 100m
      memory: 2.5Gi

## @section Ingress parameters
## @param ingress.enabled Enables ingress.
## @param ingress.className Specify class name for Ingress.
## @param ingress.annotations Specify additional annotations for ingress.
## @param ingress.hosts[].host Specify name of host.
## @param ingress.hosts[].paths[].path Specify ingress path.
## @param ingress.hosts[].paths[].pathType Specify path type.
## @param ingress.tls Specify list of pairs of TLS `secretName` and hosts.
ingress:
  enabled: false
  className: ""
  annotations: {}
  hosts:
  - host: chart-example.local
    paths:
    - path: /
      pathType: ImplementationSpecific
  tls: []

## @section Probe parameters
## @param livenessProbe.enabled Enables `livenessProbe`
## @param livenessProbe.httpGet.path `LivenessProbe` endpoint path
## @param livenessProbe.httpGet.port `LivenessProbe` endpoint port
## @param livenessProbe.initialDelaySeconds Initial delay seconds for `livenessProbe`
## @param livenessProbe.timeoutSeconds Timeout seconds for `livenessProbe`
## @param livenessProbe.periodSeconds Period seconds for `livenessProbe`
## @param livenessProbe.successThreshold Success threshold for `livenessProbe`
## @param livenessProbe.failureThreshold Failure threshold for `livenessProbe`
livenessProbe:
  enabled: false
  httpGet:
    path: /v1/health/live
    port: http
  initialDelaySeconds: 120
  periodSeconds: 10
  timeoutSeconds: 20
  failureThreshold: 20
  successThreshold: 1

## @section Probe parameters
## @param readinessProbe.enabled Enables `readinessProbe`
## @param readinessProbe.httpGet.path `ReadinessProbe` endpoint path
## @param readinessProbe.httpGet.port `ReadinessProbe` endpoint port
## @param readinessProbe.initialDelaySeconds Initial delay seconds for `readinessProbe`
## @param readinessProbe.timeoutSeconds Timeout seconds for `readinessProbe`
## @param readinessProbe.periodSeconds Period seconds for `readinessProbe`
## @param readinessProbe.successThreshold Success threshold for `readinessProbe`
## @param readinessProbe.failureThreshold Failure threshold for `readinessProbe`
readinessProbe:
  enabled: false
  httpGet:
    path: /v1/health/ready
    port: http
  initialDelaySeconds: 120
  periodSeconds: 30
  timeoutSeconds: 10
  failureThreshold: 220
  successThreshold: 1

## @section Service parameters
## @param service.type Specifies the service type for the deployment.
## @param service.name Overrides the default service name
## @param service.port Specifies the HTTP Port for the service.
## @param service.nodePort Specifies an optional HTTP Node Port for the service.
## @param service.annotations [object] Specify additional annotations to be added to service.
## @param service.labels [object] Specifies additional labels to be added to service.
service:
  type: ClusterIP
  port: 7670
  annotations: {}
  labels: {}
  name: ""  # override the default service name
  nodePort: null

## @section Service Account
## @param serviceAccount.create Specifies whether a service account should be created.
## @param serviceAccount.annotations [object] Sets annotations to be added to the service account.
## @param serviceAccount.name Specifies the name of the service account to use.
## @param serviceAccount.automount [default: true] Specifies whether to automatically mount the service account token.
serviceAccount:
  annotations: {}
  automount: true
  create: true
  name: ""

## @section Secret Creation
## @descriptionStart
## Manage the creation of secrets used by the helm chart
## @descriptionEnd

# ngcApi:
# # If set to false, the chart expects a secret with the name
#   create: false
#   password: ""

## @param ngcApiSecret.create Specifies whether to create the ngc api secret
## @param ngcApiSecret.password The password to use for the NGC Secret
ngcApiSecret:
  # If set to false, the chart expects a secret with name ngc-api to exist in the namespace
  # credentials are needed.
  create: false
  password: ""

## @param ngcImagePullSecret.create Specifies whether to create the NVCR Image Pull secret
## @param ngcImagePullSecret.password The password to use for the NVCR Image Pull Secret
## @param ngcImagePullSecret.registry [default: "nvcr.io"] The registry URL
## @param ngcImagePullSecret.name [default: "ngcImagePullSecret"] The name of the secret
## @param ngcImagePullSecret.username [default: "$oauthtoken"] The username for the registry
ngcImagePullSecret:
  create: false
  # Leave blank, if no imagePullSecret is needed.
  registry: "nvcr.io"
  name: "ngcImagePullSecret"
  # If set to false, the chart expects either a imagePullSecret
  # with the name configured above to be present on the cluster or that no
  # credentials are needed.

  username: '$oauthtoken'
  password: ""

## @section Container Configuration
## @param nemo.userID [default: "1000"] User ID for the NEMO container
## @param nemo.groupID [default: "1000"] Group ID for the NEMO container
## @param containerArgs [array] Additional arguments to pass to the container
nemo:
  userID: "1000"
  groupID: "1000"

## @param containerArgs [array] Additional arguments to pass to the container
containerArgs: []
