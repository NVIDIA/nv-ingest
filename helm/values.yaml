## @section Deployment parameters
## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity
## @param affinity [object] [default: {}] Affinity settings for deployment.
affinity: {}

## @param nodeSelector [object] Sets node selectors for the NIM -- for example `nvidia.com/gpu.present: "true"`
nodeSelector: {}

## @param extraEnvVarsCM [string] [default: ""] A Config map holding Environment variables to include in the NVIngest container
extraEnvVarsCM: ""

## @param extraEnvVarsSecret [string] [default: ""] A K8S Secret to map to Environment variables to include in the NVIngest container
extraEnvVarsSecret: ""

## @param fullnameOverride [string] [default: ""] A name to force the fullname of the NVIngest container to have, defaults to the Helm Release Name
fullnameOverride: ""

## @param nameOverride [string] [default: ""] A name to base the objects created by this helm chart
nameOverride: ""

## @section Image Configuration
## @param image.repository [string] NIM Image Repository
## @param image.tag [string] Image tag or version
## @param image.pullPolicy [string] Image pull policy
image:
  pullPolicy: IfNotPresent
  repository: "nvcr.io/nvidia/nemo-microservices/nv-ingest"
  tag: "25.3.0"

## @section Pod Configuration
## @param podAnnotations [object] Sets additional annotations on the main deployment pods
podAnnotations:
  traffic.sidecar.istio.io/excludeOutboundPorts: '8007'

## @param podLabels [object] Specify extra labels to be add to on deployed pods.
podLabels: {}

## @param podSecurityContext.fsGroup Specify file system owner group id.
podSecurityContext:
  fsGroup: 1000

## @param extraVolumes [object] Adds arbitrary additional volumes to the deployment set definition
extraVolumes: {}

## @param extraVolumeMounts [object] Specify volume mounts to the main container from `extraVolumes`
extraVolumeMounts: {}

## @section Image Pull Secrets
## @param imagePullSecrets[].name List of secret names needed for the main container and any init containers
imagePullSecrets:
  - name: ngc-api
  - name: ngc-secret

## @param containerSecurityContext [object] Sets privilege and access control settings for container (Only affects the main container, not pod-level)
containerSecurityContext: {}

## @param tolerations [array] Specify tolerations for pod assignment. Allows the scheduler to schedule pods with matching taints.
tolerations: []

## @param replicaCount [default: 1] The number of replicas for NVIngest when autoscaling is disabled
replicaCount: 1

## @section Resource Configuration
## @param resources.limits.memory [default: 200Gi] Specify limit for memory
## @param resources.limits.cpu [default: "48000m"] Specify limit for CPU
## @param resources.requests.memory [default: 24Gi] Specify request for memory
## @param resources.requests.cpu [default: "24000m"] Specify request for CPU
resources:
  limits:
    memory: 200Gi
    cpu: "48000m"
  requests:
    memory: 24Gi
    cpu: "24000m"

## @param tmpDirSize [default: 50Gi] Specify the amount of space to reserve for temporary storage
tmpDirSize: 50Gi

## @section NIM Configuration
## @descriptionStart
## Define additional values to the dependent NIM helm charts by updating the "nemoretriever-page-elements-v2", "nemoretriever-graphic-elements-v1",
## "nemoretriever-table-structure-v1", "nemoretriever-parse", and "paddle" values. A sane set of configurations are already included in this value
## file and only the "image.repository" and "image.tag" fields are explicitly called out here.
## @descriptionEnd

## @section NIM Components
## @param nemoretriever-page-elements-v2.deployed [default: true] true if the Yolox NIM should be deployed and false otherwise
## @param nemoretriever-page-elements-v2.image.repository The repository to override the location of the YOLOX
## @param nemoretriever-page-elements-v2.image.tag The tag override for YOLOX
## @param nemoretriever-page-elements-v2.image.pullPolicy [default: IfNotPresent] The pull policy for the YOLOX image
## @param nemoretriever-page-elements-v2.podSecurityContext.runAsUser [default: 1000] The user ID to run the YOLOX container as
## @param nemoretriever-page-elements-v2.podSecurityContext.runAsGroup [default: 1000] The group ID to run the YOLOX container as
## @param nemoretriever-page-elements-v2.podSecurityContext.fsGroup [default: 1000] The filesystem group ID for YOLOX volumes
## @param nemoretriever-page-elements-v2.replicaCount [default: 1] Number of YOLOX replicas when autoscaling is disabled
## @param nemoretriever-page-elements-v2.serviceAccount.create [default: false] Whether to create a service account for YOLOX
## @param nemoretriever-page-elements-v2.serviceAccount.name [default: ""] The name of the service account to use
## @param nemoretriever-page-elements-v2.statefuleSet.enabled [default: false] Whether to deploy YOLOX as a statefulset
## @param nemoretriever-page-elements-v2.autoscaling.enabled [default: false] Enable autoscaling for YOLOX
## @param nemoretriever-page-elements-v2.autoscaling.minReplicas [default: 1] Minimum number of YOLOX replicas
## @param nemoretriever-page-elements-v2.autoscaling.maxReplicas [default: 10] Maximum number of YOLOX replicas
## @param nemoretriever-page-elements-v2.autoscaling.metrics [default: []] Metrics to use for autoscaling
## @param nemoretriever-page-elements-v2.service.type [default: "ClusterIP"] The type of Kubernetes service to create
## @param nemoretriever-page-elements-v2.service.name [default: "nemoretriever-page-elements-v2"] The name of the service
## @param nemoretriever-page-elements-v2.service.httpPort [default: 8000] The HTTP port for the service
## @param nemoretriever-page-elements-v2.service.grpcPort [default: 8001] The gRPC port for the service
## @param nemoretriever-page-elements-v2.service.metricsPort [default: 0] The metrics port for the service
## @param nemoretriever-page-elements-v2.nim.grpcPort [default: 8001] The gRPC port for NIM communication
## @param nemoretriever-page-elements-v2.nim.logLevel [default: "INFO"] The log level for NIM
## @param nemoretriever-page-elements-v2.env Environment variables for the YOLOX container
nemoretriever-page-elements-v2:
  deployed: true
  image:
    repository: nvcr.io/nim/nvidia/nemoretriever-page-elements-v2
    tag: "1.2.0"
    pullPolicy: IfNotPresent
  podSecurityContext:
    runAsUser: 1000
    runAsGroup: 1000
    fsGroup: 1000
  replicaCount: 1
  serviceAccount:
    create: false
    name: ""
  statefuleSet:
    enabled: false
  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 10
    metrics: []
  service:
    type: "ClusterIP"
    name: nemoretriever-page-elements-v2
    httpPort: 8000
    grpcPort: 8001
    metricsPort: 0  # Generally unused and defaults to 0
  nim:
    grpcPort: 8001
    logLevel: "INFO"
  env:
    - name: NIM_HTTP_API_PORT
      value: "8000"
    - name: NIM_TRITON_MODEL_BATCH_SIZE
      value: "1"

## @skip nemoretriever-graphic-elements-v1
## @param nemoretriever-graphic-elements-v1.deployed [default: true] true if the nemoretriever-graphic-elements NIM should be deployed and false otherwise
## @param nemoretriever-graphic-elements-v1.image.repository The repository to override the location of the nemoretriever-graphic-elements
## @param nemoretriever-graphic-elements-v1.image.tag The tag override for nemoretriever-graphic-elements
## @param nemoretriever-graphic-elements-v1.customCommand [default: []] Custom command to override the default container command
## @param nemoretriever-graphic-elements-v1.customArgs [default: []] Custom args to override the default container args
## @param nemoretriever-graphic-elements-v1.podSecurityContext [default: {}] Pod security context
## @param nemoretriever-graphic-elements-v1.replicaCount [default: 1] Number of replicas to deploy
## @param nemoretriever-graphic-elements-v1.serviceAccount.create [default: false] Whether to create a service account
## @param nemoretriever-graphic-elements-v1.serviceAccount.name [default: ""] The name of the service account to use
## @param nemoretriever-graphic-elements-v1.statefuleSet.enabled [default: false] Whether to deploy as a statefulset
## @param nemoretriever-graphic-elements-v1.autoscaling.enabled [default: false] Enable autoscaling
## @param nemoretriever-graphic-elements-v1.autoscaling.minReplicas [default: 1] Minimum number of replicas
## @param nemoretriever-graphic-elements-v1.autoscaling.maxReplicas [default: 10] Maximum number of replicas
## @param nemoretriever-graphic-elements-v1.autoscaling.metrics [default: []] Metrics to use for autoscaling
## @param nemoretriever-graphic-elements-v1.service.type [default: "ClusterIP"] The type of Kubernetes service to create
## @param nemoretriever-graphic-elements-v1.service.name [default: "nemoretriever-graphic-elements-v1"] The name of the service
## @param nemoretriever-graphic-elements-v1.service.httpPort [default: 8000] The HTTP port for the service
## @param nemoretriever-graphic-elements-v1.service.grpcPort [default: 8001] The gRPC port for the service
## @param nemoretriever-graphic-elements-v1.service.metricsPort [default: 0] The metrics port for the service
## @param nemoretriever-graphic-elements-v1.nim.grpcPort [default: 8001] The gRPC port for NIM communication
## @param nemoretriever-graphic-elements-v1.nim.logLevel [default: "INFO"] The log level for NIM
## @param nemoretriever-graphic-elements-v1.env Environment variables for the container
nemoretriever-graphic-elements-v1:
  deployed: true
  customCommand: []
  customArgs: []
  image:
    repository: nvcr.io/nim/nvidia/nemoretriever-graphic-elements-v1
    tag: "1.2.0"
  podSecurityContext:
    runAsUser: 1000
    runAsGroup: 1000
    fsGroup: 1000
  replicaCount: 1
  serviceAccount:
    create: false
    name: ""
  statefuleSet:
    enabled: false
  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 10
    metrics: []
  service:
    type: "ClusterIP"
    name: nemoretriever-graphic-elements-v1
    httpPort: 8000
    grpcPort: 8001
    metricsPort: 0  # Generally unused and defaults to 0
  nim:
    grpcPort: 8001
    logLevel: "INFO"
  env:
    - name: NIM_HTTP_API_PORT
      value: "8000"
    - name: NIM_TRITON_MODEL_BATCH_SIZE
      value: "1"

## @skip nemoretriever-table-structure-v1
## @param nemoretriever-table-structure-v1.deployed [default: true] true if the nemoretriever-table-structure NIM should be deployed and false otherwise
## @param nemoretriever-table-structure-v1.image.repository The repository to override the location of the nemoretriever-table-structure
## @param nemoretriever-table-structure-v1.image.tag The tag override for nemoretriever-table-structure
## @param nemoretriever-table-structure-v1.customCommand [default: []] Custom command to override the default container command
## @param nemoretriever-table-structure-v1.customArgs [default: []] Custom args to override the default container args
## @param nemoretriever-table-structure-v1.podSecurityContext [default: {runAsUser: 1000, runAsGroup: 1000, fsGroup: 1000}] Security context for the pod
## @param nemoretriever-table-structure-v1.replicaCount [default: 1] Number of replicas to deploy
## @param nemoretriever-table-structure-v1.serviceAccount.create [default: false] Whether to create a service account
## @param nemoretriever-table-structure-v1.serviceAccount.name [default: ""] Name of the service account
## @param nemoretriever-table-structure-v1.statefuleSet.enabled [default: false] Whether to deploy as a statefulset
## @param nemoretriever-table-structure-v1.autoscaling.enabled [default: false] Enable autoscaling
## @param nemoretriever-table-structure-v1.autoscaling.minReplicas [default: 1] Minimum number of replicas
## @param nemoretriever-table-structure-v1.autoscaling.maxReplicas [default: 10] Maximum number of replicas
## @param nemoretriever-table-structure-v1.autoscaling.metrics [default: []] Metrics to use for autoscaling
## @param nemoretriever-table-structure-v1.service.type [default: "ClusterIP"] The type of Kubernetes service to create
## @param nemoretriever-table-structure-v1.service.name [default: "nemoretriever-table-structure-v1"] The name of the service
## @param nemoretriever-table-structure-v1.service.httpPort [default: 8000] The HTTP port for the service
## @param nemoretriever-table-structure-v1.service.grpcPort [default: 8001] The gRPC port for the service
## @param nemoretriever-table-structure-v1.service.metricsPort [default: 0] The metrics port for the service
## @param nemoretriever-table-structure-v1.nim.grpcPort [default: 8001] The gRPC port for NIM communication
## @param nemoretriever-table-structure-v1.nim.logLevel [default: "INFO"] The log level for NIM
## @param nemoretriever-table-structure-v1.env Environment variables for the container
nemoretriever-table-structure-v1:
  deployed: true
  # fullnameOverride: nemoretriever-table-structure-v1
  customCommand: []
  customArgs: []
  image:
    repository: nvcr.io/nim/nvidia/nemoretriever-table-structure-v1
    tag: "1.2.0"
  podSecurityContext:
    runAsUser: 1000
    runAsGroup: 1000
    fsGroup: 1000
  replicaCount: 1
  serviceAccount:
    create: false
    name: ""
  statefuleSet:
    enabled: false
  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 10
    metrics: []
  service:
    type: "ClusterIP"
    name: nemoretriever-table-structure-v1
    httpPort: 8000
    grpcPort: 8001
    metricsPort: 0  # Generally unused and defaults to 0
  nim:
    grpcPort: 8001
    logLevel: "INFO"
  env:
    - name: NIM_HTTP_API_PORT
      value: "8000"
    - name: NIM_TRITON_MODEL_BATCH_SIZE
      value: "1"

## @section NIM VLM Image Captioning Configuration
## @descriptionStart
## Configuration for the VLM (Vision Language Model) image captioning service.
## This service provides AI-powered image captioning capabilities.
## @descriptionEnd
## @param nim-vlm-image-captioning.deployed [default: false] Whether to deploy the VLM image captioning service
## @param nim-vlm-image-captioning.fullnameOverride [default: "nim-vlm-image-captioning"] Override the full name of the deployment
## @param nim-vlm-image-captioning.customCommand [default: []] Custom command to run in the container
## @param nim-vlm-image-captioning.customArgs [default: []] Custom arguments to pass to the container command
## @param nim-vlm-image-captioning.image.repository [default: "nvcr.io/nim/meta/llama-3.2-11b-vision-instruct"] The container image repository
## @param nim-vlm-image-captioning.image.tag [default: "latest"] The container image tag
## @param nim-vlm-image-captioning.podSecurityContext [default: {}] Pod security context configuration
## @param nim-vlm-image-captioning.replicaCount [default: 1] Number of replicas to deploy
## @param nim-vlm-image-captioning.service.type [default: "ClusterIP"] The type of Kubernetes service to create
## @param nim-vlm-image-captioning.service.name [default: "nim-vlm-image-captioning"] The name of the service
## @param nim-vlm-image-captioning.service.httpPort [default: 8000] The HTTP port for the service
## @param nim-vlm-image-captioning.service.grpcPort [default: 8001] The gRPC port for the service
## @param nim-vlm-image-captioning.service.metricsPort [default: 0] The metrics port for the service
## @param nim-vlm-image-captioning.nim.grpcPort [default: 8001] The gRPC port for NIM communication
## @param nim-vlm-image-captioning.nim.logLevel [default: "INFO"] The log level for NIM
## @param nim-vlm-image-captioning.env [default: []] Environment variables for the container
nim-vlm-image-captioning:
  deployed: false
  fullnameOverride: nim-vlm-image-captioning
  customCommand: []
  customArgs: []
  image:
    repository: nvcr.io/nim/meta/llama-3.2-11b-vision-instruct
    tag: "latest"
  podSecurityContext:
    runAsUser: 1000
    runAsGroup: 1000
    fsGroup: 1000
  replicaCount: 1
  service:
    type: "ClusterIP"
    name: nim-vlm-image-captioning
    httpPort: 8000
    grpcPort: 8001
    metricsPort: 0  # Generally unused and defaults to 0
  nim:
    grpcPort: 8001
    logLevel: "INFO"
  env:
    - name: NIM_HTTP_API_PORT
      value: "8000"
    - name: NIM_TRITON_MODEL_BATCH_SIZE
      value: "1"

## @section NIM VLM Text Extraction Configuration
## @descriptionStart
## Configuration for the VLM (Vision Language Model) text extraction service.
## This service provides AI-powered text extraction capabilities from images using NeMo Retriever.
## @descriptionEnd
## @param nim-vlm-text-extraction.deployed [default: false] Whether to deploy the VLM text extraction service
## @param nim-vlm-text-extraction.fullnameOverride [default: "nim-vlm-text-extraction-nemoretriever-parse"] Override the full name of the deployment
## @param nim-vlm-text-extraction.customCommand [default: []] Custom command to run in the container
## @param nim-vlm-text-extraction.customArgs [default: []] Custom arguments to pass to the container command
## @param nim-vlm-text-extraction.image.repository [default: "nvcr.io/nvidia/nemo-microservices/nemoretriever-parse"] The container image repository
## @param nim-vlm-text-extraction.image.tag [default: "1.2.0ea"] The container image tag
## @param nim-vlm-text-extraction.podSecurityContext [default: {}] Pod security context configuration
## @param nim-vlm-text-extraction.replicaCount [default: 1] Number of replicas to deploy
## @param nim-vlm-text-extraction.service.type [default: "ClusterIP"] The type of Kubernetes service to create
## @param nim-vlm-text-extraction.service.name [default: "nim-vlm-text-extraction-nemoretriever-parse"] The name of the service
## @param nim-vlm-text-extraction.service.httpPort [default: 8000] The HTTP port for the service
## @param nim-vlm-text-extraction.service.grpcPort [default: 8001] The gRPC port for the service
## @param nim-vlm-text-extraction.service.metricsPort [default: 0] The metrics port for the service
## @param nim-vlm-text-extraction.nim.grpcPort [default: 8001] The gRPC port for NIM communication
## @param nim-vlm-text-extraction.nim.logLevel [default: "INFO"] The log level for NIM
## @param nim-vlm-text-extraction.env [default: []] Environment variables for the container
nim-vlm-text-extraction:
  deployed: false
  fullnameOverride: nim-vlm-text-extraction-nemoretriever-parse
  customCommand: []
  customArgs: []
  image:
    repository: nvcr.io/nvidia/nemo-microservices/nemoretriever-parse
    tag: "1.2.0ea"
  podSecurityContext:
    runAsUser: 1000
    runAsGroup: 1000
    fsGroup: 1000
  replicaCount: 1
  service:
    type: "ClusterIP"
    name: nim-vlm-text-extraction-nemoretriever-parse
    httpPort: 8000
    grpcPort: 8001
    metricsPort: 0  # Generally unused and defaults to 0
  nim:
    grpcPort: 8001
    logLevel: "INFO"
  env:
    - name: NIM_HTTP_API_PORT
      value: "8000"
    - name: NIM_TRITON_MODEL_BATCH_SIZE
      value: "1"

## @section PaddleOCR NIM Configuration
## @descriptionStart
## Configuration for the PaddleOCR NIM service.
## This service provides OCR (Optical Character Recognition) capabilities using PaddleOCR.
## @descriptionEnd
## @param paddleocr-nim.deployed [default: true] Whether to deploy the PaddleOCR NIM service
## @param paddleocr-nim.fullnameOverride [default: "nv-ingest-paddle"] Override the full name of the deployment
## @param paddleocr-nim.customCommand [default: []] Custom command to run in the container
## @param paddleocr-nim.customArgs [default: []] Custom arguments to pass to the container command
## @param paddleocr-nim.image.repository [default: "nvcr.io/nim/baidu/paddleocr"] The container image repository
## @param paddleocr-nim.image.tag [default: "1.2.0"] The container image tag
## @param paddleocr-nim.podSecurityContext [default: {}] Pod security context configuration
## @param paddleocr-nim.replicaCount [default: 1] Number of replicas to deploy
## @param paddleocr-nim.serviceAccount.create [default: false] Whether to create a service account
## @param paddleocr-nim.serviceAccount.name [default: ""] The name of the service account
## @param paddleocr-nim.statefuleSet.enabled [default: false] Whether to deploy as a StatefulSet
## @param paddleocr-nim.service.type [default: "ClusterIP"] The type of Kubernetes service to create
## @param paddleocr-nim.service.name [default: "nv-ingest-paddle"] The name of the service
## @param paddleocr-nim.service.httpPort [default: 8000] The HTTP port for the service
## @param paddleocr-nim.service.grpcPort [default: 8001] The gRPC port for the service
## @param paddleocr-nim.service.metricsPort [default: 0] The metrics port for the service
## @param paddleocr-nim.nim.grpcPort [default: 8001] The gRPC port for NIM communication
## @param paddleocr-nim.nim.logLevel [default: "INFO"] The log level for NIM
## @param paddleocr-nim.env [default: []] Environment variables for the container
paddleocr-nim:
  paddleocr-nim.deployed: true
  fullnameOverride: nv-ingest-paddle
  customCommand: []
  customArgs: []
  image:
    repository: nvcr.io/nim/baidu/paddleocr
    tag: "1.2.0"
  podSecurityContext:
    runAsUser: 1000
    runAsGroup: 1000
    fsGroup: 1000
  replicaCount: 1
  serviceAccount:
    create: false
    name: ""
  statefuleSet:
    enabled: false
  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 10
    metrics: []
  service:
    type: "ClusterIP"
    name: nv-ingest-paddle
    httpPort: 8000
    grpcPort: 8001
    metricsPort: 0  # Generally unused and defaults to 0
  nim:
    grpcPort: 8001
    logLevel: "INFO"
  env:
    - name: NIM_HTTP_API_PORT
      value: "8000"
    - name: NIM_TRITON_MODEL_BATCH_SIZE
      value: "1"


## @skip text-embedding-nim
## @param text-embedding-nim.deployed [default: false] true if the text-embedding-nim should be deployed and false otherwise
## @param text-embedding-nim.image.repository The repository to override the location of the text-embedding-nim
## @param text-embedding-nim.image.tag The tag override for text-embedding-nim
## @param text-embedding-nim.fullnameOverride [default: "nv-ingest-embedqa"] The name override for the deployment
## @param text-embedding-nim.customCommand [default: []] Custom command to override container entrypoint
## @param text-embedding-nim.customArgs [default: []] Custom arguments to pass to the container
## @param text-embedding-nim.podSecurityContext Security context for the pod
## @param text-embedding-nim.replicaCount [default: 1] Number of replicas to deploy
## @param text-embedding-nim.serviceAccount.create [default: false] Whether to create a service account
## @param text-embedding-nim.serviceAccount.name [default: ""] The name of the service account
## @param text-embedding-nim.statefuleSet.enabled [default: false] Whether to deploy as a StatefulSet
## @param text-embedding-nim.autoscaling.enabled [default: false] Enable autoscaling
## @param text-embedding-nim.autoscaling.minReplicas [default: 1] Minimum number of replicas
## @param text-embedding-nim.autoscaling.maxReplicas [default: 10] Maximum number of replicas
## @param text-embedding-nim.autoscaling.metrics [default: []] Metrics for autoscaling
## @param text-embedding-nim.service.type [default: "ClusterIP"] The type of Kubernetes service to create
## @param text-embedding-nim.service.name [default: "nv-ingest-embedqa"] The name of the service
## @param text-embedding-nim.service.httpPort [default: 8000] The HTTP port for the service
## @param text-embedding-nim.service.grpcPort [default: 8001] The gRPC port for the service
## @param text-embedding-nim.service.metricsPort [default: 0] The metrics port for the service
## @param text-embedding-nim.nim.grpcPort [default: 8001] The gRPC port for NIM communication
## @param text-embedding-nim.nim.logLevel [default: "INFO"] The log level for NIM
## @param text-embedding-nim.env [default: []] Environment variables for the container
text-embedding-nim:
  deployed: false
  fullnameOverride: nv-ingest-embedqa # Share name with nvidia-nim-llama-32-nv-embedqa-1b-v2 to ease configuration
  customCommand: []
  customArgs: []
  image:
    repository: nvcr.io/nim/nvidia/nv-embedqa-e5-v5
    tag: "1.5.0"
  podSecurityContext:
    runAsUser: 1000
    runAsGroup: 1000
    fsGroup: 1000
  replicaCount: 1
  serviceAccount:
    create: false
    name: ""
  statefuleSet:
    enabled: false
  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 10
    metrics: []
  service:
    type: "ClusterIP"
    name: nv-ingest-embedqa
    httpPort: 8000
    grpcPort: 8001
    metricsPort: 0  # Generally unused and defaults to 0
  nim:
    grpcPort: 8001
    logLevel: "INFO"
  env:
    - name: NIM_HTTP_API_PORT
      value: "8000"
    - name: NIM_TRITON_MODEL_BATCH_SIZE
      value: "1"


## @skip nvidia-nim-llama-32-nv-embedqa-1b-v2
## @param nvidia-nim-llama-32-nv-embedqa-1b-v2.deployed [default: true] true if nvidia-nim-llama-32-nv-embedqa-1b-v2 should be deployed and false otherwise
## @param nvidia-nim-llama-32-nv-embedqa-1b-v2.fullnameOverride [default: "nv-ingest-embedqa"] Override for the full name of the deployment
## @param nvidia-nim-llama-32-nv-embedqa-1b-v2.customCommand [default: []] Custom command to run in the container
## @param nvidia-nim-llama-32-nv-embedqa-1b-v2.customArgs [default: []] Custom arguments to pass to the container
## @param nvidia-nim-llama-32-nv-embedqa-1b-v2.image.repository The repository to override the location of the nvEmbedqa NIM
## @param nvidia-nim-llama-32-nv-embedqa-1b-v2.image.tag The tag override for nvEmbedqa NIM
## @param nvidia-nim-llama-32-nv-embedqa-1b-v2.podSecurityContext.runAsUser [default: 1000] The user ID to run the container as
## @param nvidia-nim-llama-32-nv-embedqa-1b-v2.podSecurityContext.runAsGroup [default: 1000] The group ID to run the container as
## @param nvidia-nim-llama-32-nv-embedqa-1b-v2.podSecurityContext.fsGroup [default: 1000] The filesystem group ID to run the container as
## @param nvidia-nim-llama-32-nv-embedqa-1b-v2.replicaCount [default: 1] Number of replicas to deploy
## @param nvidia-nim-llama-32-nv-embedqa-1b-v2.serviceAccount.create [default: false] Whether to create a service account
## @param nvidia-nim-llama-32-nv-embedqa-1b-v2.serviceAccount.name [default: ""] The name of the service account to use
## @param nvidia-nim-llama-32-nv-embedqa-1b-v2.statefuleSet.enabled [default: false] Whether to deploy as a statefulset
## @param nvidia-nim-llama-32-nv-embedqa-1b-v2.autoscaling.enabled [default: false] Whether to enable autoscaling
## @param nvidia-nim-llama-32-nv-embedqa-1b-v2.autoscaling.minReplicas [default: 1] Minimum number of replicas
## @param nvidia-nim-llama-32-nv-embedqa-1b-v2.autoscaling.maxReplicas [default: 10] Maximum number of replicas
## @param nvidia-nim-llama-32-nv-embedqa-1b-v2.autoscaling.metrics [default: []] Metrics for autoscaling
## @param nvidia-nim-llama-32-nv-embedqa-1b-v2.service.type [default: "ClusterIP"] The type of Kubernetes service to create
## @param nvidia-nim-llama-32-nv-embedqa-1b-v2.service.name [default: "nv-ingest-embedqa"] The name of the service
## @param nvidia-nim-llama-32-nv-embedqa-1b-v2.service.httpPort [default: 8000] The HTTP port for the service
## @param nvidia-nim-llama-32-nv-embedqa-1b-v2.service.grpcPort [default: 8001] The gRPC port for the service
## @param nvidia-nim-llama-32-nv-embedqa-1b-v2.service.metricsPort [default: 0] The metrics port for the service
## @param nvidia-nim-llama-32-nv-embedqa-1b-v2.nim.grpcPort [default: 8001] The gRPC port for NIM communication
## @param nvidia-nim-llama-32-nv-embedqa-1b-v2.nim.logLevel [default: "INFO"] The log level for NIM
## @param nvidia-nim-llama-32-nv-embedqa-1b-v2.env [default: []] Environment variables for the container
nvidia-nim-llama-32-nv-embedqa-1b-v2:
  deployed: true
  fullnameOverride: nv-ingest-embedqa # Share name with text-embedding-nim to ease configuration
  customCommand: []
  customArgs: []
  image:
    repository: nvcr.io/nim/nvidia/llama-3.2-nv-embedqa-1b-v2
    tag: "1.5.0"
  podSecurityContext:
    runAsUser: 1000
    runAsGroup: 1000
    fsGroup: 1000
  replicaCount: 1
  serviceAccount:
    create: false
    name: ""
  statefuleSet:
    enabled: false
  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 10
    metrics: []
  service:
    type: "ClusterIP"
    name: nv-ingest-embedqa
    httpPort: 8000
    grpcPort: 8001
    metricsPort: 0  # Generally unused and defaults to 0
  nim:
    grpcPort: 8001
    logLevel: "INFO"
  env:
    - name: NIM_HTTP_API_PORT
      value: "8000"
    - name: NIM_TRITON_MAX_BATCH_SIZE
      value: "1"


## @skip llama-32-nv-rerankqa-1b-v2
## @param llama-32-nv-rerankqa-1b-v2.deployed [default: true] true if llama-32-nv-rerankqa-1b-v2 should be deployed and false otherwise
## @param llama-32-nv-rerankqa-1b-v2.fullnameOverride [default: "llama-32-nv-rerankqa-1b-v2"] Override for the full name of the deployment
## @param llama-32-nv-rerankqa-1b-v2.customCommand [default: []] Custom command to run in the container
## @param llama-32-nv-rerankqa-1b-v2.customArgs [default: []] Custom arguments to pass to the container
## @param llama-32-nv-rerankqa-1b-v2.image.repository The repository to override the location of the reranker NIM
## @param llama-32-nv-rerankqa-1b-v2.image.tag The tag override for reranker NIM
## @param llama-32-nv-rerankqa-1b-v2.podSecurityContext [default: {}] Security context for the pod
## @param llama-32-nv-rerankqa-1b-v2.replicaCount [default: 1] Number of replicas to deploy
## @param llama-32-nv-rerankqa-1b-v2.serviceAccount.create [default: false] Whether to create a service account
## @param llama-32-nv-rerankqa-1b-v2.serviceAccount.name [default: ""] Name of the service account
## @param llama-32-nv-rerankqa-1b-v2.statefuleSet.enabled [default: false] Whether to deploy as a statefulset
## @param llama-32-nv-rerankqa-1b-v2.autoscaling.enabled [default: false] Whether to enable autoscaling
## @param llama-32-nv-rerankqa-1b-v2.autoscaling.minReplicas [default: 1] Minimum number of replicas
## @param llama-32-nv-rerankqa-1b-v2.autoscaling.maxReplicas [default: 10] Maximum number of replicas
## @param llama-32-nv-rerankqa-1b-v2.autoscaling.metrics [default: []] Metrics for autoscaling
## @param llama-32-nv-rerankqa-1b-v2.service.type [default: "ClusterIP"] The type of Kubernetes service to create
## @param llama-32-nv-rerankqa-1b-v2.service.name [default: "llama-32-nv-rerankqa-1b-v2"] The name of the service
## @param llama-32-nv-rerankqa-1b-v2.service.httpPort [default: 8000] The HTTP port for the service
## @param llama-32-nv-rerankqa-1b-v2.service.grpcPort [default: 8001] The gRPC port for the service
## @param llama-32-nv-rerankqa-1b-v2.service.metricsPort [default: 0] The metrics port for the service
## @param llama-32-nv-rerankqa-1b-v2.nim.grpcPort [default: 8001] The gRPC port for NIM communication
## @param llama-32-nv-rerankqa-1b-v2.nim.logLevel [default: "INFO"] The log level for NIM
## @param llama-32-nv-rerankqa-1b-v2.env [default: []] Environment variables for the container
llama-32-nv-rerankqa-1b-v2:
  deployed: false
  # fullnameOverride: llama-32-nv-rerankqa-1b-v2
  customCommand: []
  customArgs: []
  image:
    repository: nvcr.io/nim/nvidia/llama-3.2-nv-rerankqa-1b-v2
    tag: "1.3.1"
  podSecurityContext:
    runAsUser: 1000
    runAsGroup: 1000
    fsGroup: 1000
  replicaCount: 1
  serviceAccount:
    create: false
    name: ""
  statefuleSet:
    enabled: false
  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 10
    metrics: []
  service:
    type: "ClusterIP"
    name: llama-32-nv-rerankqa-1b-v2
    httpPort: 8000
    grpcPort: 8001
    metricsPort: 0  # Generally unused and defaults to 0
  nim:
    grpcPort: 8001
    logLevel: "INFO"
  env:
    - name: NIM_HTTP_API_PORT
      value: "8000"
    - name: NIM_TRITON_MODEL_BATCH_SIZE
      value: "1"


## @skip riva-nim
## @param riva-nim.deployed [default: true] true if riva-nim should be deployed and false otherwise
## @param riva-nim.fullnameOverride [default: "riva-nim"] Override for the full name of the deployment
## @param riva-nim.customCommand [default: []] Custom command to run in the container
## @param riva-nim.customArgs [default: []] Custom arguments to pass to the container
## @param riva-nim.image.repository The repository to override the location of the riva-asr NIM
## @param riva-nim.image.tag The tag override for riva-asr NIM
## @param riva-nim.podSecurityContext [default: {}] Security context for the pod
## @param riva-nim.replicaCount [default: 1] Number of replicas to deploy
## @param riva-nim.serviceAccount.create [default: false] Whether to create a service account
## @param riva-nim.serviceAccount.name [default: ""] Name of the service account
## @param riva-nim.statefuleSet.enabled [default: false] Whether to deploy as a statefulset
## @param riva-nim.autoscaling.enabled [default: false] Whether to enable autoscaling
## @param riva-nim.autoscaling.minReplicas [default: 1] Minimum number of replicas
## @param riva-nim.autoscaling.maxReplicas [default: 10] Maximum number of replicas
## @param riva-nim.autoscaling.metrics [default: []] Metrics for autoscaling
## @param riva-nim.service.type [default: "ClusterIP"] The type of Kubernetes service to create
## @param riva-nim.service.name [default: "riva-nim"] The name of the service
## @param riva-nim.service.httpPort [default: 8000] The HTTP port for the service
## @param riva-nim.service.grpcPort [default: 8001] The gRPC port for the service
## @param riva-nim.service.metricsPort [default: 0] The metrics port for the service
## @param riva-nim.nim.grpcPort [default: 8001] The gRPC port for NIM communication
## @param riva-nim.nim.logLevel [default: "INFO"] The log level for NIM
## @param riva-nim.env [default: []] Environment variables for the container
riva-nim:
  deployed: false
  fullnameOverride: riva-nim
  customCommand: []
  customArgs: []
  image:
    repository: nvcr.io/nim/nvidia/riva-asr
    tag: "1.3.0"
  podSecurityContext:
    runAsUser: 1000
    runAsGroup: 1000
    fsGroup: 1000
  replicaCount: 1
  serviceAccount:
    create: false
    name: ""
  statefuleSet:
    enabled: false
  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 10
    metrics: []
  service:
    type: "ClusterIP"
    name: riva-nim
    httpPort: 8000
    grpcPort: 8001
    metricsPort: 0  # Generally unused and defaults to 0
  nim:
    grpcPort: 8001
    logLevel: "INFO"
  env:
    - name: NIM_HTTP_API_PORT
      value: "8000"


## @section Milvus Deployment parameters
## @descriptionStart
## NVIngest uses Milvus and Minio to store extracted images from a document
## This chart by default sets up a Milvus standalone instance in the namespace using the
## Helm chart at found https://artifacthub.io/packages/helm/milvus-helm/milvus
## @descriptionEnd
## @param milvusDeployed [default: true] Whether to deploy Milvus and Minio from this helm chart
milvusDeployed: true
## @section Milvus parameters
## @descriptionStart
## Milvus is used as the vector database for storing and searching embeddings.
## The chart uses the official Milvus Helm chart found at https://artifacthub.io/packages/helm/milvus-helm/milvus
## By default this deploys Milvus in standalone mode with GPU support.
## @descriptionEnd
## @param milvus.image.all.repository [default: milvusdb/milvus] The Milvus container image repository
## @param milvus.image.all.tag [default: v2.5.3-gpu] The Milvus container image tag
## @param milvus.cluster.enabled [default: false] Whether to deploy Milvus in cluster mode
## @param milvus.standalone.resources.limits.nvidia.com/gpu [default: 1] Number of GPUs to allocate to Milvus
## @param milvus.standalone.persistence.persistentVolumeClaim.size [default: 50Gi] Size of the PVC for Milvus data
## @param milvus.standalone.persistence.persistentVolumeClaim.storageClass Storage class to use for the PVC
## @param milvus.minio.mode [default: standalone] MinIO deployment mode
## @param milvus.minio.bucketName [default: a-bucket] Name of the MinIO bucket to create
## @param milvus.minio.persistence.size [default: 50Gi] Size of the PVC for MinIO data
## @param milvus.minio.persistence.storageClass Storage class to use for MinIO PVC
milvus:
  image:
    all:
      repository: milvusdb/milvus
      tag: v2.5.3-gpu
  cluster:
    enabled: false
  etcd:
    replicaCount: 1
    persistence:
      storageClass: null
  minio:
    mode: standalone
    bucketName: a-bucket
    persistence:
      size: 50Gi
      storageClass: null
  pulsar:
    enabled: false
  standalone:
    resources:
      limits:
        nvidia.com/gpu: 1
    persistence:
      persistentVolumeClaim:
        size: 50Gi
        storageClass: null
    extraEnv:
      - name: LOG_LEVEL
        value: error



## @section Autoscaling parameters
## @descriptionStart
## Values used for creating a `Horizontal Pod Autoscaler`. If autoscaling is not enabled, the rest are ignored.
## NVIDIA recommends usage of the custom metrics API, commonly implemented with the prometheus-adapter.
## Standard metrics of CPU and memory are of limited use in scaling NIM.
## @descriptionEnd
## @param autoscaling.enabled Enables horizontal pod autoscaler.
## @param autoscaling.minReplicas Specify minimum replicas for autoscaling.
## @param autoscaling.maxReplicas Specify maximum replicas for autoscaling.
## @param autoscaling.metrics Array of metrics for autoscaling.
autoscaling:
  enabled: false
  maxReplicas: 100
  minReplicas: 1
  metrics: []


## @section Redis configurations
## @descriptionStart
## Include any redis configuration that you'd like with the deployed Redis
## Find values at https://github.com/bitnami/charts/tree/main/bitnami/redis
## @descriptionEnd
## @param redisDeployed [default: true] Whether to deploy Redis from this helm chart
redisDeployed: true
## @section Redis parameters
## @descriptionStart
## Configure Redis settings for the deployment. These values are passed directly to the Redis Helm chart.
## For a complete list of configuration options, see: https://github.com/bitnami/charts/tree/main/bitnami/redis
## @descriptionEnd
## @param redis Redis configuration options
## @param redis.auth.enabled [default: false] Enable Redis authentication
## @param redis.replica.replicaCount [default: 1] Number of Redis replicas
## @param redis.replica.persistence.size [default: "50Gi"] Size of persistent volume for Redis replicas
## @param redis.replica.resources.requests.memory [default: "6Gi"] Memory requests for Redis replicas
## @param redis.replica.resources.limits.memory [default: "12Gi"] Memory limits for Redis replicas
## @param redis.master.persistence.size [default: "50Gi"] Size of persistent volume for Redis master
## @param redis.master.resources.requests.memory [default: "6Gi"] Memory requests for Redis master
## @param redis.master.resources.limits.memory [default: "12Gi"] Memory limits for Redis master
## @param redis.master.configmap [default: "protected-mode no"] Redis master configuration

redis:
  auth:
    enabled: false
  replica:
    replicaCount: 1
    persistence:
      size: "50Gi"
    resources:
      requests:
        memory: "6Gi"
      limits:
        memory: "12Gi"
  master:
    persistence:
      size: "50Gi"
    resources:
      requests:
        memory: "6Gi"
      limits:
        memory: "12Gi"
    configmap: |-
      protected-mode no

## @section Environment Variables
## @descriptionStart
## Define environment variables as key/value dictionary pairs. These environment variables configure various aspects
## of NV-Ingest's behavior including buffer sizes, worker counts, endpoints, protocols, and component configurations.
## @descriptionEnd
## @param env [array] List of environment variables to set in the container. Each environment variable is specified as a name/value pair.
## @param env[].INGEST_EDGE_BUFFER_SIZE [default: "64"] Size of the edge buffer for ingestion
## @param env[].MAX_INGEST_PROCESS_WORKERS [default: "16"] Maximum number of concurrent ingestion worker processes
## @param env[].MESSAGE_CLIENT_HOST [default: "nv-ingest-redis-master"] Redis host endpoint for message queue
## @param env[].MESSAGE_CLIENT_PORT [default: "6379"] Redis port for message queue
## @param env[].REDIS_INGEST_TASK_QUEUE [default: "ingest_task_queue"] Name of Redis queue for ingest tasks
## @param env[].NV_INGEST_DEFAULT_TIMEOUT_MS [default: "1234"] Default timeout in milliseconds for NV-Ingest requests
## @param env[].NV_INGEST_MAX_UTIL [default: "48"] Maximum CPU cores to utilize. Defaults to available cores if not set
## @param env[].MINIO_INTERNAL_ADDRESS [default: "nv-ingest-minio:9000"] Internal MinIO endpoint
## @param env[].MINIO_PUBLIC_ADDRESS [default: "http://localhost:9000"] Public MinIO endpoint (default assumes port-forwarding)
## @param env[].MINIO_BUCKET [default: "nv-ingest"] MinIO bucket name for storing extracted images
## @param env[].PADDLE_GRPC_ENDPOINT [default: "nv-ingest-paddle:8001"] PaddleOCR gRPC endpoint
## @param env[].PADDLE_HTTP_ENDPOINT [default: "http://nv-ingest-paddle:8000/v1/infer"] PaddleOCR HTTP endpoint
## @param env[].PADDLE_INFER_PROTOCOL [default: "grpc"] Protocol to use for PaddleOCR inference (grpc or http)
## @param env[].NEMORETRIEVER_PARSE_HTTP_ENDPOINT [default: "http://nim-vlm-text-extraction-nemoretriever-parse:8000/v1/chat/completions"] NeMo Retriever parse endpoint
## @param env[].NEMORETRIEVER_PARSE_INFER_PROTOCOL [default: "http"] Protocol for NeMo Retriever parse inference
## @param env[].YOLOX_GRPC_ENDPOINT [default: "nemoretriever-page-elements-v2:8001"] YOLOX page elements gRPC endpoint
## @param env[].YOLOX_HTTP_ENDPOINT [default: "http://nemoretriever-page-elements-v2:8000/v1/infer"] YOLOX page elements HTTP endpoint
## @param env[].YOLOX_INFER_PROTOCOL [default: "grpc"] Protocol for YOLOX page elements inference
## @param env[].YOLOX_GRAPHIC_ELEMENTS_GRPC_ENDPOINT [default: "nemoretriever-graphic-elements-v1:8001"] YOLOX graphic elements gRPC endpoint
## @param env[].YOLOX_GRAPHIC_ELEMENTS_HTTP_ENDPOINT [default: "http://nemoretriever-graphic-elements-v1:8000/v1/infer"] YOLOX graphic elements HTTP endpoint
## @param env[].YOLOX_GRAPHIC_ELEMENTS_INFER_PROTOCOL [default: "grpc"] Protocol for YOLOX graphic elements inference
## @param env[].YOLOX_TABLE_STRUCTURE_GRPC_ENDPOINT [default: "nemoretriever-table-structure-v1:8001"] YOLOX table structure gRPC endpoint
## @param env[].YOLOX_TABLE_STRUCTURE_HTTP_ENDPOINT [default: "http://nemoretriever-table-structure-v1:8000/v1/infer"] YOLOX table structure HTTP endpoint
## @param env[].YOLOX_TABLE_STRUCTURE_INFER_PROTOCOL [default: "grpc"] Protocol for YOLOX table structure inference
## @param env[].EMBEDDING_NIM_ENDPOINT [default: "http://nv-ingest-embedqa:8000/v1"] Text embedding NIM endpoint
## @param env[].EMBEDDING_NIM_MODEL_NAME [default: "nvidia/llama-3.2-nv-embedqa-1b-v2"] Model name for text embedding
## @param env[].MILVUS_ENDPOINT [default: "http://nv-ingest-milvus:19530"] Milvus vector database endpoint
## @param env[].VLM_CAPTION_ENDPOINT [default: "https://ai.api.nvidia.com/v1/gr/meta/llama-3.2-11b-vision-instruct/chat/completions"] Vision Language Model caption endpoint
## @param env[].VLM_CAPTION_MODEL_NAME [default: "meta/llama-3.2-11b-vision-instruct"] Model name for image captioning
## @param env[].AUDIO_GRPC_ENDPOINT [default: "audio:50051"] Audio processing gRPC endpoint
## @param env[].AUDIO_INFER_PROTOCOL [default: "grpc"] Protocol for audio processing inference
## @param env[].READY_CHECK_ALL_COMPONENTS [default: "true"] Whether to check readiness of all components on startup
## @param env[].MODEL_PREDOWNLOAD_PATH [default: "/workspace/models/"] Path for pre-downloading models
env:
  - name: INGEST_LOG_LEVEL
    value: "DEFAULT"
  - name: INGEST_EDGE_BUFFER_SIZE
    value: "64"
  - name: MAX_INGEST_PROCESS_WORKERS
    value: "16"
  - name: NV_INGEST_MAX_UTIL
    value: "48"
  - name: MESSAGE_CLIENT_HOST
    value: "nv-ingest-redis-master"
  - name: MESSAGE_CLIENT_PORT
    value: "6379"
  - name: REDIS_INGEST_TASK_QUEUE
    value: "ingest_task_queue"
  - name: NV_INGEST_DEFAULT_TIMEOUT_MS
    value: "1234"
  - name: MINIO_INTERNAL_ADDRESS
    value: "nv-ingest-minio:9000"
  - name: MINIO_PUBLIC_ADDRESS
    value: "http://localhost:9000"
  - name: MINIO_BUCKET
    value: "nv-ingest"
  - name: PADDLE_GRPC_ENDPOINT
    value: "nv-ingest-paddle:8001"
  - name: PADDLE_HTTP_ENDPOINT
    value: "http://nv-ingest-paddle:8000/v1/infer"
  - name: PADDLE_INFER_PROTOCOL
    value: "grpc"
  - name: NEMORETRIEVER_PARSE_HTTP_ENDPOINT
    value: "http://nim-vlm-text-extraction-nemoretriever-parse:8000/v1/chat/completions"
  - name: NEMORETRIEVER_PARSE_INFER_PROTOCOL
    value: "http"
  - name: YOLOX_GRPC_ENDPOINT
    value: "nemoretriever-page-elements-v2:8001"
  - name: YOLOX_HTTP_ENDPOINT
    value: "http://nemoretriever-page-elements-v2:8000/v1/infer"
  - name: YOLOX_INFER_PROTOCOL
    value: "grpc"
  - name: YOLOX_GRAPHIC_ELEMENTS_GRPC_ENDPOINT
    value: "nemoretriever-graphic-elements-v1:8001"
  - name: YOLOX_GRAPHIC_ELEMENTS_HTTP_ENDPOINT
    value: "http://nemoretriever-graphic-elements-v1:8000/v1/infer"
  - name: YOLOX_GRAPHIC_ELEMENTS_INFER_PROTOCOL
    value: "grpc"
  - name: YOLOX_TABLE_STRUCTURE_GRPC_ENDPOINT
    value: "nemoretriever-table-structure-v1:8001"
  - name: YOLOX_TABLE_STRUCTURE_HTTP_ENDPOINT
    value: "http://nemoretriever-table-structure-v1:8000/v1/infer"
  - name: YOLOX_TABLE_STRUCTURE_INFER_PROTOCOL
    value: "grpc"
  - name: EMBEDDING_NIM_ENDPOINT
    value: "http://nv-ingest-embedqa:8000/v1"
  - name: EMBEDDING_NIM_MODEL_NAME
    value: "nvidia/llama-3.2-nv-embedqa-1b-v2"
  - name: MILVUS_ENDPOINT
    value: "http://nv-ingest-milvus:19530"
  - name: VLM_CAPTION_ENDPOINT
    value: "https://ai.api.nvidia.com/v1/gr/meta/llama-3.2-11b-vision-instruct/chat/completions"
  - name: VLM_CAPTION_MODEL_NAME
    value: "meta/llama-3.2-11b-vision-instruct"
  - name: AUDIO_GRPC_ENDPOINT
    value: "audio:50051"
  - name: AUDIO_INFER_PROTOCOL
    value: "grpc"
  - name: COMPONENTS_TO_READY_CHECK
    value: "ALL"
  - name: MODEL_PREDOWNLOAD_PATH
    value: "/workspace/models/"


## @section Open Telemetry
## @descriptionStart
## Define environment variables as key/value dictionary pairs for configuring OTEL Deployments
## A sane set of parameters is set for the deployed version of OpenTelemetry with this Helm Chart.
## Override any values to the Open Telemetry helm chart by overriding the `open-telemetry` value.
## @descriptionEnd

## @param otelEnabled [default: true] Whether to enable OTEL collection
otelEnabled: true
## @param otelDeployed [default: true] Whether to deploy OTEL from this helm chart
otelDeployed: true

## @skip opentelemetry-collector
## @extra opentelemetry-collector [default: sane {}] Configures the opentelemetry helm chart - see https://github.com/open-telemetry/opentelemetry-helm-charts/blob/main/charts/opentelemetry-collector/values.yaml
opentelemetry-collector:
  mode: deployment
  config:
    receivers:
      otlp:
        protocols:
          grpc:
            endpoint: '${env:MY_POD_IP}:4317'
          http:
            cors:
              allowed_origins:
                - "*"
    exporters:
      # NOTE: Prior to v0.86.0 use `logging` instead of `debug`.
      zipkin:
        endpoint: "http://nv-ingest-zipkin:9411/api/v2/spans"
      debug:
        verbosity: detailed
    extensions:
      health_check: {}
      zpages:
        endpoint: 0.0.0.0:55679
    processors:
      batch: {}
      tail_sampling:
        # filter out health checks
        # https://github.com/open-telemetry/opentelemetry-collector/issues/2310#issuecomment-1268157484
        policies:
          - name: drop_noisy_traces_url
            type: string_attribute
            string_attribute:
              key: http.target
              values:
                - \/health
              enabled_regex_matching: true
              invert_match: true
      transform:
        trace_statements:
          - context: span
            statements:
              - set(status.code, 1) where attributes["http.path"] == "/health"

              # after the http target has been anonymized, replace other aspects of the span
              - replace_match(attributes["http.route"], "/v1", attributes["http.target"]) where attributes["http.target"] != nil

              # replace the title of the span with the route to be more descriptive
              - replace_pattern(name, "/v1", attributes["http.route"]) where attributes["http.route"] != nil

              # set the route to equal the URL if it's nondescriptive (for the embedding case)
              - set(name, Concat([name, attributes["http.url"]], " ")) where name == "POST"
    service:
      extensions: [zpages, health_check]
      pipelines:
        traces:
          receivers: [otlp]
          exporters: [debug, zipkin]
          processors: [tail_sampling, transform]
        metrics:
          receivers: [otlp]
          exporters: [debug]
          processors: [batch]
        logs:
          receivers: [otlp]
          exporters: [debug]
          processors: [batch]

## @param otelEnvVars [default: sane {}] Adds arbitrary environment variables for configuring OTEL using key-value pairs, for example NAME: value
## @extra otelEnvVars.OTEL_EXPORTER_OTLP_ENDPOINT Default deployment target for GRPC otel - Default "http://{{ .Release.Name }}-opentelemetry-collector:4317"
## @param otelEnvVars.OTEL_SERVICE_NAME [default: "nemo-retrieval-service" ]
## @param otelEnvVars.OTEL_TRACES_EXPORTER [default: "otlp" ]
## @param otelEnvVars.OTEL_METRICS_EXPORTER [default: "otlp" ]
## @param otelEnvVars.OTEL_LOGS_EXPORTER [default: "none" ]
## @param otelEnvVars.OTEL_PROPAGATORS [default: "tracecontext baggage" ]
## @param otelEnvVars.OTEL_RESOURCE_ATTRIBUTES [default: "deployment.environment=$(NAMESPACE)" ]
## @param otelEnvVars.OTEL_PYTHON_EXCLUDED_URLS [default: "health" ]
otelEnvVars:
  # OpenTelemetry
  OTEL_SERVICE_NAME: "nemo-retrieval-service"
  OTEL_TRACES_EXPORTER: otlp
  OTEL_METRICS_EXPORTER: otlp
  OTEL_LOGS_EXPORTER: none
  OTEL_PROPAGATORS: "tracecontext,baggage"
  OTEL_RESOURCE_ATTRIBUTES: "deployment.environment=$(NAMESPACE)"
  OTEL_PYTHON_EXCLUDED_URLS: "health"


## @param zipkinDeployed [default: true] Whether to deploy Zipkin with OpenTelemetry from this helm chart
zipkinDeployed: true


## @section Ingress parameters
## @param ingress.enabled Enables ingress.
## @param ingress.className Specify class name for Ingress.
## @param ingress.annotations Specify additional annotations for ingress.
## @param ingress.hosts[].host Specify name of host.
## @param ingress.hosts[].paths[].path Specify ingress path.
## @param ingress.hosts[].paths[].pathType Specify path type.
## @param ingress.tls Specify list of pairs of TLS `secretName` and hosts.
ingress:
  enabled: false
  className: ""
  annotations: {}
  hosts:
  - host: chart-example.local
    paths:
    - path: /
      pathType: ImplementationSpecific
  tls: []

## @section Probe parameters
## @param livenessProbe.enabled Enables `livenessProbe`
## @param livenessProbe.httpGet.path `LivenessProbe` endpoint path
## @param livenessProbe.httpGet.port `LivenessProbe` endpoint port
## @param livenessProbe.initialDelaySeconds Initial delay seconds for `livenessProbe`
## @param livenessProbe.timeoutSeconds Timeout seconds for `livenessProbe`
## @param livenessProbe.periodSeconds Period seconds for `livenessProbe`
## @param livenessProbe.successThreshold Success threshold for `livenessProbe`
## @param livenessProbe.failureThreshold Failure threshold for `livenessProbe`
livenessProbe:
  enabled: false
  httpGet:
    path: /v1/health/live
    port: http
  initialDelaySeconds: 120
  periodSeconds: 10
  timeoutSeconds: 20
  failureThreshold: 20
  successThreshold: 1

## @section Probe parameters
## @param readinessProbe.enabled Enables `readinessProbe`
## @param readinessProbe.httpGet.path `ReadinessProbe` endpoint path
## @param readinessProbe.httpGet.port `ReadinessProbe` endpoint port
## @param readinessProbe.initialDelaySeconds Initial delay seconds for `readinessProbe`
## @param readinessProbe.timeoutSeconds Timeout seconds for `readinessProbe`
## @param readinessProbe.periodSeconds Period seconds for `readinessProbe`
## @param readinessProbe.successThreshold Success threshold for `readinessProbe`
## @param readinessProbe.failureThreshold Failure threshold for `readinessProbe`
readinessProbe:
  enabled: false
  httpGet:
    path: /v1/health/ready
    port: http
  initialDelaySeconds: 120
  periodSeconds: 30
  timeoutSeconds: 10
  failureThreshold: 220
  successThreshold: 1

## @section Service parameters
## @param service.type Specifies the service type for the deployment.
## @param service.name Overrides the default service name
## @param service.port Specifies the HTTP Port for the service.
## @param service.nodePort Specifies an optional HTTP Node Port for the service.
## @param service.annotations [object] Specify additional annotations to be added to service.
## @param service.labels [object] Specifies additional labels to be added to service.
service:
  type: ClusterIP
  port: 7670
  annotations: {}
  labels: {}
  name: ""  # override the default service name
  nodePort: null

## @section Service Account
## @param serviceAccount.create Specifies whether a service account should be created.
## @param serviceAccount.annotations [object] Sets annotations to be added to the service account.
## @param serviceAccount.name Specifies the name of the service account to use.
## @param serviceAccount.automount [default: true] Specifies whether to automatically mount the service account token.
serviceAccount:
  annotations: {}
  automount: true
  create: true
  name: ""

## @section Secret Creation
## @descriptionStart
## Manage the creation of secrets used by the helm chart
## @descriptionEnd

# ngcApi:
# # If set to false, the chart expects a secret with the name
#   create: false
#   password: ""

## @param ngcApiSecret.create Specifies whether to create the ngc api secret
## @param ngcApiSecret.password The password to use for the NGC Secret
ngcApiSecret:
  # If set to false, the chart expects a secret with name ngc-api to exist in the namespace
  # credentials are needed.
  create: false
  password: ""

## @param ngcImagePullSecret.create Specifies whether to create the NVCR Image Pull secret
## @param ngcImagePullSecret.password The password to use for the NVCR Image Pull Secret
## @param ngcImagePullSecret.registry [default: "nvcr.io"] The registry URL
## @param ngcImagePullSecret.name [default: "ngcImagePullSecret"] The name of the secret
## @param ngcImagePullSecret.username [default: "$oauthtoken"] The username for the registry
ngcImagePullSecret:
  create: false
  # Leave blank, if no imagePullSecret is needed.
  registry: "nvcr.io"
  name: "ngcImagePullSecret"
  # If set to false, the chart expects either a imagePullSecret
  # with the name configured above to be present on the cluster or that no
  # credentials are needed.

  username: '$oauthtoken'
  password: ""

## @section Container Configuration
## @param nemo.userID [default: "1000"] User ID for the NEMO container
## @param nemo.groupID [default: "1000"] Group ID for the NEMO container
## @param containerArgs [array] Additional arguments to pass to the container
nemo:
  userID: "1000"
  groupID: "1000"

## @param containerArgs [array] Additional arguments to pass to the container
containerArgs: []
